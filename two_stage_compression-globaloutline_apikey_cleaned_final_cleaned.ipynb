{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0d646-213d-4cd8-8800-e735b6df82bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # 📚 全流程模块（DeepSeek + OpenRouter/Claude 3.7 双后端）\n",
    "# #   1. 拆章节 split_novels()\n",
    "# #   2. 批量 LLM 分析 process()   → 章级 *_processed.txt\n",
    "# #   3. 扩展：provider 可选 \"deepseek\" / \"openrouter\"\n",
    "# #      - 两端均支持 response_format={\"type\":\"json_object\"}\n",
    "# # ============================================================\n",
    "\n",
    "# # ── 安装必需包 ────────────────────────────────────────────────\n",
    "# !pip -q install --upgrade openai tqdm chardet pandas matplotlib jieba networkx requests\n",
    "\n",
    "# # ── 通用库 ───────────────────────────────────────────────────\n",
    "# import os, re, json, logging, unicodedata, chardet, requests\n",
    "# from pathlib import Path\n",
    "# from typing import List, Dict, Tuple\n",
    "# from collections import defaultdict\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # ── DeepSeek / OpenRouter 配置 ───────────────────────────────\n",
    "# class Provider(str):\n",
    "#     DEEPSEEK   = \"deepseek\"\n",
    "#     OPENROUTER = \"openrouter\"\n",
    "\n",
    "# DEEPSEEK_API_KEY  = os.getenv(\"DEEPSEEK_API_KEY\")   or \"your-default-api-key\"\n",
    "# DEEPSEEK_URL      = \"https://api.deepseek.com\"\n",
    "# DEEPSEEK_MODEL    = \"deepseek-chat\"\n",
    "\n",
    "# OPENROUTER_API_KEY= os.getenv(\"OPENROUTER_API_KEY\") or \"your-default-api-key\"\n",
    "# OPENROUTER_URL    = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "# OPENROUTER_MODEL  = \"anthropic/claude-3.7-sonnet\"\n",
    "\n",
    "# import openai                                # 只给 DeepSeek 用\n",
    "# deep_client = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_URL)\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "# # ============================================================\n",
    "# # 1️⃣ 拆章节 split_novels()\n",
    "# # ============================================================\n",
    "# _CHAPTER_PAT = re.compile(\n",
    "#     r\"\"\"\n",
    "#     ^\\s*(\n",
    "#         第[\\d零一二三四五六七八九十百千万]+\\s*[章节卷回]\\s* |\n",
    "#         [零一二三四五六七八九十百千万]{1,4}[\\.．、\\s]+ |\n",
    "#         (?:Chapter|CHAPTER)\\s+\\d+               |\n",
    "#         \\d{1,3}[\\.．、]\\s*                      |\n",
    "#         \\d{1,3}\\s+\n",
    "#     )\\s*(.*?)$\n",
    "#     \"\"\", re.MULTILINE | re.IGNORECASE | re.VERBOSE\n",
    "# )\n",
    "\n",
    "# def _safe_name(s: str) -> str:\n",
    "#     s = unicodedata.normalize(\"NFKC\", s)\n",
    "#     s = re.sub(r\"[\\r\\n\\t]+\", \" \", s)\n",
    "#     s = re.sub(r'[\\\\/:*?\"<>|]', \"_\", s)\n",
    "#     s = re.sub(r\"\\s+\", \"\", s)\n",
    "#     return s[:80] or \"未知\"\n",
    "\n",
    "# def _auto_decode(path: Path) -> str:\n",
    "#     raw = path.read_bytes()\n",
    "#     enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "#     return raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "# def split_novels(input_dir: str, output_base: str | None = None) -> Dict[str, List[Path]]:\n",
    "#     in_p, out_p = Path(input_dir), Path(output_base or f\"{input_dir.rstrip('/')}_chapters\")\n",
    "#     out_p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     novels: Dict[str, List[Path]] = {}\n",
    "#     for txt in in_p.glob(\"*.txt\"):\n",
    "#         book_dir = out_p / _safe_name(txt.stem); book_dir.mkdir(exist_ok=True)\n",
    "#         data = _auto_decode(txt)\n",
    "#         ms = list(_CHAPTER_PAT.finditer(data))\n",
    "#         blocks = [(ms[i].group().strip(),\n",
    "#                    data[ms[i].end():ms[i+1].start()] if i+1<len(ms) else data[ms[i].end():])\n",
    "#                   for i in range(len(ms))] if ms else \\\n",
    "#                  [(f\"未知章节{i+1}\", data[s:s+5000]) for i,s in enumerate(range(0,len(data),5000))]\n",
    "#         paths=[]\n",
    "#         for i,(title,body) in enumerate(blocks,1):\n",
    "#             p=book_dir/f\"{i:03d}_{_safe_name(title)}.txt\"\n",
    "#             p.write_text(body.strip(),encoding=\"utf-8\"); paths.append(p)\n",
    "#         novels[book_dir.name]=paths\n",
    "#         logging.info(f\"《{txt.stem}》→ {len(paths)} 章\")\n",
    "#     return novels\n",
    "# #============================================================ \n",
    "# # 2️⃣ 统一 LLM 调用 _call_llm()\n",
    "# # ============================================================\n",
    "# def _call_llm(prompt:str, content:str,\n",
    "#               *, provider:Provider=Provider.DEEPSEEK,\n",
    "#               json_mode:bool=False) -> str:\n",
    "#     if provider==Provider.DEEPSEEK:\n",
    "#         rsp = deep_client.chat.completions.create(\n",
    "#             model=DEEPSEEK_MODEL,\n",
    "#             messages=[{\"role\":\"system\",\"content\":prompt},\n",
    "#                       {\"role\":\"user\",\"content\":content}],\n",
    "#             temperature=0.3,\n",
    "#             max_tokens=4096,\n",
    "#             response_format={\"type\":\"json_object\"} if json_mode else None\n",
    "#         )\n",
    "#         return rsp.choices[0].message.content.strip()\n",
    "\n",
    "#     if provider==Provider.OPENROUTER:\n",
    "#         body = {\n",
    "#             \"model\": OPENROUTER_MODEL,\n",
    "#             \"messages\":[\n",
    "#                 {\"role\":\"system\",\"content\":prompt},\n",
    "#                 {\"role\":\"user\",\"content\":content}\n",
    "#             ]\n",
    "#         }\n",
    "#         if json_mode:\n",
    "#             body[\"response_format\"]={\"type\":\"json_object\"}\n",
    "#         r = requests.post(OPENROUTER_URL,\n",
    "#                           headers={\n",
    "#                               \"Authorization\":f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "#                               \"Content-Type\":\"application/json\"\n",
    "#                           },\n",
    "#                           data=json.dumps(body))\n",
    "#         if r.status_code!=200:\n",
    "#             raise RuntimeError(f\"OpenRouter {r.status_code}: {r.text[:200]}\")\n",
    "#         return r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "#     raise ValueError(\"Unknown provider\")\n",
    "\n",
    "# # ============================================================\n",
    "# # 3️⃣ process_chapters() / process()\n",
    "# # ============================================================\n",
    "# def process_chapters(chapter_files: List[Path],\n",
    "#                      prompt: str,\n",
    "#                      *,\n",
    "#                      provider:Provider = Provider.DEEPSEEK,\n",
    "#                      json_mode: bool = True,\n",
    "#                      workers:int = 4,\n",
    "#                      suffix=\"_processed.txt\"):\n",
    "#     def _run(fp:Path):\n",
    "#         res=_call_llm(prompt,fp.read_text(encoding=\"utf-8\"),\n",
    "#                       provider=provider,json_mode=json_mode)\n",
    "#         out=fp.with_name(fp.stem+suffix); out.write_text(res,encoding=\"utf-8\")\n",
    "#         return out\n",
    "#     with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "#         list(ex.map(_run, chapter_files))\n",
    "\n",
    "# def _gather(root:Path)->List[Path]:\n",
    "#     return sorted(root.rglob(\"*.txt\"))\n",
    "\n",
    "# def process(chapter_root:str,*,\n",
    "#             prompt:str,\n",
    "#             provider:Provider=Provider.DEEPSEEK,\n",
    "#             out_dir:str=\"/content/json_results\",\n",
    "#             json_mode:bool=True,\n",
    "#             chapters:Tuple[int,int]|None=None):\n",
    "#     s,e = chapters or (1,float(\"inf\"))\n",
    "#     root, out_base = Path(chapter_root), Path(out_dir); out_base.mkdir(parents=True,exist_ok=True)\n",
    "#     books = [d for d in root.iterdir() if d.is_dir()] or [root]\n",
    "\n",
    "#     for book in tqdm(books, desc=\"📚 书库\"):\n",
    "#         files=[p for p in _gather(book) if s<=int(p.stem.split(\"_\")[0])<=e]\n",
    "#         for group_idx in range(0,len(files),10):\n",
    "#             grp=files[group_idx:group_idx+10]\n",
    "#             if not grp: continue\n",
    "#             start=int(grp[0].stem[:3]); end=start+len(grp)-1\n",
    "#             sub=out_base/book.name/f\"{start:03d}-{end:03d}\"; sub.mkdir(parents=True,exist_ok=True)\n",
    "#             todo=[p for p in grp if not (sub/f\"{p.stem}_processed.txt\").exists()]\n",
    "#             if todo:\n",
    "#                 process_chapters(todo,prompt,\n",
    "#                                  provider=provider,json_mode=json_mode)\n",
    "#                 for tp in todo:\n",
    "#                     (sub/f\"{tp.stem}_processed.txt\").write_text(\n",
    "#                         (tp.parent/f\"{tp.stem}_processed.txt\").read_text(encoding=\"utf-8\"),\n",
    "#                         encoding=\"utf-8\")\n",
    "#                     (tp.parent/f\"{tp.stem}_processed.txt\").unlink()\n",
    "#     print(f\"✅ 输出目录：{out_base}\")\n",
    "\n",
    "\n",
    "# def run_analysis(\n",
    "#     chapter_root: str,\n",
    "#     *,\n",
    "#     prompt: str,\n",
    "#     provider: Provider = Provider.DEEPSEEK,\n",
    "#     out_dir: str = \"/content/json_results\",\n",
    "#     mode: str | tuple[int, int] = \"full\",   # ← 关键参数\n",
    "#     json_mode: bool = True,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     mode 用法\n",
    "#     --------\n",
    "#     • \"full\"          → 分析整本\n",
    "#     • 100             → 只分析 1~100 章\n",
    "#     • (51, 150)       → 分析 51~150 章\n",
    "#     \"\"\"\n",
    "#     if mode == \"full\":\n",
    "#         chapters = None\n",
    "#     elif isinstance(mode, int):\n",
    "#         chapters = (1, mode)\n",
    "#     elif isinstance(mode, tuple) and len(mode) == 2:\n",
    "#         chapters = mode\n",
    "#     else:\n",
    "#         raise ValueError(\"mode 必须是 'full'、整数 N，或 (start, end) 元组\")\n",
    "\n",
    "#     process(\n",
    "#         chapter_root=chapter_root,\n",
    "#         prompt=prompt,\n",
    "#         provider=provider,\n",
    "#         out_dir=out_dir,\n",
    "#         json_mode=json_mode,\n",
    "#         chapters=chapters\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09090f2f-f67d-40c6-8a75-373f657b4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # 📊 小说章节 JSON 统计（库级批量版，区间压缩 + 保存）\n",
    "# # ============================================================\n",
    "# import json, re\n",
    "# from pathlib import Path\n",
    "# from collections import defaultdict\n",
    "# from typing import Dict, List, Tuple\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display\n",
    "\n",
    "# # ---------- 工具 ----------\n",
    "# _DIGIT_RE = re.compile(r\"(\\d{3})\")\n",
    "# def _extract_no(p: Path) -> int:\n",
    "#     m = _DIGIT_RE.search(p.stem) or re.match(r\"(\\d{3})-\", p.parent.name)\n",
    "#     return int(m.group(1)) if m else -1\n",
    "\n",
    "# def _compress(nums: List[int]) -> List[str]:\n",
    "#     if not nums: return []\n",
    "#     nums = sorted(nums)\n",
    "#     res, s, prev = [], nums[0], nums[0]\n",
    "#     for n in nums[1:]:\n",
    "#         if n == prev + 1: prev = n; continue\n",
    "#         res.append(f\"{s}-{prev}\" if s != prev else f\"{s}\")\n",
    "#         s = prev = n\n",
    "#     res.append(f\"{s}-{prev}\" if s != prev else f\"{s}\")\n",
    "#     return res\n",
    "\n",
    "# # ---------- 单本 ----------\n",
    "# # ---------- 修正版 stat_book ----------\n",
    "# # ---------- 工具：出现映射 ----------\n",
    "# def _map_occ(df: pd.DataFrame, col: str) -> Dict[str, List[int]]:\n",
    "#     mp = defaultdict(list)\n",
    "#     for _, row in df.iterrows():\n",
    "#         for item in row[col]:\n",
    "#             mp[item].append(row[\"chapter\"])\n",
    "#     return mp\n",
    "\n",
    "# def _build(mp: Dict[str,List[int]], *, min_chaps=1, top_n=20):\n",
    "#     data = [\n",
    "#         {\"name\": k, \"count\": len(v), \"chapters\": _compress(v)}\n",
    "#         for k, v in mp.items() if len(v) >= min_chaps\n",
    "#     ]\n",
    "#     return sorted(data, key=lambda x: x[\"count\"], reverse=True)[:top_n]\n",
    "\n",
    "# # ---------- 单本统计 ----------\n",
    "# def stat_book(book_dir: str | Path,\n",
    "#               *,\n",
    "#               min_chapters: int = 3,\n",
    "#               top_n: int = 30,\n",
    "#               show_plot: bool = False) -> dict:\n",
    "#     \"\"\"对单本小说目录生成统计 dict\"\"\"\n",
    "#     book_dir = Path(book_dir)\n",
    "#     rows, auto_idx = [], 1\n",
    "\n",
    "#     for p in sorted(book_dir.rglob(\"*_processed.txt\")):\n",
    "#         chap_no = _extract_no(p)\n",
    "#         if chap_no == -1:\n",
    "#             chap_no, auto_idx = auto_idx, auto_idx + 1   # 顺序补号\n",
    "#         try:\n",
    "#             data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "#         except json.JSONDecodeError:\n",
    "#             print(f\"⚠️ 跳过无效 JSON: {p}\"); continue\n",
    "\n",
    "#         rows.append({\n",
    "#             \"chapter\":   chap_no,\n",
    "#             \"characters\": data.get(\"出现人物\", []),\n",
    "#             \"scenes\":     data.get(\"出现场景\", []),\n",
    "#             \"props\":      data.get(\"出现道具\", []),\n",
    "#             \"setup\":      data.get(\"伏笔_设下\", []),\n",
    "#             \"recycle\":    data.get(\"伏笔_回收\", []),\n",
    "#         })\n",
    "\n",
    "#     if not rows:\n",
    "#         raise ValueError(f\"{book_dir} 无有效 *_processed.txt\")\n",
    "\n",
    "#     df = pd.DataFrame(rows).sort_values(\"chapter\").reset_index(drop=True)  # ← 这行之前缺失\n",
    "\n",
    "#     char_map = _map_occ(df, \"characters\")\n",
    "#     major    = {k: v for k, v in char_map.items() if len(v) >= min_chapters}\n",
    "\n",
    "#     summary = {\n",
    "#         \"主要角色\": _build(major, min_chaps=min_chapters, top_n=top_n),\n",
    "#         \"场景\":     _build(_map_occ(df, \"scenes\"),  top_n=top_n),\n",
    "#         \"道具\":     _build(_map_occ(df, \"props\"),   top_n=top_n),\n",
    "#         \"伏笔_设下\": _build(_map_occ(df, \"setup\"),  top_n=top_n),\n",
    "#         \"伏笔_回收\": _build(_map_occ(df, \"recycle\"),top_n=top_n),\n",
    "#     }\n",
    "\n",
    "#     if show_plot and major:\n",
    "#         plt.figure(figsize=(10, 4))\n",
    "#         for c, chs in major.items():\n",
    "#             plt.plot(df[\"chapter\"], [1 if n in chs else 0 for n in df[\"chapter\"]], label=c)\n",
    "#         plt.xlabel(\"章节号\"); plt.ylabel(\"出现(1)\"); plt.title(book_dir.name); plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "#     return summary\n",
    "\n",
    "# # ---------- 整库批量 ----------\n",
    "# def stat_library(root_dir: str | Path,\n",
    "#                  out_root: str | Path = \"/content/novel_stats\",\n",
    "#                  *, min_chapters: int = 3,\n",
    "#                  top_n: int = 10,\n",
    "#                  show_plot: bool = False):\n",
    "#     root_dir = Path(root_dir); out_root = Path(out_root)\n",
    "#     out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     for book in root_dir.iterdir():\n",
    "#         if not book.is_dir(): continue\n",
    "#         try:\n",
    "#             res = stat_book(book,\n",
    "#                             min_chapters=min_chapters,\n",
    "#                             top_n=top_n,\n",
    "#                             show_plot=show_plot)\n",
    "#         except ValueError as e:\n",
    "#             print(e); continue\n",
    "\n",
    "#         out_file = out_root / book.name / f\"{book.name}_stats.json\"\n",
    "#         out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         out_file.write_text(json.dumps(res, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "#         print(f\"✅ 写入 {out_file.relative_to(out_root)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220f539-97ff-42ca-a829-502c2b15018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 拆章节\n",
    "# # split_novels(\"/content/novels\", \"/content/novels_chapters\")\n",
    "\n",
    "# # 2. 定义章节分析 Prompt,有两种，json的和plain的，400字的和800字的，200字的。\n",
    "# # prompt_chapter_json = r\"\"\"\n",
    "# # 你是一位专业文学编辑。请阅读我接下来提供的【章节全文】，按照下列要求生成 **400-500 字** 的详细章节分析，并 **仅以 JSON 格式** 返回结果（严禁输出任何解释或 Markdown）。\n",
    "\n",
    "# # JSON 顶层字段与要求：\n",
    "# # {\n",
    "# #   \"章节定位导语\":         \"<100-200 字，说明章节在整书中的位置和作用>\",\n",
    "# #   \"情节摘要导语\":         \"<200-300 字，概括该章节的主要内容,请尽量把剧情讲述完整>\",\n",
    "# #   \"情感与节奏导语\":       \"<200-300 字，分析章节的情感和节奏变化>\",\n",
    "# #   \"关键场景分析\":         \"<200-400 字，深入分析章节中的重要场景>\",\n",
    "# #   \"人物角色变化\":         \"<200-300 字，分析章节中人物的表现和变化>\",\n",
    "# #   \"情感张力变化\":         \"<100-200 字，分析章节情感的起伏>\",\n",
    "# #   \"节奏与结构观察\":       \"<100-200 字，分析章节节奏与整书主题和情节的关联>\",\n",
    "# #   \"出现人物\":             [\"角色A\", \"角色B\", ...],\n",
    "# #   \"出现道具\":             [\"道具1\", \"道具2\", ...],\n",
    "# #   \"出现场景\":             [\"场景1\", \"场景2\"],\n",
    "# #   \"伏笔_设下\":            [\"伏笔1 描述\", \"伏笔2 描述\"],\n",
    "# #   \"伏笔_回收\":            [\"伏笔A 回收方式\", ...]\n",
    "# # }\n",
    "\n",
    "# # 严格要求：\n",
    "# # 1. 文本字段须为完整中文段落；字数必须落在区间内。\n",
    "# # 2. 上述 12 个键一个都不能少，也不能多。\n",
    "# # 3. “出现人物 / 道具 / 场景 / 伏笔” 用 JSON 数组。\n",
    "# # 4. 可引用原文≤10%，需加引号注明段落号。\n",
    "# # 【章节全文】：\n",
    "# # \"\"\"\n",
    "\n",
    "# # prompt_chapter_json_100000_json = r\"\"\"\n",
    "# # 你是一位专业文学编辑。请阅读我接下来提供的【章节全文】，按照下列要求生成 **400-500 字** 的详细章节分析，并 **仅以 JSON 格式** 返回结果（严禁输出任何解释或 Markdown）。\n",
    "\n",
    "# # JSON 顶层字段与要求：\n",
    "# # {\n",
    "# #   \"章节定位导语\":         \"<40 字，说明章节在整书中的位置和作用>\",\n",
    "# #   \"情节摘要导语\":         \"<40 字，概括该章节的主要内容,请尽量把剧情讲述完整>\",\n",
    "# #   \"情感与节奏导语\":       \"<40 字，分析章节的情感和节奏变化>\",\n",
    "# #   \"关键场景分析\":         \"<40 字，深入分析章节中的重要场景>\",\n",
    "# #   \"人物角色变化\":         \"<40 字，分析章节中人物的表现和变化>\",\n",
    "# #   \"情感张力变化\":         \"<40 字，分析章节情感的起伏>\",\n",
    "# #   \"节奏与结构观察\":       \"<40 字，分析章节节奏与整书主题和情节的关联>\",\n",
    "# #   \"出现人物\":             [\"角色A\", \"角色B\", ...],\n",
    "# #   \"出现道具\":             [\"道具1\", \"道具2\", ...],\n",
    "# #   \"出现场景\":             [\"场景1\", \"场景2\"],\n",
    "# #   \"伏笔_设下\":            [\"伏笔1 描述\", \"伏笔2 描述\"],\n",
    "# #   \"伏笔_回收\":            [\"伏笔A 回收方式\", ...]\n",
    "# # }\n",
    "\n",
    "# # 严格要求：\n",
    "# # 1. 文本字段须为完整中文段落；字数必须落在区间内。\n",
    "# # 2. 上述 12 个键一个都不能少，也不能多。\n",
    "# # 3. “出现人物 / 道具 / 场景 / 伏笔” 用 JSON 数组。\n",
    "# # 4. 可引用原文≤10%，需加引号注明段落号。\n",
    "# # 【章节全文】：\n",
    "# # \"\"\"\n",
    "\n",
    "\n",
    "# prompt_chapter_json_100000_json = r\"\"\"\n",
    "# 你是一位专业文学编辑。请阅读我接下来提供的【章节全文】，按照下列要求生成总计 **400-500 字** 的详细章节分析，并 **仅以 JSON 格式** 返回结果（严禁输出任何解释或 Markdown）。\n",
    "\n",
    "# JSON 顶层字段与要求：\n",
    "# {\n",
    "#   \"情节摘要导语\":         \"<400 字，概括该章节的主要内容,请尽量把剧情讲述完整>\",\n",
    "#   \"出现人物\":             [\"角色A\", \"角色B\", ...],\n",
    "#   \"出现道具\":             [\"道具1\", \"道具2\", ...],\n",
    "#   \"出现场景\":             [\"场景1\", \"场景2\"],\n",
    "#   \"伏笔_设下\":            [\"伏笔1 描述\", \"伏笔2 描述\"],\n",
    "#   \"伏笔_回收\":            [\"伏笔A 回收方式\", ...]\n",
    "# }\n",
    "\n",
    "# 严格要求：\n",
    "# 1. 文本字段须为完整中文段落；字数必须落在区间内。\n",
    "# 2. 上述 6 个键一个都不能少，也不能多。\n",
    "# 3. “出现人物 / 道具 / 场景 / 伏笔” 用 JSON 数组。\n",
    "# 4. 可引用原文≤10%，需加引号注明段落号。\n",
    "# 【章节全文】：\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_chapter_json_50000_json = r\"\"\"\n",
    "# 你是一位专业文学编辑。请阅读我接下来提供的【章节全文】，按照下列要求生成总计 **200-250 字** 的详细章节分析，并 **仅以 JSON 格式** 返回结果（严禁输出任何解释或 Markdown）。\n",
    "\n",
    "# JSON 顶层字段与要求：\n",
    "# {\n",
    "#   \"情节摘要导语\":         \"<200 字，概括该章节的主要内容,请尽量把剧情讲述完整>\",\n",
    "#   \"出现人物\":             [\"角色A\", \"角色B\", ...],\n",
    "#   \"出现道具\":             [\"道具1\", \"道具2\", ...],\n",
    "#   \"出现场景\":             [\"场景1\", \"场景2\"],\n",
    "#   \"伏笔_设下\":            [\"伏笔1 描述\", \"伏笔2 描述\"],\n",
    "#   \"伏笔_回收\":            [\"伏笔A 回收方式\", ...]\n",
    "# }\n",
    "\n",
    "# 严格要求：\n",
    "# 1. 文本字段须为完整中文段落；字数必须落在区间内。\n",
    "# 2. 上述 6 个键一个都不能少，也不能多。\n",
    "# 3. “出现人物 / 道具 / 场景 / 伏笔” 用 JSON 数组。\n",
    "# 4. 可引用原文≤10%，需加引号注明段落号。\n",
    "# 【章节全文】：\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# prompt_chapter_json_200000_json = r\"\"\"\n",
    "# 你是一位专业文学编辑。请阅读我接下来提供的【章节全文】，按照下列要求生成总计 **700-800 字** 的详细章节分析，并 **仅以 JSON 格式** 返回结果（严禁输出任何解释或 Markdown）。\n",
    "\n",
    "# JSON 顶层字段与要求：\n",
    "# {\n",
    "#   \"情节摘要导语\":         \"<700 字，概括该章节的主要内容,请尽量把剧情讲述完整>\",\n",
    "#   \"出现人物\":             [\"角色A\", \"角色B\", ...],\n",
    "#   \"出现道具\":             [\"道具1\", \"道具2\", ...],\n",
    "#   \"出现场景\":             [\"场景1\", \"场景2\"],\n",
    "#   \"伏笔_设下\":            [\"伏笔1 描述\", \"伏笔2 描述\"],\n",
    "#   \"伏笔_回收\":            [\"伏笔A 回收方式\", ...]\n",
    "# }\n",
    "\n",
    "# 严格要求：\n",
    "# 1. 文本字段须为完整中文段落；字数必须落在区间内。\n",
    "# 2. 上述 6 个键一个都不能少，也不能多。\n",
    "# 3. “出现人物 / 道具 / 场景 / 伏笔” 用 JSON 数组。\n",
    "# 4. 可引用原文≤10%，需加引号注明段落号。\n",
    "# 【章节全文】：\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # 3. 批量分析（只处理 1–100 章）\n",
    "# # 只分析 1–100 章\n",
    "# # process(\n",
    "# #     chapter_root=\"/content/novels_chapters\",\n",
    "# #     prompt=prompt_chapter_json,\n",
    "# #     out_dir=\"/content/json_results\",\n",
    "# #     json_mode=True,\n",
    "# #     chapters=(1, 10)\n",
    "# # )\n",
    "\n",
    "# # # 1) 分析前 150 章（DeepSeek）\n",
    "# run_analysis(\n",
    "#     chapter_root=\"/content/novels_chapters\",\n",
    "#     prompt=prompt_chapter_json_50000_json,\n",
    "#     provider=Provider.DEEPSEEK,\n",
    "#     out_dir=\"/content/json_results/50000_json\",\n",
    "#     mode=(1, 150)      # 指定区间\n",
    "# )\n",
    "\n",
    "# run_analysis(\n",
    "#     chapter_root=\"/content/novels_chapters\",\n",
    "#     prompt=prompt_chapter_json_100000_json,\n",
    "#     provider=Provider.DEEPSEEK,\n",
    "#     out_dir=\"/content/json_results/100000_json\",\n",
    "#     mode=(1, 150)      # 指定区间\n",
    "# )\n",
    "\n",
    "# run_analysis(\n",
    "#     chapter_root=\"/content/novels_chapters\",\n",
    "#     prompt=prompt_chapter_json_200000_json,\n",
    "#     provider=Provider.DEEPSEEK,\n",
    "#     out_dir=\"/content/json_results/200000_json\",\n",
    "#     mode=(1, 150)      # 指定区间\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4c963-0a8b-43ae-b9af-41db6c18c912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /venv/main/lib/python3.10/site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /venv/main/lib/python3.10/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /venv/main/lib/python3.10/site-packages (from google-generativeai) (2.25.0rc0)\n",
      "Requirement already satisfied: google-api-python-client in /venv/main/lib/python3.10/site-packages (from google-generativeai) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /venv/main/lib/python3.10/site-packages (from google-generativeai) (2.39.0)\n",
      "Requirement already satisfied: protobuf in /venv/main/lib/python3.10/site-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: pydantic in /venv/main/lib/python3.10/site-packages (from google-generativeai) (2.11.3)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /venv/main/lib/python3.10/site-packages (from google-generativeai) (4.13.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /venv/main/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /venv/main/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /venv/main/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /venv/main/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /venv/main/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /venv/main/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /venv/main/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /venv/main/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /venv/main/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /venv/main/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /venv/main/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /venv/main/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /venv/main/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /venv/main/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /venv/main/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /venv/main/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "# # =============================\n",
    "# # ✅ Gemini 分析小说章节，结构化输出 JSON（强制调用函数）\n",
    "# # =============================\n",
    "# !pip install -q --upgrade google-generativeai chardet tqdm \n",
    "# !pip install -U google-generativeai\n",
    "\n",
    "\n",
    "# import os, json, re, time, random, string, chardet, logging\n",
    "# from pathlib import Path\n",
    "# from typing import List\n",
    "# from tqdm.auto import tqdm\n",
    "# import google.generativeai as genai\n",
    "\n",
    "# # —— API 初始化 ——\n",
    "# os.environ[\"GEMINI_API_KEY\"]     = \"your-api-key\"     # ★必填用 Gemini\n",
    "\n",
    "# GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "# genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "# gemini_model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "# # —— 通用函数 ——\n",
    "# def _auto_decode(path: Path) -> str:\n",
    "#     raw = path.read_bytes()\n",
    "#     enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "#     return raw.decode(enc, errors=\"ignore\").strip()\n",
    "\n",
    "# def _rand_tag(k=6): return ''.join(random.choices(string.ascii_uppercase, k=k))\n",
    "\n",
    "# # —— JSON 输出字段要求（6 项） ——\n",
    "# json_schema = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"情节摘要导语\": {\"type\": \"string\"},\n",
    "#         \"出现人物\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "#         \"出现道具\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "#         \"出现场景\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "#         \"伏笔_设下\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "#         \"伏笔_回收\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "#     },\n",
    "#     \"required\": [\"情节摘要导语\", \"出现人物\", \"出现道具\", \"出现场景\", \"伏笔_设下\", \"伏笔_回收\"]\n",
    "# }\n",
    "\n",
    "# # —— Prompt 模板（可替换为你指定的 50k / 100k / 200k） ——\n",
    "# prompt_chapter_json = r\"\"\"\n",
    "# 你是一位专业文学编辑。请阅读我接下来提供的【章节全文】，按照下列要求生成总计 **400-500 字** 的详细章节分析，并 **仅以 JSON 格式** 返回结果（严禁输出任何解释或 Markdown）。\n",
    "\n",
    "# JSON 顶层字段与要求：\n",
    "# {\n",
    "#   \"情节摘要导语\": \"<400 字，概括该章节的主要内容,请尽量把剧情讲述完整>\",\n",
    "#   \"出现人物\": [\"角色A\", \"角色B\", ...],\n",
    "#   \"出现道具\": [\"道具1\", \"道具2\", ...],\n",
    "#   \"出现场景\": [\"场景1\", \"场景2\"],\n",
    "#   \"伏笔_设下\": [\"伏笔1 描述\", \"伏笔2 描述\"],\n",
    "#   \"伏笔_回收\": [\"伏笔A 回收方式\", ...]\n",
    "# }\n",
    "\n",
    "# 严格要求：\n",
    "# 1. 文本字段须为完整中文段落；字数必须落在区间内。\n",
    "# 2. 上述 6 个键一个都不能少，也不能多。\n",
    "# 3. “出现人物 / 道具 / 场景 / 伏笔” 用 JSON 数组。\n",
    "# 4. 可引用原文≤10%，需加引号注明段落号。\n",
    "# 【章节全文】：\n",
    "# \"\"\"\n",
    "\n",
    "# # —— 分析单章（使用工具函数 + 强制调用） ——\n",
    "# def analyze_chapter(path: Path, prompt: str, retries: int = 3) -> str:\n",
    "#     text = _auto_decode(path)\n",
    "#     base_prompt = f\"{prompt.strip()}\\n\\n#TAG:{_rand_tag()}\"\n",
    "\n",
    "#     for attempt in range(retries):\n",
    "#         try:\n",
    "#             rsp = gemini_model.generate_content(\n",
    "#                 [base_prompt, text],\n",
    "#                 generation_config={\"temperature\": 0.3, \"max_output_tokens\": 8096},\n",
    "#                 tools=[{\n",
    "#                     \"function_declarations\": [\n",
    "#                         {\n",
    "#                             \"name\": \"analyze_chapter\",\n",
    "#                             \"description\": \"提取小说章节结构化信息（摘要、人物、道具、场景、伏笔）\",\n",
    "#                             \"parameters\": json_schema\n",
    "#                         }\n",
    "#                     ]\n",
    "#                 }],\n",
    "#                 tool_choice={\"function_call\": {\"name\": \"analyze_chapter\"}}  # ✅ 强制调用\n",
    "#             )\n",
    "#             fc = rsp.candidates[0].content.parts[0].function_call\n",
    "#             return json.dumps(fc.args, ensure_ascii=False, indent=2)\n",
    "#         except Exception as e:\n",
    "#             logging.warning(f\"Gemini 错误重试: {e}\")\n",
    "#             time.sleep(2 ** attempt)\n",
    "\n",
    "#     raise RuntimeError(f\"❌ 分析失败：{path.name}\")\n",
    "\n",
    "# # —— 批量处理目录下所有章节 ——\n",
    "# def run_analysis(\n",
    "#     chapter_root: str,\n",
    "#     prompt: str,\n",
    "#     out_dir: str,\n",
    "#     mode: tuple[int, int] = (1, 150)\n",
    "# ):\n",
    "#     root = Path(chapter_root)\n",
    "#     out_base = Path(out_dir)\n",
    "#     out_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     books = [d for d in root.iterdir() if d.is_dir()]\n",
    "#     s, e = mode\n",
    "\n",
    "#     for book in tqdm(books, desc=\"📚 小说书目\"):\n",
    "#         chapters = sorted([p for p in book.glob(\"*.txt\")\n",
    "#                            if s <= int(p.stem[:3]) <= e])\n",
    "#         out_book_dir = out_base / book.name\n",
    "#         out_book_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         for chap in tqdm(chapters, desc=f\"📖 {book.name}\"):\n",
    "#             out_path = out_book_dir / f\"{chap.stem}_processed.txt\"\n",
    "#             if out_path.exists(): continue\n",
    "#             try:\n",
    "#                 result = analyze_chapter(chap, prompt)\n",
    "#                 out_path.write_text(result, encoding=\"utf-8\")\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(f\"❌ {chap.name} 分析失败: {e}\")\n",
    "\n",
    "#     print(f\"\\n✅ 所有章节结构化完成，结果保存在：{out_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297d2ce-c93b-4817-9601-e8433c8616a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) 分析 200–250 字版本（可用于中短章）\n",
    "# run_analysis(\n",
    "#     chapter_root=\"/content/novels_chapters\",\n",
    "#     prompt=prompt_chapter_json.replace(\"400-500\", \"200-250\").replace(\"<400\", \"<200\"),\n",
    "#     out_dir=\"/content/json_results/50000_json_gemini\"\n",
    "# )\n",
    "\n",
    "# # 2) 分析 400–500 字版本（标准分析）\n",
    "# run_analysis(\n",
    "#     chapter_root=\"/content/novels_chapters\",\n",
    "#     prompt=prompt_chapter_json,\n",
    "#     out_dir=\"/content/json_results/100000_json_gemini\"\n",
    "# )\n",
    "\n",
    "# # 3) 分析 700–800 字版本（超长章节）\n",
    "# run_analysis(\n",
    "#     chapter_root=\"/content/novels_chapters\",\n",
    "#     prompt=prompt_chapter_json.replace(\"400-500\", \"700-800\").replace(\"<400\", \"<700\"),\n",
    "#     out_dir=\"/content/json_results/200000_json_gemini\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11bbbc9-58be-42f2-967f-73907e4d2d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0690200-4feb-45ca-9888-458057c0c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================\n",
    "# # ✅ Gemini 分析小说章节，结构化输出 JSON（强制调用函数 + 可变摘要长度 + JSON修复 + 硬编码Key）\n",
    "# # =============================\n",
    "# # !pip install -q --upgrade google-generativeai chardet tqdm\n",
    "# # !pip install -U google-generativeai # 确保安装最新版本\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import re\n",
    "# import time\n",
    "# import random\n",
    "# import string\n",
    "# import chardet\n",
    "# import logging\n",
    "# import copy  # 导入 copy\n",
    "# from pathlib import Path\n",
    "# from typing import List, Dict, Any\n",
    "# from tqdm.auto import tqdm\n",
    "# import google.generativeai as genai\n",
    "# from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "# # —— API 初始化 (⚠️ 警告：直接写入 API 密钥极不安全！) ——\n",
    "# # 强烈建议您使用环境变量或 Secrets Manager 等更安全的方式管理 API 密钥。\n",
    "# # 直接将密钥写入代码会带来严重的安全风险，尤其是在共享或版本控制代码时。\n",
    "\n",
    "# # ⚠️ 请将下面的 \"YOUR_API_KEY_HERE\" 替换为您真实的 Gemini API 密钥！\n",
    "# api_key = \"your-default-api-key\"\n",
    "\n",
    "# # 基本检查，确保用户替换了占位符\n",
    "# if not api_key or api_key == \"YOUR_API_KEY_HERE\":\n",
    "#     raise ValueError(\"❌ 错误：请务必将代码中的 'YOUR_API_KEY_HERE' 替换为您的真实 Gemini API 密钥。\")\n",
    "\n",
    "# # 使用直接写入的密钥配置 SDK\n",
    "# try:\n",
    "#     genai.configure(api_key=api_key)\n",
    "#     logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\") # Setup logging after configure\n",
    "#     logging.info(\"Gemini API 已使用直接写入的密钥进行配置。\")\n",
    "# except Exception as e:\n",
    "#     # 如果配置失败，提前记录错误并退出\n",
    "#     # logging 可能尚未完全配置，尝试打印错误\n",
    "#     print(f\"CRITICAL: 使用提供的 API 密钥配置 Gemini SDK 时出错: {e}\")\n",
    "#     raise ValueError(f\"API 密钥配置失败: {e}\")\n",
    "\n",
    "# # 使用推荐的最新 Flash 模型\n",
    "# GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "# # —— 通用函数 ——\n",
    "# def _auto_decode(path: Path) -> str:\n",
    "#     \"\"\"自动检测文件编码并读取内容\"\"\"\n",
    "#     try:\n",
    "#         raw = path.read_bytes()\n",
    "#         enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "#         # 增加对常见中文编码的尝试\n",
    "#         if enc.lower() not in ['utf-8', 'gbk', 'gb2312', 'big5']:\n",
    "#              try: return raw.decode('utf-8', errors='ignore').strip()\n",
    "#              except UnicodeDecodeError:\n",
    "#                  try: return raw.decode('gbk', errors='ignore').strip()\n",
    "#                  except UnicodeDecodeError: return raw.decode(enc, errors='ignore').strip() # 最后尝试 chardet 的结果\n",
    "#         return raw.decode(enc, errors=\"ignore\").strip()\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"读取文件 {path.name} 时出错: {e}\")\n",
    "#         return \"\" # 返回空字符串，让后续处理知道失败\n",
    "\n",
    "# def _rand_tag(k=6):\n",
    "#     \"\"\"生成随机标签（可选）\"\"\"\n",
    "#     return ''.join(random.choices(string.ascii_uppercase, k=k))\n",
    "\n",
    "# # —— 基础 JSON 输出字段要求 ——\n",
    "# base_json_schema = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"情节摘要导语\": {\"type\": \"string\", \"description\": \"\"}, # 描述将在调用时动态设置\n",
    "#         \"出现人物\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章出现的所有人物名称列表\"},\n",
    "#         \"出现道具\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章出现的关键道具列表\"},\n",
    "#         \"出现场景\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章发生故事的主要场景列表\"},\n",
    "#         \"伏笔_设下\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章新埋下的伏笔描述\"},\n",
    "#         \"伏笔_回收\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章回收或呼应的过往伏笔描述\"}\n",
    "#     },\n",
    "#     \"required\": [\"情节摘要导语\", \"出现人物\", \"出现道具\", \"出现场景\", \"伏笔_设下\", \"伏笔_回收\"]\n",
    "# }\n",
    "\n",
    "# # —— Prompt 模板 (强调遵循参数描述) ——\n",
    "# prompt_chapter_template = r\"\"\"\n",
    "# 你是一位专业的文学编辑。请仔细阅读我提供的【章节全文】。\n",
    "# 你的任务是提取结构化信息，并 **必须** 调用 `extract_chapter_details` 函数来返回结果。\n",
    "# 请严格按照函数参数的描述（特别是关于“情节摘要导语”的详细程度要求）来填充信息。\n",
    "# **绝对不要** 输出任何 JSON 格式之外的文本、解释、代码块标记（如 ```json ... ```）或 Markdown。\n",
    "# 直接调用函数并填充其参数。\n",
    "\n",
    "# 【章节全文】：\n",
    "# {chapter_text}\n",
    "# \"\"\"\n",
    "\n",
    "# # —— 分析单章（接受 summary_description，包含 JSON 修复） ——\n",
    "# def analyze_chapter(\n",
    "#     path: Path,\n",
    "#     prompt_template: str,\n",
    "#     summary_description: str, # 具体的摘要要求\n",
    "#     retries: int = 3\n",
    "# ) -> str | None:\n",
    "#     \"\"\"\n",
    "#     使用 Gemini 分析单个小说章节文件，强制调用函数并返回 JSON 字符串。\n",
    "#     允许通过 summary_description 指定摘要的详细程度。\n",
    "#     如果分析失败，则返回 None。\n",
    "#     \"\"\"\n",
    "#     text = _auto_decode(path)\n",
    "#     if not text:\n",
    "#         logging.error(f\"无法读取或解码文件: {path.name}\")\n",
    "#         return None\n",
    "\n",
    "#     full_prompt = prompt_template.format(chapter_text=text)\n",
    "\n",
    "#     # --- 动态构建函数声明 ---\n",
    "#     current_schema = copy.deepcopy(base_json_schema)\n",
    "#     current_schema[\"properties\"][\"情节摘要导语\"][\"description\"] = summary_description\n",
    "#     extract_details_func_declaration = {\n",
    "#         \"name\": \"extract_chapter_details\",\n",
    "#         \"description\": \"提取小说章节的结构化信息，包括情节摘要、人物、道具、场景和伏笔。\",\n",
    "#         \"parameters\": current_schema\n",
    "#     }\n",
    "#     # --- 动态构建结束 ---\n",
    "\n",
    "#     safety_settings = {\n",
    "#         HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "#         HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "#         HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "#         HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "#     }\n",
    "#     # 注意：如果 API Key 无效，此处也可能抛出异常，尽管 configure 时已检查\n",
    "#     try:\n",
    "#         gemini_model = genai.GenerativeModel(\n",
    "#             GEMINI_MODEL,\n",
    "#             safety_settings=safety_settings\n",
    "#         )\n",
    "#     except Exception as model_init_err:\n",
    "#         logging.error(f\"初始化 Gemini 模型时出错: {model_init_err}\", exc_info=True)\n",
    "#         return None # 无法初始化模型，直接失败\n",
    "\n",
    "#     # --- 重试循环 ---\n",
    "#     for attempt in range(retries):\n",
    "#         func_call_args_raw = None # 用于在出错时记录原始参数\n",
    "#         try:\n",
    "#             logging.info(f\"开始分析章节: {path.name} (摘要要求: '{summary_description}', 尝试 {attempt + 1}/{retries})\")\n",
    "#             rsp = gemini_model.generate_content(\n",
    "#                 full_prompt,\n",
    "#                 generation_config={\"temperature\": 0.3},\n",
    "#                 tools=[{\"function_declarations\": [extract_details_func_declaration]}],\n",
    "#                 tool_config={'function_calling_config': 'ANY'}\n",
    "#             )\n",
    "\n",
    "#             # --- 健壮性检查 ---\n",
    "#             if not rsp.candidates:\n",
    "#                  logging.warning(f\"分析失败 (尝试 {attempt + 1}): API 未返回候选内容。响应: {rsp}\")\n",
    "#                  time.sleep(2 ** attempt + random.uniform(0, 1))\n",
    "#                  continue\n",
    "\n",
    "#             first_candidate = rsp.candidates[0]\n",
    "#             if not first_candidate.content or not first_candidate.content.parts:\n",
    "#                 # 检查是否有 block 是因为安全设置\n",
    "#                 if first_candidate.finish_reason == genai.types.FinishReason.SAFETY:\n",
    "#                      logging.warning(f\"分析失败 (尝试 {attempt + 1}): 内容被安全设置阻止。安全评级: {first_candidate.safety_ratings}\")\n",
    "#                 else:\n",
    "#                      logging.warning(f\"分析失败 (尝试 {attempt + 1}): 候选内容为空或无 Part。完成原因: {first_candidate.finish_reason}\")\n",
    "#                 time.sleep(2 ** attempt + random.uniform(0, 1))\n",
    "#                 continue\n",
    "\n",
    "#             # --- 查找函数调用 ---\n",
    "#             func_call_part = None\n",
    "#             for part in first_candidate.content.parts:\n",
    "#                 if part.function_call:\n",
    "#                     func_call_part = part\n",
    "#                     break\n",
    "\n",
    "#             if func_call_part and func_call_part.function_call:\n",
    "#                 fc = func_call_part.function_call\n",
    "#                 func_call_args_raw = fc.args # 保存原始参数以备调试\n",
    "\n",
    "#                 if fc.name == \"extract_chapter_details\":\n",
    "#                     logging.info(f\"成功分析章节: {path.name} (摘要要求: '{summary_description}')\")\n",
    "\n",
    "#                   # --- FIX v2: Handle Nested RepeatedComposite ---\n",
    "#                     try:\n",
    "#                         # 1. Convert top-level MapComposite to dict (shallow)\n",
    "#                         args_dict_shallow = dict(fc.args)\n",
    "\n",
    "#                         # 2. Create a new dict for fully native Python types\n",
    "#                         args_dict_native = {}\n",
    "#                         for key, value in args_dict_shallow.items():\n",
    "#                             # Check if the value is the problematic list type\n",
    "#                             # Using type().__name__ is slightly brittle but targets the known issue.\n",
    "#                             # isinstance() might be better if we knew the exact class path.\n",
    "#                             if type(value).__name__ == 'RepeatedComposite':\n",
    "#                                 # Convert RepeatedComposite to a standard Python list\n",
    "#                                 # This assumes the items *within* the list are already native types\n",
    "#                                 # (like strings, which matches your schema: \"items\": {\"type\": \"string\"})\n",
    "#                                 args_dict_native[key] = list(value)\n",
    "#                             else:\n",
    "#                                 # Assume other types (like the string for '情节摘要导语') are already native or serializable\n",
    "#                                 args_dict_native[key] = value\n",
    "\n",
    "#                         # 3. Now serialize the deeply converted dict\n",
    "#                         json_output = json.dumps(args_dict_native, ensure_ascii=False, indent=2)\n",
    "#                         return json_output # Success\n",
    "\n",
    "#                     except TypeError as json_err:\n",
    "#                         # Catch potential errors during the deeper conversion or final serialization\n",
    "#                         logging.error(f\"序列化从API接收的参数时出错: {json_err}\", exc_info=False)\n",
    "#                         logging.error(f\"  未能序列化的原始参数类型: {type(fc.args)}\")\n",
    "#                         # Log the shallow dict as well, as it might show the structure better\n",
    "#                         logging.error(f\"  浅层转换后的字典内容 (部分): {str(args_dict_shallow)[:500]}...\")\n",
    "#                         # Continue to the next retry attempt\n",
    "#                         time.sleep(2 ** attempt + random.uniform(0, 1))\n",
    "#                         continue # Important: go to next retry if serialization fails\n",
    "#                     # --- END FIX v2 ---\n",
    "#                     # --- END FIX ---\n",
    "\n",
    "#                 else:\n",
    "#                     logging.warning(f\"分析警告 (尝试 {attempt + 1}): 模型调用了意外的函数 '{fc.name}'\")\n",
    "#             else:\n",
    "#                 # 模型未调用函数\n",
    "#                 finish_reason = first_candidate.finish_reason\n",
    "#                 safety_ratings = first_candidate.safety_ratings\n",
    "#                 logging.warning(f\"分析失败 (尝试 {attempt + 1}): 未找到预期的函数调用。\")\n",
    "#                 logging.warning(f\"  完成原因: {finish_reason}\")\n",
    "#                 if safety_ratings: # 仅当存在时打印\n",
    "#                     logging.warning(f\"  安全评级: {safety_ratings}\")\n",
    "#                 # 如果有文本输出，也记录下来帮助调试\n",
    "#                 text_output = \"\"\n",
    "#                 try:\n",
    "#                     text_output = first_candidate.text\n",
    "#                 except ValueError: # 有时访问 .text 会出错如果内容不是文本\n",
    "#                      if first_candidate.content and first_candidate.content.parts:\n",
    "#                          text_output = str(first_candidate.content.parts[0]) # 尝试获取原始部分\n",
    "#                 if text_output:\n",
    "#                      logging.warning(f\"  模型返回内容 (部分): {text_output[:200]}...\") # 只记录部分文本\n",
    "\n",
    "#             # 如果代码执行到这里，表示当前尝试失败，准备下一次重试\n",
    "#             time.sleep(2 ** attempt + random.uniform(0, 1))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             # 捕获所有其他在 API 调用或处理期间的异常\n",
    "#             error_context = \"\"\n",
    "#             if func_call_args_raw is not None: # 如果出错前已获取参数\n",
    "#                  error_context = f\" | 参数类型: {type(func_call_args_raw)}, 内容 (部分): {str(func_call_args_raw)[:200]}...\"\n",
    "\n",
    "#             # 检查是否是特定的 API 错误类型 (可选，需要 import)\n",
    "#             # from google.api_core.exceptions import GoogleAPIError\n",
    "#             # if isinstance(e, GoogleAPIError): ...\n",
    "\n",
    "#             logging.error(f\"Gemini API 调用或处理时发生异常 (尝试 {attempt + 1}): {e}{error_context}\", exc_info=True) # 记录完整堆栈\n",
    "#             time.sleep(2 ** attempt + random.uniform(0, 1)) # 等待后重试\n",
    "\n",
    "#     # --- 重试结束 ---\n",
    "#     logging.error(f\"❌ 分析失败，已达最大重试次数: {path.name} (摘要要求: '{summary_description}')\")\n",
    "#     return None # 所有重试失败后返回 None\n",
    "\n",
    "\n",
    "# # —— 批量处理目录下所有章节 (接受 summary_description) ——\n",
    "# def run_analysis(\n",
    "#     chapter_root: str,\n",
    "#     prompt_template: str,\n",
    "#     summary_description: str, # 摘要要求\n",
    "#     out_dir: str,\n",
    "#     mode: tuple[int, int] = (1, 150) # 处理章节范围\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     批量处理指定目录下的所有小说章节。\n",
    "#     \"\"\"\n",
    "#     root = Path(chapter_root)\n",
    "#     if not root.is_dir():\n",
    "#         logging.error(f\"错误：输入目录 '{chapter_root}' 不存在或不是一个目录。\")\n",
    "#         return\n",
    "\n",
    "#     out_base = Path(out_dir)\n",
    "#     out_base.mkdir(parents=True, exist_ok=True)\n",
    "#     logging.info(f\"开始批量分析 (摘要要求: '{summary_description}'), 输出到: {out_base}\")\n",
    "\n",
    "#     books = [d for d in root.iterdir() if d.is_dir()]\n",
    "#     if not books:\n",
    "#         logging.warning(f\"在 '{chapter_root}' 下未找到任何小说子目录。\")\n",
    "#         return\n",
    "\n",
    "#     s, e = mode\n",
    "#     logging.info(f\"处理章节范围: {s} 到 {e}\")\n",
    "\n",
    "#     total_processed = 0\n",
    "#     total_failed = 0\n",
    "#     total_skipped = 0\n",
    "\n",
    "#     for book in tqdm(books, desc=\"📚 处理小说书目\"):\n",
    "#         # 查找符合命名规范（以3位数字开头）且在范围内的 txt 文件\n",
    "#         chapters = sorted([\n",
    "#             p for p in book.glob(\"*.txt\")\n",
    "#             if p.name[:3].isdigit() and s <= int(p.name[:3]) <= e\n",
    "#         ], key=lambda p: int(p.name[:3])) # 按数字排序\n",
    "\n",
    "#         if not chapters:\n",
    "#             logging.warning(f\"在 '{book.name}' 目录中未找到符合范围 {s}-{e} 的章节文件 (例如 '001_xxx.txt')。\")\n",
    "#             continue\n",
    "\n",
    "#         out_book_dir = out_base / book.name\n",
    "#         out_book_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         processed_count = 0\n",
    "#         failed_count = 0\n",
    "#         skipped_count = 0\n",
    "\n",
    "#         for chap_path in tqdm(chapters, desc=f\"📖 分析 '{book.name}'\", leave=False):\n",
    "#             # 输出文件名使用原始章节名 + '_analysis.json'\n",
    "#             out_filename = f\"{chap_path.stem}_analysis.json\"\n",
    "#             out_path = out_book_dir / out_filename\n",
    "\n",
    "#             if out_path.exists():\n",
    "#                 skipped_count += 1\n",
    "#                 continue # 如果已存在，则跳过\n",
    "\n",
    "#             try:\n",
    "#                 # 调用分析函数，传入 summary_description\n",
    "#                 result_json = analyze_chapter(chap_path, prompt_template, summary_description)\n",
    "\n",
    "#                 if result_json:\n",
    "#                     out_path.write_text(result_json, encoding=\"utf-8\")\n",
    "#                     processed_count += 1\n",
    "#                 else:\n",
    "#                     # analyze_chapter 内部已记录错误，这里只计数\n",
    "#                     failed_count += 1\n",
    "#             except Exception as e:\n",
    "#                 # 捕获 analyze_chapter 未处理的意外错误\n",
    "#                 logging.error(f\"处理章节 {chap_path.name} 时发生意外错误: {e}\", exc_info=True)\n",
    "#                 failed_count += 1\n",
    "\n",
    "#         logging.info(f\"完成处理 '{book.name}': {processed_count} 个成功, {failed_count} 个失败, {skipped_count} 个跳过。\")\n",
    "#         total_processed += processed_count\n",
    "#         total_failed += failed_count\n",
    "#         total_skipped += skipped_count\n",
    "\n",
    "#     print(f\"\\n✅ 所有章节结构化分析完成（或尝试完成）。\")\n",
    "#     print(f\"  总计: {total_processed} 个成功, {total_failed} 个失败, {total_skipped} 个跳过。\")\n",
    "#     print(f\"  结果保存在对应的输出子目录中，根目录为：{OUTPUT_BASE_DIR}\") # 指向总目录\n",
    "\n",
    "\n",
    "# # --- 主程序入口 ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 定义输入目录 (★★★ 根据你的实际情况修改 ★★★)\n",
    "#     CHAPTERS_INPUT_DIR = \"/content/novels_chapters\"\n",
    "#     # 定义输出根目录 (★★★ 根据你的实际情况修改 ★★★)\n",
    "#     OUTPUT_BASE_DIR = \"/content/json_results/json_gemini\"\n",
    "\n",
    "#     # --- 定义不同摘要长度的要求 ---\n",
    "#     summary_req_short = \"生成一个非常简短的核心情节摘要（ 约 150-200 字）\"\n",
    "#     summary_req_medium = \"生成一个标准的情节摘要，概括主要内容（约 300-400 字）\"\n",
    "#     summary_req_long = \"生成一个比较详细的情节摘要，包含更多细节和转折（约 600-700 字）\"\n",
    "\n",
    "#     # --- 运行不同版本的分析 ---\n",
    "#     # (你可以取消注释掉不想运行的版本)\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始简短摘要分析 \" + \"=\"*20)\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_short, # <---- 传入简短要求\n",
    "#         out_dir=os.path.join(OUTPUT_BASE_DIR, \"50000_summary\"), # 输出到子目录\n",
    "#         mode=(1, 150) # 分析 1 到 150 章\n",
    "#     )\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始标准摘要分析 \" + \"=\"*20)\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_medium, # <---- 传入标准要求\n",
    "#         out_dir=os.path.join(OUTPUT_BASE_DIR, \"100000_summary\"), # 输出到子目录\n",
    "#         mode=(1, 150)\n",
    "#     )\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始详细摘要分析 \" + \"=\"*20)\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_long, # <---- 传入详细要求\n",
    "#         out_dir=os.path.join(OUTPUT_BASE_DIR, \"200000_summary\"), # 输出到子目录\n",
    "#         mode=(1, 150)\n",
    "#     )\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 所有不同摘要长度的分析任务已提交 \" + \"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5ee3cf-0fc7-4aba-ab0e-74e899ac3714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Gemini API 已使用直接写入的密钥进行配置。\n"
     ]
    }
   ],
   "source": [
    "# # =============================\n",
    "# # ✅ Gemini 分析小说章节，结构化输出 JSON + TXT细纲生成 (主程序参数更新)\n",
    "# # =============================\n",
    "# # !pip install -q --upgrade google-generativeai chardet tqdm\n",
    "# # !pip install -U google-generativeai # 确保安装最新版本\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import re\n",
    "# import time\n",
    "# import random\n",
    "# import string\n",
    "# import chardet\n",
    "# import logging\n",
    "# import copy\n",
    "# from pathlib import Path\n",
    "# from typing import List, Dict, Any, Tuple\n",
    "# from tqdm.auto import tqdm\n",
    "# import google.generativeai as genai\n",
    "# from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "# # --- (API 初始化, 通用函数, Schema, Prompt 模板 - 与上一版本相同) ---\n",
    "\n",
    "# # —— API 初始化 (⚠️ 警告：直接写入 API 密钥极不安全！) ——\n",
    "# api_key = \"your-default-api-key\" # ⚠️ 替换为您的真实 API 密钥\n",
    "# if not api_key or api_key == \"YOUR_API_KEY_HERE\":\n",
    "#     raise ValueError(\"❌ 错误：请务必将代码中的 'YOUR_API_KEY_HERE' 替换为您的真实 Gemini API 密钥。\")\n",
    "# try:\n",
    "#     genai.configure(api_key=api_key)\n",
    "#     logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "#     logging.info(\"Gemini API 已使用直接写入的密钥进行配置。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"CRITICAL: 使用提供的 API 密钥配置 Gemini SDK 时出错: {e}\")\n",
    "#     raise ValueError(f\"API 密钥配置失败: {e}\")\n",
    "\n",
    "# GEMINI_MODEL = \"gemini-2.0-flash-latest\"\n",
    "\n",
    "# def _auto_decode(path: Path) -> str:\n",
    "#     try:\n",
    "#         raw = path.read_bytes()\n",
    "#         enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "#         if enc.lower() not in ['utf-8', 'gbk', 'gb2312', 'big5']:\n",
    "#              try: return raw.decode('utf-8', errors='ignore').strip()\n",
    "#              except UnicodeDecodeError:\n",
    "#                  try: return raw.decode('gbk', errors='ignore').strip()\n",
    "#                  except UnicodeDecodeError: return raw.decode(enc, errors='ignore').strip()\n",
    "#         return raw.decode(enc, errors=\"ignore\").strip()\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"读取文件 {path.name} 时出错: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# def _rand_tag(k=6): return ''.join(random.choices(string.ascii_uppercase, k=k))\n",
    "\n",
    "# base_json_schema = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"情节摘要导语\": {\"type\": \"string\", \"description\": \"\"},\n",
    "#         \"出现人物\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章出现的所有人物名称列表\"},\n",
    "#         \"出现道具\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章出现的关键道具列表\"},\n",
    "#         \"出现场景\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章发生故事的主要场景列表\"},\n",
    "#         \"伏笔_设下\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章新埋下的伏笔描述\"},\n",
    "#         \"伏笔_回收\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章回收或呼应的过往伏笔描述\"}\n",
    "#     },\n",
    "#     \"required\": [\"情节摘要导语\", \"出现人物\", \"出现道具\", \"出现场景\", \"伏笔_设下\", \"伏笔_回收\"]\n",
    "# }\n",
    "\n",
    "# prompt_chapter_template = r\"\"\"\n",
    "# 你是一位专业的文学编辑。请仔细阅读我提供的【章节全文】。\n",
    "# 你的任务是提取结构化信息，并 **必须** 调用 `extract_chapter_details` 函数来返回结果。\n",
    "# 请严格按照函数参数的描述（特别是关于“情节摘要导语”的详细程度要求）来填充信息。\n",
    "# **绝对不要** 输出任何 JSON 格式之外的文本、解释、代码块标记（如 ```json ... ```）或 Markdown。\n",
    "# 直接调用函数并填充其参数。\n",
    "\n",
    "# 【章节全文】：\n",
    "# {chapter_text}\n",
    "# \"\"\"\n",
    "\n",
    "# # --- (analyze_chapter 函数 - 与上一版本修复后相同) ---\n",
    "# def analyze_chapter(\n",
    "#     path: Path,\n",
    "#     prompt_template: str,\n",
    "#     summary_description: str,\n",
    "#     retries: int = 3\n",
    "# ) -> str | None:\n",
    "#     text = _auto_decode(path)\n",
    "#     if not text: return None\n",
    "#     full_prompt = prompt_template.format(chapter_text=text)\n",
    "#     current_schema = copy.deepcopy(base_json_schema)\n",
    "#     current_schema[\"properties\"][\"情节摘要导语\"][\"description\"] = summary_description\n",
    "#     extract_details_func_declaration = {\n",
    "#         \"name\": \"extract_chapter_details\",\n",
    "#         \"description\": \"提取小说章节的结构化信息...\",\n",
    "#         \"parameters\": current_schema\n",
    "#     }\n",
    "#     safety_settings = { HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, }\n",
    "#     try:\n",
    "#         gemini_model = genai.GenerativeModel(GEMINI_MODEL, safety_settings=safety_settings)\n",
    "#     except Exception as model_init_err:\n",
    "#         logging.error(f\"初始化 Gemini 模型时出错: {model_init_err}\", exc_info=True)\n",
    "#         return None\n",
    "\n",
    "#     for attempt in range(retries):\n",
    "#         func_call_args_raw = None\n",
    "#         try:\n",
    "#             logging.info(f\"开始分析章节: {path.name} (摘要要求: '{summary_description}', 尝试 {attempt + 1}/{retries})\")\n",
    "#             rsp = gemini_model.generate_content(full_prompt, generation_config={\"temperature\": 0.3}, tools=[{\"function_declarations\": [extract_details_func_declaration]}], tool_config={'function_calling_config': 'ANY'})\n",
    "#             if not rsp.candidates: logging.warning(f\"分析失败 (尝试 {attempt + 1}): API 未返回候选内容。\"); time.sleep(2 ** attempt + random.uniform(0, 1)); continue\n",
    "#             first_candidate = rsp.candidates[0]\n",
    "#             if not first_candidate.content or not first_candidate.content.parts:\n",
    "#                 if first_candidate.finish_reason == genai.types.FinishReason.SAFETY: logging.warning(f\"分析失败 (尝试 {attempt + 1}): 内容被安全设置阻止。\")\n",
    "#                 else: logging.warning(f\"分析失败 (尝试 {attempt + 1}): 候选内容为空。原因: {first_candidate.finish_reason}\")\n",
    "#                 time.sleep(2 ** attempt + random.uniform(0, 1)); continue\n",
    "#             func_call_part = None\n",
    "#             for part in first_candidate.content.parts:\n",
    "#                 if part.function_call: func_call_part = part; break\n",
    "#             if func_call_part and func_call_part.function_call:\n",
    "#                 fc = func_call_part.function_call\n",
    "#                 func_call_args_raw = fc.args\n",
    "#                 if fc.name == \"extract_chapter_details\":\n",
    "#                     logging.info(f\"成功分析章节: {path.name} (摘要要求: '{summary_description}')\")\n",
    "#                     try:\n",
    "#                         args_dict_shallow = dict(fc.args)\n",
    "#                         args_dict_native = {}\n",
    "#                         for key, value in args_dict_shallow.items():\n",
    "#                             if type(value).__name__ == 'RepeatedComposite':\n",
    "#                                 args_dict_native[key] = list(value)\n",
    "#                             else:\n",
    "#                                 args_dict_native[key] = value\n",
    "#                         json_output = json.dumps(args_dict_native, ensure_ascii=False, indent=2)\n",
    "#                         return json_output\n",
    "#                     except TypeError as json_err:\n",
    "#                         logging.error(f\"序列化参数时出错: {json_err}\", exc_info=False)\n",
    "#                         logging.error(f\"  原始参数类型: {type(func_call_args_raw)}\")\n",
    "#                         logging.error(f\"  浅层字典 (部分): {str(args_dict_shallow)[:500]}...\")\n",
    "#                         time.sleep(2 ** attempt + random.uniform(0, 1)); continue # Go to next retry on serialization error\n",
    "#                 else: logging.warning(f\"分析警告 (尝试 {attempt + 1}): 调用了意外函数 '{fc.name}'\")\n",
    "#             else:\n",
    "#                 finish_reason = first_candidate.finish_reason; safety_ratings = first_candidate.safety_ratings\n",
    "#                 logging.warning(f\"分析失败 (尝试 {attempt + 1}): 未找到函数调用。原因: {finish_reason}\")\n",
    "#                 if safety_ratings: logging.warning(f\"  安全评级: {safety_ratings}\")\n",
    "#                 try: text_output = first_candidate.text\n",
    "#                 except ValueError: text_output = str(first_candidate.content.parts[0]) if first_candidate.content and first_candidate.content.parts else \"\"\n",
    "#                 if text_output: logging.warning(f\"  模型返回内容 (部分): {text_output[:200]}...\")\n",
    "#             time.sleep(2 ** attempt + random.uniform(0, 1)) # Wait before next retry if this attempt failed here\n",
    "#         except Exception as e:\n",
    "#             error_context = f\" | 参数类型: {type(func_call_args_raw)}, 内容 (部分): {str(func_call_args_raw)[:200]}...\" if func_call_args_raw else \"\"\n",
    "#             logging.error(f\"API 调用或处理时发生异常 (尝试 {attempt + 1}): {e}{error_context}\", exc_info=True)\n",
    "#             time.sleep(2 ** attempt + random.uniform(0, 1)) # Wait before next retry on general exception\n",
    "#     logging.error(f\"❌ 分析失败，已达最大重试次数: {path.name} (摘要要求: '{summary_description}')\")\n",
    "#     return None\n",
    "\n",
    "\n",
    "# # --- (run_analysis 函数 - 与上一版本相同) ---\n",
    "# def run_analysis(chapter_root: str, prompt_template: str, summary_description: str, out_dir: str, mode: tuple[int, int] = (1, 150)):\n",
    "#     root = Path(chapter_root); out_base = Path(out_dir)\n",
    "#     if not root.is_dir(): logging.error(f\"错误：输入目录 '{chapter_root}' 不存在。\"); return\n",
    "#     out_base.mkdir(parents=True, exist_ok=True)\n",
    "#     logging.info(f\"开始批量分析 (摘要要求: '{summary_description}'), 输出到: {out_base}\")\n",
    "#     books = [d for d in root.iterdir() if d.is_dir()]\n",
    "#     if not books: logging.warning(f\"在 '{chapter_root}' 下未找到任何小说子目录。\"); return\n",
    "#     s, e = mode; logging.info(f\"处理章节范围: {s} 到 {e}\")\n",
    "#     total_processed, total_failed, total_skipped = 0, 0, 0\n",
    "#     for book in tqdm(books, desc=\"📚 处理小说书目\"):\n",
    "#         chapters = sorted([p for p in book.glob(\"*.txt\") if p.name[:3].isdigit() and s <= int(p.name[:3]) <= e], key=lambda p: int(p.name[:3]))\n",
    "#         if not chapters: logging.warning(f\"在 '{book.name}' 目录中未找到符合范围 {s}-{e} 的章节文件。\"); continue\n",
    "#         out_book_dir = out_base / book.name; out_book_dir.mkdir(parents=True, exist_ok=True)\n",
    "#         processed_count, failed_count, skipped_count = 0, 0, 0\n",
    "#         for chap_path in tqdm(chapters, desc=f\"📖 分析 '{book.name}'\", leave=False):\n",
    "#             out_filename = f\"{chap_path.stem}_analysis.json\"; out_path = out_book_dir / out_filename\n",
    "#             if out_path.exists(): skipped_count += 1; continue\n",
    "#             try:\n",
    "#                 result_json = analyze_chapter(chap_path, prompt_template, summary_description)\n",
    "#                 if result_json: out_path.write_text(result_json, encoding=\"utf-8\"); processed_count += 1\n",
    "#                 else: failed_count += 1\n",
    "#             except Exception as e: logging.error(f\"处理章节 {chap_path.name} 时发生意外错误: {e}\", exc_info=True); failed_count += 1\n",
    "#         logging.info(f\"完成处理 '{book.name}': {processed_count} 成功, {failed_count} 失败, {skipped_count} 跳过。\")\n",
    "#         total_processed += processed_count; total_failed += failed_count; total_skipped += skipped_count\n",
    "#     # Ensure the final message regarding where results are saved uses the correct base directory variable\n",
    "#     print(f\"\\n✅ JSON 分析完成。总计: {total_processed} 成功, {total_failed} 失败, {total_skipped} 跳过。\")\n",
    "#     print(f\"  JSON 结果保存在: {out_dir}\") # Use out_dir specific to this run_analysis call\n",
    "\n",
    "\n",
    "# # --- (JSON 转 TXT 及合并功能 - format_list_output, convert_json_to_txt, merge_txt_outlines, run_post_processing - 与上一版本相同) ---\n",
    "\n",
    "# def format_list_output(items: List[str]) -> str:\n",
    "#     \"\"\"Helper function to format lists for TXT output.\"\"\"\n",
    "#     if not items: return \"- 无\"\n",
    "#     return \"\\n\".join(f\"- {item}\" for item in items)\n",
    "\n",
    "# def convert_json_to_txt(json_path: Path, txt_path: Path) -> bool:\n",
    "#     \"\"\"Reads JSON analysis file, writes formatted TXT outline.\"\"\"\n",
    "#     try:\n",
    "#         with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "#         chapter_title = json_path.stem.replace(\"_analysis\", \"\")\n",
    "#         output_lines = [\n",
    "#             f\"章节：{chapter_title}\", \"\\n【情节摘要导语】\", data.get(\"情节摘要导语\", \"N/A\"),\n",
    "#             \"\\n【出现人物】\", format_list_output(data.get(\"出现人物\", [])),\n",
    "#             \"\\n【出现道具】\", format_list_output(data.get(\"出现道具\", [])),\n",
    "#             \"\\n【出现场景】\", format_list_output(data.get(\"出现场景\", [])),\n",
    "#             \"\\n【伏笔_设下】\", format_list_output(data.get(\"伏笔_设下\", [])),\n",
    "#             \"\\n【伏笔_回收】\", format_list_output(data.get(\"伏笔_回收\", [])),\n",
    "#             \"\\n\" + \"-\" * 40 + \"\\n\"\n",
    "#         ]\n",
    "#         txt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         with open(txt_path, 'w', encoding='utf-8') as f: f.write(\"\\n\".join(output_lines))\n",
    "#         return True\n",
    "#     except FileNotFoundError: logging.error(f\"JSON 文件未找到: {json_path}\"); return False\n",
    "#     except json.JSONDecodeError: logging.error(f\"无法解析 JSON 文件: {json_path}\"); return False\n",
    "#     except Exception as e: logging.error(f\"转换 {json_path.name} 到 TXT 时发生意外错误: {e}\", exc_info=True); return False\n",
    "\n",
    "# def merge_txt_outlines(txt_dir: Path, output_file: Path) -> bool:\n",
    "#     \"\"\"Merges all chapter TXT outlines in a directory into a single file.\"\"\"\n",
    "#     try:\n",
    "#         chapter_files = list(txt_dir.glob(\"*_outline.txt\"))\n",
    "#         if not chapter_files: logging.warning(f\"在目录 {txt_dir} 中未找到要合并的 TXT 文件 (*_outline.txt)。\"); return False\n",
    "#         def get_chapter_num(file_path: Path) -> int:\n",
    "#             try: match = re.match(r\"(\\d+)\", file_path.name); return int(match.group(1)) if match else float('inf')\n",
    "#             except ValueError: return float('inf')\n",
    "#         chapter_files.sort(key=get_chapter_num)\n",
    "#         merged_content = []\n",
    "#         logging.info(f\"开始合并 {len(chapter_files)} 个 TXT 文件到 {output_file.name}...\")\n",
    "#         for chap_file in tqdm(chapter_files, desc=f\"  合并 TXT\", leave=False):\n",
    "#             try:\n",
    "#                 with open(chap_file, 'r', encoding='utf-8') as f: merged_content.append(f.read())\n",
    "#             except Exception as e: logging.error(f\"读取 TXT 文件 {chap_file.name} 时出错: {e}\")\n",
    "#         if not merged_content: logging.error(f\"未能读取任何 TXT 文件内容进行合并。\"); return False\n",
    "#         output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         with open(output_file, 'w', encoding='utf-8') as f: f.write(\"\".join(merged_content))\n",
    "#         logging.info(f\"成功合并 TXT 文件到: {output_file}\")\n",
    "#         return True\n",
    "#     except Exception as e: logging.error(f\"合并 TXT 文件到 {output_file.name} 时发生意外错误: {e}\", exc_info=True); return False\n",
    "\n",
    "# def run_post_processing(base_json_dir: str):\n",
    "#     \"\"\"Runs JSON-to-TXT conversion and merging for all books in a base directory.\"\"\"\n",
    "#     base_path = Path(base_json_dir)\n",
    "#     if not base_path.is_dir(): logging.warning(f\"跳过后期处理：目录 {base_path} 不存在。\"); return\n",
    "#     logging.info(f\"\\n{'='*20} 开始对 '{base_path.name}' 进行后期处理 (TXT 生成与合并) {'='*20}\")\n",
    "#     book_dirs = [d for d in base_path.iterdir() if d.is_dir()]\n",
    "#     if not book_dirs: logging.warning(f\"在 {base_path} 中未找到书籍子目录进行后期处理。\"); return\n",
    "#     total_books_processed, total_books_failed = 0, 0\n",
    "#     for book_json_dir in tqdm(book_dirs, desc=\"📚 处理书籍 (后期)\"):\n",
    "#         book_name = book_json_dir.name\n",
    "#         book_txt_outlines_dir = book_json_dir / f\"{book_name}_txt_outlines\"\n",
    "#         merged_output_file = base_path / f\"{book_name}_完整细纲.txt\" # Save merged file one level up\n",
    "#         logging.info(f\"处理书籍 '{book_name}': JSON={book_json_dir}, TXT={book_txt_outlines_dir}, Merged={merged_output_file}\")\n",
    "#         json_files = list(book_json_dir.glob(\"*_analysis.json\"))\n",
    "#         if not json_files: logging.warning(f\"  在 {book_json_dir} 中未找到 JSON 文件进行转换。\"); continue\n",
    "#         conversion_success_count, conversion_fail_count = 0, 0\n",
    "#         logging.info(f\"  开始转换 {len(json_files)} 个 JSON 文件到 TXT...\")\n",
    "#         for json_file in tqdm(json_files, desc=f\"  转换 JSON\", leave=False):\n",
    "#             txt_filename = json_file.stem.replace(\"_analysis\", \"_outline.txt\")\n",
    "#             txt_file_path = book_txt_outlines_dir / txt_filename\n",
    "#             if convert_json_to_txt(json_file, txt_file_path): conversion_success_count += 1\n",
    "#             else: conversion_fail_count += 1\n",
    "#         logging.info(f\"  JSON 到 TXT 转换完成: {conversion_success_count} 成功, {conversion_fail_count} 失败。\")\n",
    "#         if conversion_success_count == 0:\n",
    "#             logging.error(f\"  未能成功转换任何 JSON 文件为 TXT，跳过合并步骤。\"); total_books_failed += 1; continue\n",
    "#         if merge_txt_outlines(book_txt_outlines_dir, merged_output_file): total_books_processed += 1\n",
    "#         else: total_books_failed +=1; logging.error(f\"  未能成功合并 '{book_name}' 的 TXT 细纲。\")\n",
    "#     logging.info(f\"\\n{'='*20} '{base_path.name}' 后期处理完成 {'='*20}\")\n",
    "#     logging.info(f\"  成功生成完整细纲的书籍数量: {total_books_processed}\")\n",
    "#     logging.info(f\"  处理失败或未完成的书籍数量: {total_books_failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63534fe5-ddaa-44a2-a682-7d01076e0f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ========================================\n",
    "# # ==           主程序入口 (更新)        ==\n",
    "# # ========================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 定义输入目录 (★★★ 根据你的实际情况修改 ★★★)\n",
    "#     CHAPTERS_INPUT_DIR = \"/content/novels_normalized\"\n",
    "#     # 定义输出根目录 (★★★ 根据你的实际情况修改 ★★★)\n",
    "#     OUTPUT_BASE_DIR = \"/content/json_results/json_gemini_normalized\" # <--- 更新\n",
    "\n",
    "#     # --- 定义不同摘要长度的要求 (更新) ---\n",
    "#     summary_req_short = \"生成一个非常简短的核心情节摘要（ 约 150-200 字）\" # <--- 更新\n",
    "#     summary_req_medium = \"生成一个标准的情节摘要，概括主要内容（约 300-400 字）\" # <--- 更新\n",
    "#     summary_req_long = \"生成一个比较详细的情节摘要，包含更多细节和转折（约 600-700 字）\" # <--- 更新\n",
    "\n",
    "#     # --- 运行不同版本的分析 ---\n",
    "#     # (你可以取消注释掉不想运行的版本)\n",
    "#     analysis_dirs_to_process = [] # Store dirs for post-processing\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始简短摘要分析 (150-200字) \" + \"=\"*20)\n",
    "#     short_summary_dir = os.path.join(OUTPUT_BASE_DIR, \"30000_summary\") # <--- 更新\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_short, # 使用更新后的描述\n",
    "#         out_dir=short_summary_dir,             # 使用更新后的目录\n",
    "#         mode=(1, 200) # 分析 1 到 150 章\n",
    "#     )\n",
    "#     analysis_dirs_to_process.append(short_summary_dir) # 添加更新后的目录\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始标准摘要分析 (300-400字) \" + \"=\"*20)\n",
    "#     medium_summary_dir = os.path.join(OUTPUT_BASE_DIR, \"50000_summary\") # <--- 更新\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_medium, # 使用更新后的描述\n",
    "#         out_dir=medium_summary_dir,             # 使用更新后的目录\n",
    "#         mode=(1, 200)\n",
    "#     )\n",
    "#     analysis_dirs_to_process.append(medium_summary_dir) # 添加更新后的目录\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始详细摘要分析 (600-700字) \" + \"=\"*20)\n",
    "#     long_summary_dir = os.path.join(OUTPUT_BASE_DIR, \"100000_summary\") # <--- 更新\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_long, # 使用更新后的描述\n",
    "#         out_dir=long_summary_dir,             # 使用更新后的目录\n",
    "#         mode=(1, 200)\n",
    "#     )\n",
    "#     analysis_dirs_to_process.append(long_summary_dir) # 添加更新后的目录\n",
    "\n",
    "#     logging.info(\"\\n\" + \"=\"*20 + \" 所有 JSON 分析任务已完成/提交 \" + \"=\"*20)\n",
    "\n",
    "#     # --- 运行后期处理：生成 TXT 细纲 ---\n",
    "#     # (这部分保持不变，它会使用上面 analysis_dirs_to_process 列表中的新目录)\n",
    "#     logging.info(\"\\n\" + \"=\"*20 + \" 开始运行后期处理 (生成 TXT 细纲) \" + \"=\"*20)\n",
    "#     for dir_to_process in analysis_dirs_to_process:\n",
    "#         run_post_processing(dir_to_process)\n",
    "\n",
    "#     logging.info(\"\\n\" + \"=\"*20 + \" 全部处理流程结束 \" + \"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23063278-384a-45a7-9005-4dfd46c25485",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/json_results/json_gemini_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acef1755-6507-435c-afc4-890911d0f092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📚 处理书籍: 100%|██████████| 40/40 [00:00<00:00, 45.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 全部完成。概览：\n",
      "                           book  chapters  ok  bad                                                                                          out_file\n",
      "    《奋斗在新明朝》(校对版全本)作者_随轻风去_utf8       149 149    0     /content/json_results/json_gemini/30000_summary_gathered/《奋斗在新明朝》(校对版全本)作者_随轻风去_utf8/完整细纲.txt\n",
      "   《反正我是超能力者》(校对版全本)作者_吃书妖_utf8       150 150    0    /content/json_results/json_gemini/30000_summary_gathered/《反正我是超能力者》(校对版全本)作者_吃书妖_utf8/完整细纲.txt\n",
      "        《天可汗》(校对版全本)作者_西风紧_utf8       150 150    0         /content/json_results/json_gemini/30000_summary_gathered/《天可汗》(校对版全本)作者_西风紧_utf8/完整细纲.txt\n",
      "《崩坏世界的传奇大冒险》(精校版全本)作者_国王陛下_utf8       145 145    0 /content/json_results/json_gemini/30000_summary_gathered/《崩坏世界的传奇大冒险》(精校版全本)作者_国王陛下_utf8/完整细纲.txt\n",
      "        《全球进化》(精校版全本)作者_咬狗_utf8       150 150    0         /content/json_results/json_gemini/30000_summary_gathered/《全球进化》(精校版全本)作者_咬狗_utf8/完整细纲.txt\n",
      "      《武林半侠传》(校对版全本)作者_文抄公_utf8       150 150    0       /content/json_results/json_gemini/30000_summary_gathered/《武林半侠传》(校对版全本)作者_文抄公_utf8/完整细纲.txt\n",
      "                        国宴大厨在八零       659 659    0                         /content/json_results/json_gemini/30000_summary_gathered/国宴大厨在八零/完整细纲.txt\n",
      "       《搜神记》(精校版全本)作者_树下野狐_utf8       131 131    0        /content/json_results/json_gemini/30000_summary_gathered/《搜神记》(精校版全本)作者_树下野狐_utf8/完整细纲.txt\n",
      "                     重生八零_毒妻不好惹       149 149    0                      /content/json_results/json_gemini/30000_summary_gathered/重生八零_毒妻不好惹/完整细纲.txt\n",
      "    《窃明》(校对版全本)作者_大爆炸(灰熊猫)_utf8       143 143    0     /content/json_results/json_gemini/30000_summary_gathered/《窃明》(校对版全本)作者_大爆炸(灰熊猫)_utf8/完整细纲.txt\n",
      "       《蜀山》(精校版全本)作者_流浪的蛤蟆_utf8       141 141    0        /content/json_results/json_gemini/30000_summary_gathered/《蜀山》(精校版全本)作者_流浪的蛤蟆_utf8/完整细纲.txt\n",
      " 《陈二狗的妖孽人生》(校对版全本)作者_烽火戏诸侯_utf8       141 141    0  /content/json_results/json_gemini/30000_summary_gathered/《陈二狗的妖孽人生》(校对版全本)作者_烽火戏诸侯_utf8/完整细纲.txt\n",
      "        《贩罪》(精校版全本)作者_三天两觉_utf8       147 147    0         /content/json_results/json_gemini/30000_summary_gathered/《贩罪》(精校版全本)作者_三天两觉_utf8/完整细纲.txt\n",
      "  《重生之出人头地》(校对版全本)作者_闹闹不爱闹_utf8       149 149    0   /content/json_results/json_gemini/30000_summary_gathered/《重生之出人头地》(校对版全本)作者_闹闹不爱闹_utf8/完整细纲.txt\n",
      "                   八零喜事_当家肥妻大翻身       660 660    0                    /content/json_results/json_gemini/30000_summary_gathered/八零喜事_当家肥妻大翻身/完整细纲.txt\n",
      "        《肆虐韩娱》(校对版全本)作者_姬叉_utf8       146 146    0         /content/json_results/json_gemini/30000_summary_gathered/《肆虐韩娱》(校对版全本)作者_姬叉_utf8/完整细纲.txt\n",
      "                        八零年代好时光       436 436    0                         /content/json_results/json_gemini/30000_summary_gathered/八零年代好时光/完整细纲.txt\n",
      "   《食物链顶端的男人》(校对版全本)作者_熊狼狗_utf8       150 150    0    /content/json_results/json_gemini/30000_summary_gathered/《食物链顶端的男人》(校对版全本)作者_熊狼狗_utf8/完整细纲.txt\n",
      "     《高手寂寞2》(校对版全本)作者_兰帝魅晨_utf8       646 646    0      /content/json_results/json_gemini/30000_summary_gathered/《高手寂寞2》(校对版全本)作者_兰帝魅晨_utf8/完整细纲.txt\n",
      "        《黑龙法典》(校对版全本)作者_欢声_utf8       148 148    0         /content/json_results/json_gemini/30000_summary_gathered/《黑龙法典》(校对版全本)作者_欢声_utf8/完整细纲.txt\n",
      "          《诛仙》(校对版全本)作者_萧鼎_utf8       150 150    0           /content/json_results/json_gemini/30000_summary_gathered/《诛仙》(校对版全本)作者_萧鼎_utf8/完整细纲.txt\n",
      "   《回到过去变成猫》(精校版全本)作者_陈词懒调_utf8       149 149    0    /content/json_results/json_gemini/30000_summary_gathered/《回到过去变成猫》(精校版全本)作者_陈词懒调_utf8/完整细纲.txt\n",
      "       《神游》(校对版全本)作者_徐公子胜治_utf8       125 125    0        /content/json_results/json_gemini/30000_summary_gathered/《神游》(校对版全本)作者_徐公子胜治_utf8/完整细纲.txt\n",
      "   《老子是癞蛤蟆》(校对版全本)作者_烽火戏诸侯_utf8       150 150    0    /content/json_results/json_gemini/30000_summary_gathered/《老子是癞蛤蟆》(校对版全本)作者_烽火戏诸侯_utf8/完整细纲.txt\n",
      "      《未来天王》(校对版全本)作者_陈词懒调_utf8       150 150    0       /content/json_results/json_gemini/30000_summary_gathered/《未来天王》(校对版全本)作者_陈词懒调_utf8/完整细纲.txt\n",
      "         《大画家》(校对版全本)作者_醛石_utf8       150 150    0          /content/json_results/json_gemini/30000_summary_gathered/《大画家》(校对版全本)作者_醛石_utf8/完整细纲.txt\n",
      "       《人道天堂》(校对版全本)作者_荆柯守_utf8       148 148    0        /content/json_results/json_gemini/30000_summary_gathered/《人道天堂》(校对版全本)作者_荆柯守_utf8/完整细纲.txt\n",
      "           《超级惊悚直播》作者_宇文长弓_utf8       150 150    0            /content/json_results/json_gemini/30000_summary_gathered/《超级惊悚直播》作者_宇文长弓_utf8/完整细纲.txt\n",
      "      《道缘浮图》(校对版全本)作者_烟雨江南_utf8       150 150    0       /content/json_results/json_gemini/30000_summary_gathered/《道缘浮图》(校对版全本)作者_烟雨江南_utf8/完整细纲.txt\n",
      "         《重活了》(精校版全本)作者_尝谕_utf8       150 150    0          /content/json_results/json_gemini/30000_summary_gathered/《重活了》(精校版全本)作者_尝谕_utf8/完整细纲.txt\n",
      "        《雅骚》(校对版全本)作者_贼道三痴_utf8       150 150    0         /content/json_results/json_gemini/30000_summary_gathered/《雅骚》(校对版全本)作者_贼道三痴_utf8/完整细纲.txt\n",
      "                        八零福星俏媳妇       298 298    0                         /content/json_results/json_gemini/30000_summary_gathered/八零福星俏媳妇/完整细纲.txt\n",
      "      《上品寒士》(校对版全本)作者_贼道三痴_utf8       150 150    0       /content/json_results/json_gemini/30000_summary_gathered/《上品寒士》(校对版全本)作者_贼道三痴_utf8/完整细纲.txt\n",
      "       《十州风云志》(校对版全本)作者_知秋_utf8       142 142    0        /content/json_results/json_gemini/30000_summary_gathered/《十州风云志》(校对版全本)作者_知秋_utf8/完整细纲.txt\n",
      "     《史上第一混乱》(校对版全本)作者_张小花_utf8       148 148    0      /content/json_results/json_gemini/30000_summary_gathered/《史上第一混乱》(校对版全本)作者_张小花_utf8/完整细纲.txt\n",
      " 《随波逐流之一代军师》(校对版全本)作者_随波逐流_utf8       150 150    0  /content/json_results/json_gemini/30000_summary_gathered/《随波逐流之一代军师》(校对版全本)作者_随波逐流_utf8/完整细纲.txt\n",
      "                     重生八零_佳妻致富忙       659 659    0                      /content/json_results/json_gemini/30000_summary_gathered/重生八零_佳妻致富忙/完整细纲.txt\n",
      "                       重回八零过好日子       660 660    0                        /content/json_results/json_gemini/30000_summary_gathered/重回八零过好日子/完整细纲.txt\n",
      "     《绝对一番》(校对版全本)作者_海底漫步者_utf8       147 147    0      /content/json_results/json_gemini/30000_summary_gathered/《绝对一番》(校对版全本)作者_海底漫步者_utf8/完整细纲.txt\n",
      "  《我的女友是恶女》(校对版全本)作者_海底漫步者_utf8       150 150    0   /content/json_results/json_gemini/30000_summary_gathered/《我的女友是恶女》(校对版全本)作者_海底漫步者_utf8/完整细纲.txt\n",
      "\n",
      "所有 TXT 已输出至：/content/json_results/json_gemini/30000_summary_gathered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # ===============================================================\n",
    "# # 一键把 *_analysis.json → 每本书一个《完整细纲.txt》\n",
    "# # 支持文件名形态：\n",
    "# #   118_第二十八章xxx_analysis.json\n",
    "# #   第005章_xxx_analysis.json\n",
    "# #   书名_001_analysis.json（也OK）\n",
    "# #   ——核心：文件名里**第一个数字**认为是章号，\n",
    "# #   书名直接取最外层文件夹名。\n",
    "# # ===============================================================\n",
    "\n",
    "# #!pip install -q tqdm pandas          # ← 如本环境已装可注释\n",
    "\n",
    "# from pathlib import Path\n",
    "# import re, json, tqdm, logging, pandas as pd\n",
    "\n",
    "# # ================= 修改这里 =================\n",
    "# JSON_ROOT    = Path(\"/content/json_results/json_gemini_normalized/30000_summary\")   # 放 JSON 的根目录\n",
    "# TXT_OUT_ROOT = Path(\"/content/json_results/json_gemini_normalized/30000_summary_gathered\")                # 输出根目录\n",
    "# # ============================================\n",
    "# TXT_OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO,\n",
    "#                     format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# # ---------- 简单工具 ----------\n",
    "# DIGIT_RE = re.compile(r\"(\\d{1,4})\")          # 抓 1-4 位章号\n",
    "\n",
    "# def pick_book_and_chapter(path: Path):\n",
    "#     \"\"\"返回 (book_id, chap_no)；识别失败 → None\"\"\"\n",
    "#     book_id = path.parent.name               # 父文件夹名即书名\n",
    "#     m = DIGIT_RE.match(path.stem) or DIGIT_RE.search(path.stem)\n",
    "#     if not m:\n",
    "#         return None\n",
    "#     return book_id, int(m.group(1))\n",
    "\n",
    "# def bullets(lst):\n",
    "#     return \"- 无\" if not lst else \"\\n\".join(f\"- {x}\" for x in lst)\n",
    "\n",
    "# def json_to_outline(jp: Path) -> str:\n",
    "#     data = json.loads(jp.read_text(encoding=\"utf-8\"))\n",
    "#     chap_tag = jp.stem.replace(\"_analysis\", \"\")\n",
    "#     return \"\\n\".join([\n",
    "#         f\"章节：{chap_tag}\",\n",
    "#         \"\\n【情节摘要导语】\",\n",
    "#         data.get(\"情节摘要导语\",\"N/A\"),\n",
    "#         \"\\n【出现人物】\",\n",
    "#         bullets(data.get(\"出现人物\", [])),\n",
    "#         \"\\n【出现道具】\",\n",
    "#         bullets(data.get(\"出现道具\", [])),\n",
    "#         \"\\n【出现场景】\",\n",
    "#         bullets(data.get(\"出现场景\", [])),\n",
    "#         \"\\n【伏笔_设下】\",\n",
    "#         bullets(data.get(\"伏笔_设下\", [])),\n",
    "#         \"\\n【伏笔_回收】\",\n",
    "#         bullets(data.get(\"伏笔_回收\", [])),\n",
    "#         \"\\n\" + \"-\"*40 + \"\\n\"\n",
    "#     ])\n",
    "\n",
    "# # ---------- 把所有 JSON 归簇 ----------\n",
    "# book_files = {}\n",
    "# for jp in JSON_ROOT.rglob(\"*_analysis.json\"):\n",
    "#     res = pick_book_and_chapter(jp)\n",
    "#     if not res:\n",
    "#         logging.warning(f\"跳过无法识别文件名: {jp}\")\n",
    "#         continue\n",
    "#     book, chap = res\n",
    "#     book_files.setdefault(book, []).append((chap, jp))\n",
    "\n",
    "# if not book_files:\n",
    "#     raise SystemExit(f\"❌ 在 {JSON_ROOT} 下没找到 *_analysis.json\")\n",
    "\n",
    "# # ---------- 逐书写 TXT ----------\n",
    "# stats = []\n",
    "# for book, lst in tqdm.tqdm(book_files.items(), desc=\"📚 处理书籍\"):\n",
    "#     lst.sort(key=lambda x: x[0])                     # 按章号\n",
    "#     out_dir = TXT_OUT_ROOT / book\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     merged, ok, bad = [], 0, 0\n",
    "#     for num, jp in lst:\n",
    "#         try:\n",
    "#             outline = json_to_outline(jp)\n",
    "#             (out_dir / f\"{num:03}_outline.txt\").write_text(outline, \"utf-8\")\n",
    "#             merged.append(outline); ok += 1\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"解析 {jp} 出错: {e}\"); bad += 1\n",
    "\n",
    "#     (out_dir / \"完整细纲.txt\").write_text(\"\".join(merged), \"utf-8\")\n",
    "#     stats.append(dict(book=book, chapters=len(lst), ok=ok, bad=bad,\n",
    "#                       out_file=str(out_dir / '完整细纲.txt')))\n",
    "\n",
    "# # ---------- 汇总显示 ----------\n",
    "# df = pd.DataFrame(stats)\n",
    "# print(\"\\n✅ 全部完成。概览：\")\n",
    "# print(df.to_string(index=False))\n",
    "# print(f\"\\n所有 TXT 已输出至：{TXT_OUT_ROOT.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54b69f27-d2e2-4a41-8e12-c67c6b82e386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📚 处理书籍:  38%|███▊      | 15/40 [00:00<00:00, 141.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 【《奋斗在新明朝》（校对版全本）作者：随轻风去_utf8】 完整细纲（共 199 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《反正我是超能力者》（校对版全本）作者：吃书妖_utf8】 完整细纲（共 199 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《天可汗》（校对版全本）作者：西风紧_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《崩坏世界的传奇大冒险》（精校版全本）作者：国王陛下_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《全球进化》（精校版全本）作者：咬狗_utf8】 完整细纲（共 197 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《武林半侠传》（校对版全本）作者：文抄公_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【国宴大厨在八零】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《搜神记》（精校版全本）作者：树下野狐_utf8】 完整细纲（共 199 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【重生八零：毒妻不好惹】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《窃明》（校对版全本）作者：大爆炸(灰熊猫)_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《蜀山》（精校版全本）作者：流浪的蛤蟆_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《陈二狗的妖孽人生》（校对版全本）作者：烽火戏诸侯_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《贩罪》（精校版全本）作者：三天两觉_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《重生之出人头地》（校对版全本）作者：闹闹不爱闹_utf8】 完整细纲（共 197 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【八零喜事：当家肥妻大翻身】 完整细纲（共 199 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《肆虐韩娱》（校对版全本）作者：姬叉_utf8】 完整细纲（共 197 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【八零年代好时光】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《食物链顶端的男人》（校对版全本）作者：熊狼狗_utf8】 完整细纲（共 199 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《高手寂寞2》（校对版全本）作者：兰帝魅晨_utf8】 完整细纲（共 197 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《黑龙法典》（校对版全本）作者：欢声_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《诛仙》（校对版全本）作者：萧鼎_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《回到过去变成猫》（精校版全本）作者：陈词懒调_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《神游》（校对版全本）作者：徐公子胜治_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《老子是癞蛤蟆》（校对版全本） 作者：烽火戏诸侯_utf8】 完整细纲（共 197 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《未来天王》（校对版全本）作者：陈词懒调_utf8】 完整细纲（共 199 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《大画家》（校对版全本）作者：醛石_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《超级惊悚直播》作者：宇文长弓_utf8】 完整细纲（共 194 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《人道天堂》（校对版全本）作者：荆柯守_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《道缘浮图》（校对版全本）作者：烟雨江南_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📚 处理书籍: 100%|██████████| 40/40 [00:00<00:00, 137.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 【《重活了》（精校版全本）作者： 尝谕_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《雅骚》（校对版全本）作者：贼道三痴_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【八零福星俏媳妇】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《上品寒士》（校对版全本）作者：贼道三痴_utf8】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《十州风云志》（校对版全本）作者：知秋_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《史上第一混乱》（校对版全本）作者：张小花_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《随波逐流之一代军师》（校对版全本）作者：随波逐流_utf8】 完整细纲（共 199 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【重生八零：佳妻致富忙】 完整细纲（共 197 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【重回八零过好日子】 完整细纲（共 200 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《绝对一番》（校对版全本）作者：海底漫步者_utf8】 完整细纲（共 198 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "\n",
      "📘 【《我的女友是恶女》（校对版全本）作者：海底漫步者_utf8】 完整细纲（共 195 章）\n",
      "============================================================\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#汇总细纲\n",
    "from pathlib import Path\n",
    "import re, json, tqdm, logging\n",
    "\n",
    "# ================= 修改这里 =================\n",
    "JSON_ROOT    = Path(\"/content/json_results/json_gemini_normalized/50000_summary\")   # JSON 输入目录\n",
    "TXT_OUT_ROOT = Path(\"/content/json_results/json_gemini_normalized/100000_summary_gathered\")  # 输出目录\n",
    "# ============================================\n",
    "TXT_OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "DIGIT_RE = re.compile(r\"(\\d{1,4})\")\n",
    "\n",
    "def pick_book_and_chapter(path: Path):\n",
    "    book_id = path.parent.name\n",
    "    m = DIGIT_RE.match(path.stem) or DIGIT_RE.search(path.stem)\n",
    "    if not m:\n",
    "        return None\n",
    "    return book_id, int(m.group(1))\n",
    "\n",
    "def bullets(lst):\n",
    "    return \"- 无\" if not lst else \"\\n\".join(f\"- {x}\" for x in lst)\n",
    "\n",
    "def json_to_outline(jp: Path) -> str:\n",
    "    data = json.loads(jp.read_text(encoding=\"utf-8\"))\n",
    "    chap_tag = jp.stem.replace(\"_analysis\", \"\")\n",
    "\n",
    "    # 加入移除“第n章”模式的正则表达式\n",
    "    chapter_pattern = re.compile(r'(第\\s*[零一二三四五六七八九十百千万亿\\d]+\\s*[章节回集篇部])|([零一二三四五六七八九十百千万亿\\d]+\\s*[章节回集篇部])')    \n",
    "    summary_intro = chapter_pattern.sub('', data.get(\"情节摘要导语\", \"N/A\"))\n",
    "\n",
    "    return \"\\n\".join([\n",
    "        f\"章节：{chap_tag}\",\n",
    "        \"\\n【情节摘要导语】\",\n",
    "        summary_intro.strip(),\n",
    "        \"\\n【出现人物】\",\n",
    "        bullets(data.get(\"出现人物\", [])),\n",
    "        \"\\n【出现道具】\",\n",
    "        bullets(data.get(\"出现道具\", [])),\n",
    "        \"\\n【出现场景】\",\n",
    "        bullets(data.get(\"出现场景\", [])),\n",
    "        \"\\n【伏笔_设下】\",\n",
    "        bullets(data.get(\"伏笔_设下\", [])),\n",
    "        \"\\n【伏笔_回收】\",\n",
    "        bullets(data.get(\"伏笔_回收\", [])),\n",
    "        \"\\n\" + \"-\" * 40 + \"\\n\"\n",
    "    ])\n",
    "\n",
    "# ---------- 聚合 JSON ----------\n",
    "book_files = {}\n",
    "for jp in JSON_ROOT.rglob(\"*_analysis.json\"):\n",
    "    res = pick_book_and_chapter(jp)\n",
    "    if not res:\n",
    "        logging.warning(f\"跳过无法识别文件名: {jp}\")\n",
    "        continue\n",
    "    book, chap = res\n",
    "    book_files.setdefault(book, []).append((chap, jp))\n",
    "\n",
    "if not book_files:\n",
    "    raise SystemExit(f\"❌ 在 {JSON_ROOT} 下没找到 *_analysis.json\")\n",
    "\n",
    "# ---------- 汇总并写入 ----------\n",
    "for book, lst in tqdm.tqdm(book_files.items(), desc=\"📚 处理书籍\"):\n",
    "    lst.sort(key=lambda x: x[0])\n",
    "    merged, ok, bad = [], 0, 0\n",
    "    for _, jp in lst:\n",
    "        try:\n",
    "            outline = json_to_outline(jp)\n",
    "            merged.append(outline)\n",
    "            ok += 1\n",
    "        except Exception as e:\n",
    "            logging.error(f\"解析 {jp} 出错: {e}\")\n",
    "            bad += 1\n",
    "\n",
    "    full_text = \"\".join(merged)\n",
    "    print(f\"\\n📘 【{book}】 完整细纲（共 {ok} 章）\")\n",
    "    print(\"=\" * 60)\n",
    "    #print(full_text)\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    out_dir = TXT_OUT_ROOT / book\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_dir / \"entire_section_outline.txt\"\n",
    "    out_file.write_text(full_text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf92233-d584-4b01-a370-f0eceb568008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168f246-88aa-447b-a920-e516d9ef4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================\n",
    "# # ✅ Gemini 分析小说章节，结构化输出 JSON + TXT细纲生成 (主程序参数更新)\n",
    "# # =============================\n",
    "# # !pip install -q --upgrade google-generativeai chardet tqdm\n",
    "# # !pip install -U google-generativeai # 确保安装最新版本\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import re\n",
    "# import time\n",
    "# import random\n",
    "# import string\n",
    "# import chardet\n",
    "# import logging\n",
    "# import copy\n",
    "# from pathlib import Path\n",
    "# from typing import List, Dict, Any, Tuple\n",
    "# from tqdm.auto import tqdm\n",
    "# import google.generativeai as genai\n",
    "# from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "# # --- (API 初始化, 通用函数, Schema, Prompt 模板 - 与上一版本相同) ---\n",
    "\n",
    "# # —— API 初始化 (⚠️ 警告：直接写入 API 密钥极不安全！) ——\n",
    "# api_key = \"your-default-api-key\" # ⚠️ 替换为您的真实 API 密钥\n",
    "# if not api_key or api_key == \"YOUR_API_KEY_HERE\":\n",
    "#     raise ValueError(\"❌ 错误：请务必将代码中的 'YOUR_API_KEY_HERE' 替换为您的真实 Gemini API 密钥。\")\n",
    "# try:\n",
    "#     genai.configure(api_key=api_key)\n",
    "#     logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "#     logging.info(\"Gemini API 已使用直接写入的密钥进行配置。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"CRITICAL: 使用提供的 API 密钥配置 Gemini SDK 时出错: {e}\")\n",
    "#     raise ValueError(f\"API 密钥配置失败: {e}\")\n",
    "\n",
    "# GEMINI_MODEL = \"gemini-2.0-flash-latest\"\n",
    "\n",
    "# def _auto_decode(path: Path) -> str:\n",
    "#     try:\n",
    "#         raw = path.read_bytes()\n",
    "#         enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "#         if enc.lower() not in ['utf-8', 'gbk', 'gb2312', 'big5']:\n",
    "#              try: return raw.decode('utf-8', errors='ignore').strip()\n",
    "#              except UnicodeDecodeError:\n",
    "#                  try: return raw.decode('gbk', errors='ignore').strip()\n",
    "#                  except UnicodeDecodeError: return raw.decode(enc, errors='ignore').strip()\n",
    "#         return raw.decode(enc, errors=\"ignore\").strip()\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"读取文件 {path.name} 时出错: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# def _rand_tag(k=6): return ''.join(random.choices(string.ascii_uppercase, k=k))\n",
    "\n",
    "# base_json_schema = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"情节摘要导语\": {\"type\": \"string\", \"description\": \"\"},\n",
    "#         \"出现人物\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章出现的所有人物名称列表\"},\n",
    "#         \"出现道具\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章出现的关键道具列表\"},\n",
    "#         \"出现场景\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章发生故事的主要场景列表\"},\n",
    "#         \"伏笔_设下\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章新埋下的伏笔描述\"},\n",
    "#         \"伏笔_回收\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"本章回收或呼应的过往伏笔描述\"}\n",
    "#     },\n",
    "#     \"required\": [\"情节摘要导语\", \"出现人物\", \"出现道具\", \"出现场景\", \"伏笔_设下\", \"伏笔_回收\"]\n",
    "# }\n",
    "\n",
    "# prompt_chapter_template = r\"\"\"\n",
    "# 你是一位专业的文学编辑。请仔细阅读我提供的【章节全文】。\n",
    "# 你的任务是提取结构化信息，并 **必须** 调用 `extract_chapter_details` 函数来返回结果。\n",
    "# 请严格按照函数参数的描述（特别是关于“情节摘要导语”的详细程度要求）来填充信息。\n",
    "# **绝对不要** 输出任何 JSON 格式之外的文本、解释、代码块标记（如 ```json ... ```）或 Markdown。\n",
    "# 直接调用函数并填充其参数。\n",
    "\n",
    "# 【章节全文】：\n",
    "# {chapter_text}\n",
    "# \"\"\"\n",
    "\n",
    "# # --- (analyze_chapter 函数 - 与上一版本修复后相同) ---\n",
    "# def analyze_chapter(\n",
    "#     path: Path,\n",
    "#     prompt_template: str,\n",
    "#     summary_description: str,\n",
    "#     retries: int = 3\n",
    "# ) -> str | None:\n",
    "#     text = _auto_decode(path)\n",
    "#     if not text: return None\n",
    "#     full_prompt = prompt_template.format(chapter_text=text)\n",
    "#     current_schema = copy.deepcopy(base_json_schema)\n",
    "#     current_schema[\"properties\"][\"情节摘要导语\"][\"description\"] = summary_description\n",
    "#     extract_details_func_declaration = {\n",
    "#         \"name\": \"extract_chapter_details\",\n",
    "#         \"description\": \"提取小说章节的结构化信息...\",\n",
    "#         \"parameters\": current_schema\n",
    "#     }\n",
    "#     safety_settings = { HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE, }\n",
    "#     try:\n",
    "#         gemini_model = genai.GenerativeModel(GEMINI_MODEL, safety_settings=safety_settings)\n",
    "#     except Exception as model_init_err:\n",
    "#         logging.error(f\"初始化 Gemini 模型时出错: {model_init_err}\", exc_info=True)\n",
    "#         return None\n",
    "\n",
    "#     for attempt in range(retries):\n",
    "#         func_call_args_raw = None\n",
    "#         try:\n",
    "#             logging.info(f\"开始分析章节: {path.name} (摘要要求: '{summary_description}', 尝试 {attempt + 1}/{retries})\")\n",
    "#             rsp = gemini_model.generate_content(full_prompt, generation_config={\"temperature\": 0.3}, tools=[{\"function_declarations\": [extract_details_func_declaration]}], tool_config={'function_calling_config': 'ANY'})\n",
    "#             if not rsp.candidates: logging.warning(f\"分析失败 (尝试 {attempt + 1}): API 未返回候选内容。\"); time.sleep(2 ** attempt + random.uniform(0, 1)); continue\n",
    "#             first_candidate = rsp.candidates[0]\n",
    "#             if not first_candidate.content or not first_candidate.content.parts:\n",
    "#                 if first_candidate.finish_reason == genai.types.FinishReason.SAFETY: logging.warning(f\"分析失败 (尝试 {attempt + 1}): 内容被安全设置阻止。\")\n",
    "#                 else: logging.warning(f\"分析失败 (尝试 {attempt + 1}): 候选内容为空。原因: {first_candidate.finish_reason}\")\n",
    "#                 time.sleep(2 ** attempt + random.uniform(0, 1)); continue\n",
    "#             func_call_part = None\n",
    "#             for part in first_candidate.content.parts:\n",
    "#                 if part.function_call: func_call_part = part; break\n",
    "#             if func_call_part and func_call_part.function_call:\n",
    "#                 fc = func_call_part.function_call\n",
    "#                 func_call_args_raw = fc.args\n",
    "#                 if fc.name == \"extract_chapter_details\":\n",
    "#                     logging.info(f\"成功分析章节: {path.name} (摘要要求: '{summary_description}')\")\n",
    "#                     try:\n",
    "#                         args_dict_shallow = dict(fc.args)\n",
    "#                         args_dict_native = {}\n",
    "#                         for key, value in args_dict_shallow.items():\n",
    "#                             if type(value).__name__ == 'RepeatedComposite':\n",
    "#                                 args_dict_native[key] = list(value)\n",
    "#                             else:\n",
    "#                                 args_dict_native[key] = value\n",
    "#                         json_output = json.dumps(args_dict_native, ensure_ascii=False, indent=2)\n",
    "#                         return json_output\n",
    "#                     except TypeError as json_err:\n",
    "#                         logging.error(f\"序列化参数时出错: {json_err}\", exc_info=False)\n",
    "#                         logging.error(f\"  原始参数类型: {type(func_call_args_raw)}\")\n",
    "#                         logging.error(f\"  浅层字典 (部分): {str(args_dict_shallow)[:500]}...\")\n",
    "#                         time.sleep(2 ** attempt + random.uniform(0, 1)); continue # Go to next retry on serialization error\n",
    "#                 else: logging.warning(f\"分析警告 (尝试 {attempt + 1}): 调用了意外函数 '{fc.name}'\")\n",
    "#             else:\n",
    "#                 finish_reason = first_candidate.finish_reason; safety_ratings = first_candidate.safety_ratings\n",
    "#                 logging.warning(f\"分析失败 (尝试 {attempt + 1}): 未找到函数调用。原因: {finish_reason}\")\n",
    "#                 if safety_ratings: logging.warning(f\"  安全评级: {safety_ratings}\")\n",
    "#                 try: text_output = first_candidate.text\n",
    "#                 except ValueError: text_output = str(first_candidate.content.parts[0]) if first_candidate.content and first_candidate.content.parts else \"\"\n",
    "#                 if text_output: logging.warning(f\"  模型返回内容 (部分): {text_output[:200]}...\")\n",
    "#             time.sleep(2 ** attempt + random.uniform(0, 1)) # Wait before next retry if this attempt failed here\n",
    "#         except Exception as e:\n",
    "#             error_context = f\" | 参数类型: {type(func_call_args_raw)}, 内容 (部分): {str(func_call_args_raw)[:200]}...\" if func_call_args_raw else \"\"\n",
    "#             logging.error(f\"API 调用或处理时发生异常 (尝试 {attempt + 1}): {e}{error_context}\", exc_info=True)\n",
    "#             time.sleep(2 ** attempt + random.uniform(0, 1)) # Wait before next retry on general exception\n",
    "#     logging.error(f\"❌ 分析失败，已达最大重试次数: {path.name} (摘要要求: '{summary_description}')\")\n",
    "#     return None\n",
    "\n",
    "\n",
    "# # --- (run_analysis 函数 - 与上一版本相同) ---\n",
    "# def run_analysis(chapter_root: str, prompt_template: str, summary_description: str, out_dir: str, mode: tuple[int, int] = (1, 150)):\n",
    "#     root = Path(chapter_root); out_base = Path(out_dir)\n",
    "#     if not root.is_dir(): logging.error(f\"错误：输入目录 '{chapter_root}' 不存在。\"); return\n",
    "#     out_base.mkdir(parents=True, exist_ok=True)\n",
    "#     logging.info(f\"开始批量分析 (摘要要求: '{summary_description}'), 输出到: {out_base}\")\n",
    "#     books = [d for d in root.iterdir() if d.is_dir()]\n",
    "#     if not books: logging.warning(f\"在 '{chapter_root}' 下未找到任何小说子目录。\"); return\n",
    "#     s, e = mode; logging.info(f\"处理章节范围: {s} 到 {e}\")\n",
    "#     total_processed, total_failed, total_skipped = 0, 0, 0\n",
    "#     for book in tqdm(books, desc=\"📚 处理小说书目\"):\n",
    "#         chapters = sorted([p for p in book.glob(\"*.txt\") if p.name[:3].isdigit() and s <= int(p.name[:3]) <= e], key=lambda p: int(p.name[:3]))\n",
    "#         if not chapters: logging.warning(f\"在 '{book.name}' 目录中未找到符合范围 {s}-{e} 的章节文件。\"); continue\n",
    "#         out_book_dir = out_base / book.name; out_book_dir.mkdir(parents=True, exist_ok=True)\n",
    "#         processed_count, failed_count, skipped_count = 0, 0, 0\n",
    "#         for chap_path in tqdm(chapters, desc=f\"📖 分析 '{book.name}'\", leave=False):\n",
    "#             out_filename = f\"{chap_path.stem}_analysis.json\"; out_path = out_book_dir / out_filename\n",
    "#             if out_path.exists(): skipped_count += 1; continue\n",
    "#             try:\n",
    "#                 result_json = analyze_chapter(chap_path, prompt_template, summary_description)\n",
    "#                 if result_json: out_path.write_text(result_json, encoding=\"utf-8\"); processed_count += 1\n",
    "#                 else: failed_count += 1\n",
    "#             except Exception as e: logging.error(f\"处理章节 {chap_path.name} 时发生意外错误: {e}\", exc_info=True); failed_count += 1\n",
    "#         logging.info(f\"完成处理 '{book.name}': {processed_count} 成功, {failed_count} 失败, {skipped_count} 跳过。\")\n",
    "#         total_processed += processed_count; total_failed += failed_count; total_skipped += skipped_count\n",
    "#     # Ensure the final message regarding where results are saved uses the correct base directory variable\n",
    "#     print(f\"\\n✅ JSON 分析完成。总计: {total_processed} 成功, {total_failed} 失败, {total_skipped} 跳过。\")\n",
    "#     print(f\"  JSON 结果保存在: {out_dir}\") # Use out_dir specific to this run_analysis call\n",
    "\n",
    "\n",
    "# # --- (JSON 转 TXT 及合并功能 - format_list_output, convert_json_to_txt, merge_txt_outlines, run_post_processing - 与上一版本相同) ---\n",
    "\n",
    "# def format_list_output(items: List[str]) -> str:\n",
    "#     \"\"\"Helper function to format lists for TXT output.\"\"\"\n",
    "#     if not items: return \"- 无\"\n",
    "#     return \"\\n\".join(f\"- {item}\" for item in items)\n",
    "\n",
    "# def convert_json_to_txt(json_path: Path, txt_path: Path) -> bool:\n",
    "#     \"\"\"Reads JSON analysis file, writes formatted TXT outline.\"\"\"\n",
    "#     try:\n",
    "#         with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "#         chapter_title = json_path.stem.replace(\"_analysis\", \"\")\n",
    "#         output_lines = [\n",
    "#             f\"章节：{chapter_title}\", \"\\n【情节摘要导语】\", data.get(\"情节摘要导语\", \"N/A\"),\n",
    "#             \"\\n【出现人物】\", format_list_output(data.get(\"出现人物\", [])),\n",
    "#             \"\\n【出现道具】\", format_list_output(data.get(\"出现道具\", [])),\n",
    "#             \"\\n【出现场景】\", format_list_output(data.get(\"出现场景\", [])),\n",
    "#             \"\\n【伏笔_设下】\", format_list_output(data.get(\"伏笔_设下\", [])),\n",
    "#             \"\\n【伏笔_回收】\", format_list_output(data.get(\"伏笔_回收\", [])),\n",
    "#             \"\\n\" + \"-\" * 40 + \"\\n\"\n",
    "#         ]\n",
    "#         txt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         with open(txt_path, 'w', encoding='utf-8') as f: f.write(\"\\n\".join(output_lines))\n",
    "#         return True\n",
    "#     except FileNotFoundError: logging.error(f\"JSON 文件未找到: {json_path}\"); return False\n",
    "#     except json.JSONDecodeError: logging.error(f\"无法解析 JSON 文件: {json_path}\"); return False\n",
    "#     except Exception as e: logging.error(f\"转换 {json_path.name} 到 TXT 时发生意外错误: {e}\", exc_info=True); return False\n",
    "\n",
    "# def merge_txt_outlines(txt_dir: Path, output_file: Path) -> bool:\n",
    "#     \"\"\"Merges all chapter TXT outlines in a directory into a single file.\"\"\"\n",
    "#     try:\n",
    "#         chapter_files = list(txt_dir.glob(\"*_outline.txt\"))\n",
    "#         if not chapter_files: logging.warning(f\"在目录 {txt_dir} 中未找到要合并的 TXT 文件 (*_outline.txt)。\"); return False\n",
    "#         def get_chapter_num(file_path: Path) -> int:\n",
    "#             try: match = re.match(r\"(\\d+)\", file_path.name); return int(match.group(1)) if match else float('inf')\n",
    "#             except ValueError: return float('inf')\n",
    "#         chapter_files.sort(key=get_chapter_num)\n",
    "#         merged_content = []\n",
    "#         logging.info(f\"开始合并 {len(chapter_files)} 个 TXT 文件到 {output_file.name}...\")\n",
    "#         for chap_file in tqdm(chapter_files, desc=f\"  合并 TXT\", leave=False):\n",
    "#             try:\n",
    "#                 with open(chap_file, 'r', encoding='utf-8') as f: merged_content.append(f.read())\n",
    "#             except Exception as e: logging.error(f\"读取 TXT 文件 {chap_file.name} 时出错: {e}\")\n",
    "#         if not merged_content: logging.error(f\"未能读取任何 TXT 文件内容进行合并。\"); return False\n",
    "#         output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         with open(output_file, 'w', encoding='utf-8') as f: f.write(\"\".join(merged_content))\n",
    "#         logging.info(f\"成功合并 TXT 文件到: {output_file}\")\n",
    "#         return True\n",
    "#     except Exception as e: logging.error(f\"合并 TXT 文件到 {output_file.name} 时发生意外错误: {e}\", exc_info=True); return False\n",
    "\n",
    "# def run_post_processing(base_json_dir: str):\n",
    "#     \"\"\"Runs JSON-to-TXT conversion and merging for all books in a base directory.\"\"\"\n",
    "#     base_path = Path(base_json_dir)\n",
    "#     if not base_path.is_dir(): logging.warning(f\"跳过后期处理：目录 {base_path} 不存在。\"); return\n",
    "#     logging.info(f\"\\n{'='*20} 开始对 '{base_path.name}' 进行后期处理 (TXT 生成与合并) {'='*20}\")\n",
    "#     book_dirs = [d for d in base_path.iterdir() if d.is_dir()]\n",
    "#     if not book_dirs: logging.warning(f\"在 {base_path} 中未找到书籍子目录进行后期处理。\"); return\n",
    "#     total_books_processed, total_books_failed = 0, 0\n",
    "#     for book_json_dir in tqdm(book_dirs, desc=\"📚 处理书籍 (后期)\"):\n",
    "#         book_name = book_json_dir.name\n",
    "#         book_txt_outlines_dir = book_json_dir / f\"{book_name}_txt_outlines\"\n",
    "#         merged_output_file = base_path / f\"{book_name}_完整细纲.txt\" # Save merged file one level up\n",
    "#         logging.info(f\"处理书籍 '{book_name}': JSON={book_json_dir}, TXT={book_txt_outlines_dir}, Merged={merged_output_file}\")\n",
    "#         json_files = list(book_json_dir.glob(\"*_analysis.json\"))\n",
    "#         if not json_files: logging.warning(f\"  在 {book_json_dir} 中未找到 JSON 文件进行转换。\"); continue\n",
    "#         conversion_success_count, conversion_fail_count = 0, 0\n",
    "#         logging.info(f\"  开始转换 {len(json_files)} 个 JSON 文件到 TXT...\")\n",
    "#         for json_file in tqdm(json_files, desc=f\"  转换 JSON\", leave=False):\n",
    "#             txt_filename = json_file.stem.replace(\"_analysis\", \"_outline.txt\")\n",
    "#             txt_file_path = book_txt_outlines_dir / txt_filename\n",
    "#             if convert_json_to_txt(json_file, txt_file_path): conversion_success_count += 1\n",
    "#             else: conversion_fail_count += 1\n",
    "#         logging.info(f\"  JSON 到 TXT 转换完成: {conversion_success_count} 成功, {conversion_fail_count} 失败。\")\n",
    "#         if conversion_success_count == 0:\n",
    "#             logging.error(f\"  未能成功转换任何 JSON 文件为 TXT，跳过合并步骤。\"); total_books_failed += 1; continue\n",
    "#         if merge_txt_outlines(book_txt_outlines_dir, merged_output_file): total_books_processed += 1\n",
    "#         else: total_books_failed +=1; logging.error(f\"  未能成功合并 '{book_name}' 的 TXT 细纲。\")\n",
    "#     logging.info(f\"\\n{'='*20} '{base_path.name}' 后期处理完成 {'='*20}\")\n",
    "#     logging.info(f\"  成功生成完整细纲的书籍数量: {total_books_processed}\")\n",
    "#     logging.info(f\"  处理失败或未完成的书籍数量: {total_books_failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1f843-f475-4af5-9bb2-f8a34a098b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========================================\n",
    "# # ==           主程序入口 (更新)        ==\n",
    "# # ========================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 定义输入目录 (★★★ 根据你的实际情况修改 ★★★)\n",
    "#     CHAPTERS_INPUT_DIR = \"/content/novels_normalized\"\n",
    "#     # 定义输出根目录 (★★★ 根据你的实际情况修改 ★★★)\n",
    "#     OUTPUT_BASE_DIR = \"/content/json_results/json_gemini_normalized\" # <--- 更新\n",
    "\n",
    "#     # --- 定义不同摘要长度的要求 (更新) ---\n",
    "#     summary_req_short = \"生成一个非常简短的核心情节摘要（ 约 150-200 字）\" # <--- 更新\n",
    "#     summary_req_medium = \"生成一个标准的情节摘要，概括主要内容（约 300-400 字）\" # <--- 更新\n",
    "#     summary_req_long = \"生成一个比较详细的情节摘要，包含更多细节和转折（约 600-700 字）\" # <--- 更新\n",
    "\n",
    "#     # --- 运行不同版本的分析 ---\n",
    "#     # (你可以取消注释掉不想运行的版本)\n",
    "#     analysis_dirs_to_process = [] # Store dirs for post-processing\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始简短摘要分析 (150-200字) \" + \"=\"*20)\n",
    "#     short_summary_dir = os.path.join(OUTPUT_BASE_DIR, \"30000_summary\") # <--- 更新\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_short, # 使用更新后的描述\n",
    "#         out_dir=short_summary_dir,             # 使用更新后的目录\n",
    "#         mode=(1, 200) # 分析 1 到 150 章\n",
    "#     )\n",
    "#     analysis_dirs_to_process.append(short_summary_dir) # 添加更新后的目录\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始标准摘要分析 (300-400字) \" + \"=\"*20)\n",
    "#     medium_summary_dir = os.path.join(OUTPUT_BASE_DIR, \"50000_summary\") # <--- 更新\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_medium, # 使用更新后的描述\n",
    "#         out_dir=medium_summary_dir,             # 使用更新后的目录\n",
    "#         mode=(1, 200)\n",
    "#     )\n",
    "#     analysis_dirs_to_process.append(medium_summary_dir) # 添加更新后的目录\n",
    "\n",
    "#     logging.info(\"=\"*20 + \" 开始详细摘要分析 (600-700字) \" + \"=\"*20)\n",
    "#     long_summary_dir = os.path.join(OUTPUT_BASE_DIR, \"100000_summary\") # <--- 更新\n",
    "#     run_analysis(\n",
    "#         chapter_root=CHAPTERS_INPUT_DIR,\n",
    "#         prompt_template=prompt_chapter_template,\n",
    "#         summary_description=summary_req_long, # 使用更新后的描述\n",
    "#         out_dir=long_summary_dir,             # 使用更新后的目录\n",
    "#         mode=(1, 200)\n",
    "#     )\n",
    "#     analysis_dirs_to_process.append(long_summary_dir) # 添加更新后的目录\n",
    "\n",
    "#     logging.info(\"\\n\" + \"=\"*20 + \" 所有 JSON 分析任务已完成/提交 \" + \"=\"*20)\n",
    "\n",
    "#     # --- 运行后期处理：生成 TXT 细纲 ---\n",
    "#     # (这部分保持不变，它会使用上面 analysis_dirs_to_process 列表中的新目录)\n",
    "#     logging.info(\"\\n\" + \"=\"*20 + \" 开始运行后期处理 (生成 TXT 细纲) \" + \"=\"*20)\n",
    "#     for dir_to_process in analysis_dirs_to_process:\n",
    "#         run_post_processing(dir_to_process)\n",
    "\n",
    "#     logging.info(\"\\n\" + \"=\"*20 + \" 全部处理流程结束 \" + \"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f373d0-7791-4e90-864f-5c621d157cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📚 处理书籍: 0it [00:52, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_file\u001b[38;5;241m.\u001b[39mexists(): \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    116\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PROMPT_TMPL\u001b[38;5;241m.\u001b[39mformat(target_len\u001b[38;5;241m=\u001b[39mtgt, outline\u001b[38;5;241m=\u001b[39moutline)\n\u001b[0;32m--> 117\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcall_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFUNC_DECL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# schema里是统一字段名 summary → 重新 key\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     common \u001b[38;5;241m=\u001b[39m {k: result[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_characters\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_scenes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_items\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n",
      "Cell \u001b[0;32mIn[13], line 72\u001b[0m, in \u001b[0;36mcall_gemini\u001b[0;34m(prompt, func_decl, retries)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         rsp \u001b[38;5;241m=\u001b[39m \u001b[43mgm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_declarations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc_decl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_calling_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m         cand \u001b[38;5;241m=\u001b[39m rsp\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# --- 解析 function_call ----\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/grpc/_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    270\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/grpc/_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interceptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:79\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[0;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[1;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[1;32m     68\u001b[0m     }\n\u001b[1;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m         },\n\u001b[1;32m     77\u001b[0m     )\n\u001b[0;32m---> 79\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    306\u001b[0m (\n\u001b[1;32m    307\u001b[0m     new_method,\n\u001b[1;32m    308\u001b[0m     new_timeout,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     new_compression,\n\u001b[1;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/grpc/_channel.py:1195\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwith_call\u001b[39m(\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1185\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[1;32m   1192\u001b[0m     (\n\u001b[1;32m   1193\u001b[0m         state,\n\u001b[1;32m   1194\u001b[0m         call,\n\u001b[0;32m-> 1195\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # ===============================================================\n",
    "# # ① 依赖\n",
    "# # ===============================================================\n",
    "# !pip install -q --upgrade google-generativeai tqdm pandas chardet\n",
    "\n",
    "# # ===============================================================\n",
    "# # ② 路径 & KEY\n",
    "# # ===============================================================\n",
    "# from pathlib import Path\n",
    "# import google.generativeai as genai\n",
    "# from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "# import json, logging, random, time, textwrap, tqdm, pandas as pd\n",
    "\n",
    "# TXT_SRC = Path(\"/content/json_results/json_gemini/30000_summary_gathered\")\n",
    "# DIR_1000   = Path(\"/content/json_results/json_gemini/1000_global_json\")\n",
    "# DIR_5000   = Path(\"/content/json_results/json_gemini/5000_global_json\")\n",
    "# DIR_10000  = Path(\"/content/json_results/json_gemini/10000_global_json\")\n",
    "# for d in (DIR_1000, DIR_5000, DIR_10000): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# API_KEY = \"your-default-api-key\"\n",
    "# GEMINI_MODEL = \"gemini-1.5-flash\"      # 同章节脚本风格\n",
    "\n",
    "# genai.configure(api_key=API_KEY)\n",
    "# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# SAFETY_SETTINGS = {\n",
    "#     HarmCategory.HARM_CATEGORY_HARASSMENT       : HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "#     HarmCategory.HARM_CATEGORY_HATE_SPEECH      : HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "#     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "#     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "# }\n",
    "\n",
    "# # ===============================================================\n",
    "# # ③ prompt 模板 & schemas\n",
    "# # ===============================================================\n",
    "# BASE_SCHEMA = {\n",
    "#     \"type\":\"object\",\n",
    "#     \"properties\":{\n",
    "#         \"total_characters\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\n",
    "#         \"total_scenes\"    :{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\n",
    "#         \"total_items\"     :{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\n",
    "#         \"summary\"         :{\"type\":\"string\"}\n",
    "#     },\n",
    "#     \"required\":[\"total_characters\",\"total_scenes\",\"total_items\",\"summary\"]\n",
    "# }\n",
    "\n",
    "# FUNC_DECL = {\n",
    "#     \"name\":\"extract_book_outline\",\n",
    "#     \"description\":\"整书大纲提炼：去重三张列表 + 指定字数剧情大纲\",\n",
    "#     \"parameters\":BASE_SCHEMA\n",
    "# }\n",
    "\n",
    "# PROMPT_TMPL = textwrap.dedent(\"\"\"\n",
    "# 你是一位严谨的出版编辑。阅读《完整细纲》后：\n",
    "# 1. 去重列出全书【人物】【场景】【道具】。\n",
    "# 2. 写一篇约 {target_len} 字（±10%）的剧情大纲。\n",
    "# 必须调用 extract_book_outline 函数返回 JSON，仅含\n",
    "#  total_characters / total_scenes / total_items / summary\n",
    "# 三个列表与一段大纲。\n",
    "\n",
    "# 《完整细纲》：\n",
    "# {outline}\n",
    "# \"\"\")\n",
    "\n",
    "# # ===============================================================\n",
    "# # ④ Gemini 调用（与章节脚本同风格）\n",
    "# # ===============================================================\n",
    "# def call_gemini(prompt: str, func_decl: dict, retries=3):\n",
    "#     gm = genai.GenerativeModel(GEMINI_MODEL, safety_settings=SAFETY_SETTINGS)\n",
    "#     for i in range(retries):\n",
    "#         try:\n",
    "#             rsp = gm.generate_content(\n",
    "#                 prompt,\n",
    "#                 generation_config={\"temperature\":0.3},\n",
    "#                 tools=[{\"function_declarations\":[func_decl]}],\n",
    "#                 tool_config={\"function_calling_config\":\"ANY\"}\n",
    "#             )\n",
    "#             cand = rsp.candidates[0]\n",
    "#             # --- 解析 function_call ----\n",
    "#             func_part = next((p for p in cand.content.parts if p.function_call), None)\n",
    "#             if func_part and func_part.function_call and \\\n",
    "#                func_part.function_call.name == \"extract_book_outline\":\n",
    "#                 # protobuf → python\n",
    "#                 def to_py(v):\n",
    "#                     if type(v).__name__ == \"RepeatedComposite\": return list(v)\n",
    "#                     return v\n",
    "#                 return {k:to_py(v) for k,v in dict(func_part.function_call.args).items()}\n",
    "#             # --- 备用：普通 text 返回 ---\n",
    "#             if cand.content.parts and cand.content.parts[0].text:\n",
    "#                 return json.loads(cand.content.parts[0].text)\n",
    "#             if hasattr(cand,\"text\") and cand.text:\n",
    "#                 return json.loads(cand.text)\n",
    "#             logging.warning(f\"Gemini 无有效输出（尝试{i+1}） finish_reason={cand.finish_reason}\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Gemini 调用异常（尝试{i+1}）：{e}\")\n",
    "#         time.sleep(2**i+random.random())\n",
    "#     return None\n",
    "\n",
    "# def save_json(out_dir:Path, book:str, data:dict):\n",
    "#     (out_dir/f\"{book}.json\").write_text(json.dumps(data,ensure_ascii=False,indent=2),\"utf-8\")\n",
    "\n",
    "# # ===============================================================\n",
    "# # ⑤ 主循环\n",
    "# # ===============================================================\n",
    "# stats=[]\n",
    "# for outline_path in tqdm.tqdm(TXT_SRC.rglob(\"*完整细纲*.txt\"), desc=\"📚 处理书籍\"):\n",
    "#     book = outline_path.parent.name\n",
    "#     if (DIR_1000/f\"{book}.json\").exists() and (DIR_5000/f\"{book}.json\").exists() and (DIR_10000/f\"{book}.json\").exists():\n",
    "#         logging.info(f\"跳过《{book}》—三档已存在\"); continue\n",
    "\n",
    "#     outline = outline_path.read_text(encoding=\"utf-8\")[:250_000]  # 控制长度\n",
    "#     for tag, tgt, out_dir in [(\"1000\",1000,DIR_1000), (\"5000\",5000,DIR_5000), (\"10000\",10000,DIR_10000)]:\n",
    "#         out_file = out_dir / f\"{book}.json\"\n",
    "#         if out_file.exists(): continue\n",
    "\n",
    "#         prompt = PROMPT_TMPL.format(target_len=tgt, outline=outline)\n",
    "#         result = call_gemini(prompt, FUNC_DECL)\n",
    "#         if result:\n",
    "#             # schema里是统一字段名 summary → 重新 key\n",
    "#             common = {k: result[k] for k in (\"total_characters\",\"total_scenes\",\"total_items\")}\n",
    "#             save_json(out_dir, book, {**common, f\"summary_{tag}\": result[\"summary\"]})\n",
    "#             logging.info(f\"生成 {tag} 字大纲成功：《{book}》\")\n",
    "#         else:\n",
    "#             logging.error(f\"生成 {tag} 字大纲失败：《{book}》\")\n",
    "#     stats.append(dict(book=book))\n",
    "\n",
    "# # ===============================================================\n",
    "# # ⑥ 结果概览\n",
    "# # ===============================================================\n",
    "# print(\"\\n完成，目录：\")\n",
    "# print(\"1000字 ⇒\", DIR_1000)\n",
    "# print(\"5000字 ⇒\", DIR_5000)\n",
    "# print(\"10000字 ⇒\", DIR_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df47fc-9a47-4bec-a3c3-2e0e77af81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ✅ Gemini 总纲结构化调用（参考章节脚本，完全兼容）\n",
    "# ===============================================================\n",
    "!pip install -q --upgrade google-generativeai tqdm chardet\n",
    "\n",
    "import os, json, logging, random, time, textwrap\n",
    "from pathlib import Path\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================================================\n",
    "# ① 设置参数\n",
    "# ===============================================================\n",
    "API_KEY = \"your-default-api-key\"  # ← 替换\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"  # 推荐用 1.5 处理整书，flash 容易爆上下文\n",
    "BOOK_DIR = Path(\"/content/json_results/json_gemini/30000_summary_gathered/《贩罪》(精校版全本)作者_三天两觉_utf8\")\n",
    "OUTLINE_FILE = BOOK_DIR / \"完整细纲.txt\"\n",
    "TARGET_LEN = 1000  # 可设 5000 或 10000\n",
    "\n",
    "# ===============================================================\n",
    "# ② API 初始化（不加 safety）\n",
    "# ===============================================================\n",
    "genai.configure(api_key=API_KEY)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "\n",
    "# ===============================================================\n",
    "# ③ Function Schema + Prompt\n",
    "# ===============================================================\n",
    "base_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"total_characters\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"total_scenes\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"total_items\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"summary\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"total_characters\", \"total_scenes\", \"total_items\", \"summary\"]\n",
    "}\n",
    "\n",
    "function_declaration = {\n",
    "    \"name\": \"extract_book_outline\",\n",
    "    \"description\": \"提炼整本小说的核心结构信息与剧情大纲\",\n",
    "    \"parameters\": base_schema\n",
    "}\n",
    "\n",
    "prompt_template = textwrap.dedent(f\"\"\"\n",
    "你是一位专业出版编辑。请阅读下面提供的《完整细纲》：\n",
    "\n",
    "任务：\n",
    "1. 去重列出【人物】【场景】【道具】三类信息。\n",
    "2. 写出一篇 ≈ {TARGET_LEN} 字（±10%）的剧情大纲。\n",
    "\n",
    "必须调用 extract_book_outline 函数，返回 JSON 格式。\n",
    "返回字段固定为：\n",
    "  - total_characters\n",
    "  - total_scenes\n",
    "  - total_items\n",
    "  - summary\n",
    "\n",
    "《完整细纲》：\n",
    "{{chapter_text}}\n",
    "\"\"\")\n",
    "\n",
    "# ===============================================================\n",
    "# ④ 调用函数（完全照章节版风格）\n",
    "# ===============================================================\n",
    "def analyze_outline_text(text: str, retries: int = 3) -> dict | None:\n",
    "    full_prompt = prompt_template.format(chapter_text=text)\n",
    "    print(f\"\\n📝 Prompt 预览（前500字）：\\n{textwrap.shorten(full_prompt, width=500)}\\n\")\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            logging.info(f\"🔁 Gemini 第 {attempt + 1}/{retries} 次调用中...\")\n",
    "            rsp = model.generate_content(\n",
    "                full_prompt,\n",
    "                generation_config={\"temperature\": 0.3},\n",
    "                tools=[{\"function_declarations\": [function_declaration]}],\n",
    "                tool_config={'function_calling_config': 'ANY'}\n",
    "            )\n",
    "\n",
    "            candidate = rsp.candidates[0]\n",
    "            logging.info(f\"✅ 接收响应，finish_reason: {candidate.finish_reason}\")\n",
    "\n",
    "            if hasattr(candidate, \"safety_ratings\"):\n",
    "                logging.info(f\"🔒 安全评级: {[r.category.name for r in candidate.safety_ratings]}\")\n",
    "\n",
    "            parts = candidate.content.parts\n",
    "            print(f\"📦 返回内容 parts 数量：{len(parts)}\")\n",
    "\n",
    "            for i, part in enumerate(parts):\n",
    "                print(f\"🔍 Part[{i}] 类型：\", \n",
    "                      \"function_call\" if part.function_call else \"text\", \n",
    "                      \" / 内容预览:\", str(part)[:120].replace(\"\\n\", \" \"))\n",
    "\n",
    "                # -- 主函数解析 --\n",
    "                if part.function_call and part.function_call.name == \"extract_book_outline\":\n",
    "                    logging.info(f\"✅ 发现函数调用 extract_book_outline\")\n",
    "                    args = part.function_call.args\n",
    "                    native = {k: list(v) if hasattr(v, '__iter__') and not isinstance(v, str) else v\n",
    "                              for k, v in dict(args).items()}\n",
    "                    return native\n",
    "\n",
    "            # -- fallback: 纯 JSON 文本 --\n",
    "            if parts and hasattr(parts[0], \"text\") and parts[0].text.strip().startswith(\"{\"):\n",
    "                logging.warning(\"⚠️ 未使用函数调用，尝试 fallback 为纯 JSON 文本解析\")\n",
    "                return json.loads(parts[0].text)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Gemini 调用失败：{e}\", exc_info=True)\n",
    "            time.sleep(2 ** attempt + random.random())\n",
    "\n",
    "    logging.error(\"❌ 所有尝试失败，未获取任何结构化数据\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# ⑤ 运行分析\n",
    "# ===============================================================\n",
    "if not OUTLINE_FILE.exists():\n",
    "    raise FileNotFoundError(f\"未找到细纲文件：{OUTLINE_FILE}\")\n",
    "text = OUTLINE_FILE.read_text(encoding=\"utf-8\")[:300_000]\n",
    "\n",
    "result = analyze_outline_text(text)\n",
    "if result:\n",
    "    print(\"✅ 结果字段：\", list(result.keys()))\n",
    "    # print(\"📌 摘要开头：\", result[\"summary\"][:200], \"...\")\n",
    "    # 保存\n",
    "    output_path = Path(f\"/content/{BOOK_DIR.name}_summary_{TARGET_LEN}.json\")\n",
    "    output_path.write_text(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    print(\"✅ 已保存至：\", output_path)\n",
    "else:\n",
    "    print(\"❌ 未能获得结果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd219dc6-2161-4e11-b652-77b68c19ee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 17:14:47,984 - INFO - 原细纲字数: 66,242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已生成 ≤10 000 字大纲: /content/json_results/json_gemini/30000_summary_gathered/《上品寒士》(校对版全本)作者_贼道三痴_utf8/full_outline_trimmed.txt\n"
     ]
    }
   ],
   "source": [
    "# # ============================================================\n",
    "# # ✅ 20 000 字细纲 → ≤10 000 字大纲（Gemini 2.0 Flash）\n",
    "# #   · 遍历 /content/json_results/json_gemini/30000_summary_gathered\n",
    "# #   · 只处理目标细纲：\n",
    "# #       《上品寒士》(校对版全本)作者_贼道三痴_utf8/完整细纲.txt\n",
    "# #   · 输出：同目录下生成  full_outline_trimmed.txt\n",
    "# # ============================================================\n",
    "\n",
    "# # 0⃣ 依赖\n",
    "# # ------------------------------------------------------------\n",
    "# # !pip install -U google-generativeai chardet tqdm\n",
    "\n",
    "# import os, chardet, logging, time, random, string\n",
    "# from pathlib import Path\n",
    "# from tqdm.auto import tqdm\n",
    "# import google.generativeai as genai\n",
    "\n",
    "# # 1⃣ API 初始化\n",
    "# # ------------------------------------------------------------\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"your-api-key\"\n",
    "# genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "# gemini = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# # 2⃣ 工具函数\n",
    "# # ------------------------------------------------------------\n",
    "# def _auto_decode(p: Path) -> str:\n",
    "#     raw = p.read_bytes()\n",
    "#     enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "#     return raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "# def _rand_tag(k=6):\n",
    "#     return ''.join(random.choices(string.ascii_uppercase, k=k))\n",
    "\n",
    "# def summarize_to_10k(text: str,\n",
    "#                      prompt_header: str,\n",
    "#                      retries: int = 3) -> str:\n",
    "#     base_prompt = f\"{prompt_header}\\n\\n#TAG:{_rand_tag()}\"\n",
    "#     for attempt in range(retries):\n",
    "#         try:\n",
    "#             rsp = gemini.generate_content(\n",
    "#                 [base_prompt, text],\n",
    "#                 generation_config={\"temperature\": 0.2,\n",
    "#                                    \"max_output_tokens\": 8192}\n",
    "#             )\n",
    "#             if rsp.text:\n",
    "#                 return rsp.text.strip()\n",
    "#         except Exception as e:\n",
    "#             logging.warning(f\"Gemini retry {attempt+1}: {e}\")\n",
    "#             time.sleep(2 ** attempt)\n",
    "#     raise RuntimeError(\"Gemini summarization failed\")\n",
    "\n",
    "# # 3⃣ 主流程\n",
    "# # ------------------------------------------------------------\n",
    "# ROOT = Path(\"/content/json_results/json_gemini/30000_summary_gathered\")\n",
    "# target_rel = \"《上品寒士》(校对版全本)作者_贼道三痴_utf8/完整细纲.txt\"\n",
    "# target_path = ROOT / target_rel\n",
    "\n",
    "# if not target_path.exists():\n",
    "#     raise FileNotFoundError(f\"未找到目标文件: {target_path}\")\n",
    "\n",
    "# full_outline = _auto_decode(target_path)\n",
    "# logging.info(f\"原细纲字数: {len(full_outline):,}\")\n",
    "\n",
    "# PROMPT_HEADER = (\n",
    "#     \"你是一位资深中文小说编辑，请将下面约 50 000 字的【整书细纲】\"\n",
    "#     \"精炼为总字数 ≤10000 字的大纲。\\n\"\n",
    "#     \"【输出要求】\\n\"\n",
    "#     \"• 只用简体中文，以章节：大纲情节的格式输出，不要章节这两个字；\\n\"\n",
    "#     \"• 保持情节完整，突出主要人物、冲突、转折、结局；\\n\"\n",
    "#     \"• 不得输出任何额外解释或注释。\\n\"\n",
    "#     \"• 每个段落要精简，必须涵盖从第一章到最后一章的全部细纲内容。\\n\\n\"\n",
    "#     \"【整书细纲】：\"\n",
    "# )\n",
    "\n",
    "# trimmed_outline = summarize_to_10k(full_outline, PROMPT_HEADER)\n",
    "\n",
    "# out_path = target_path.with_name(\"full_outline_trimmed.txt\")\n",
    "# out_path.write_text(trimmed_outline, encoding=\"utf-8\")\n",
    "\n",
    "# print(\"✅ 已生成 ≤10 000 字大纲:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f14e923-8f79-4baa-bff9-d7da0c50ac76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d359cbf59648fb8ee50fa07459e0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "📚 Summarizing Outlines:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:29:29,023 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《奋斗在新明朝》（校对版全本）作者：随轻风去_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:29:42,998 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《反正我是超能力者》（校对版全本）作者：吃书妖_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:29:53,509 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《天可汗》（校对版全本）作者：西风紧_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:30:07,543 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《崩坏世界的传奇大冒险》（精校版全本）作者：国王陛下_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:30:20,531 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《全球进化》（精校版全本）作者：咬狗_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:30:33,232 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《武林半侠传》（校对版全本）作者：文抄公_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:30:43,921 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/国宴大厨在八零/summary_trimmed.txt\n",
      "2025-05-03 14:30:57,842 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《搜神记》（精校版全本）作者：树下野狐_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:31:14,348 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/重生八零：毒妻不好惹/summary_trimmed.txt\n",
      "2025-05-03 14:31:23,134 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《窃明》（校对版全本）作者：大爆炸(灰熊猫)_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:31:34,435 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《蜀山》（精校版全本）作者：流浪的蛤蟆_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:31:43,489 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《陈二狗的妖孽人生》（校对版全本）作者：烽火戏诸侯_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:31:53,708 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《贩罪》（精校版全本）作者：三天两觉_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:32:03,245 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《重生之出人头地》（校对版全本）作者：闹闹不爱闹_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:32:19,828 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/八零喜事：当家肥妻大翻身/summary_trimmed.txt\n",
      "2025-05-03 14:32:32,642 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《肆虐韩娱》（校对版全本）作者：姬叉_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:32:43,611 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/八零年代好时光/summary_trimmed.txt\n",
      "2025-05-03 14:32:52,954 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《食物链顶端的男人》（校对版全本）作者：熊狼狗_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:33:03,559 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《高手寂寞2》（校对版全本）作者：兰帝魅晨_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:33:14,989 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《黑龙法典》（校对版全本）作者：欢声_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:33:23,983 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《诛仙》（校对版全本）作者：萧鼎_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:33:34,850 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《回到过去变成猫》（精校版全本）作者：陈词懒调_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:33:48,012 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《神游》（校对版全本）作者：徐公子胜治_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:33:55,874 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《老子是癞蛤蟆》（校对版全本） 作者：烽火戏诸侯_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:34:06,449 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《未来天王》（校对版全本）作者：陈词懒调_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:34:21,674 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《大画家》（校对版全本）作者：醛石_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:34:33,687 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《超级惊悚直播》作者：宇文长弓_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:34:42,880 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《人道天堂》（校对版全本）作者：荆柯守_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:34:53,284 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《道缘浮图》（校对版全本）作者：烟雨江南_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:35:03,496 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《重活了》（精校版全本）作者： 尝谕_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:35:15,691 - WARNING - Gemini retry 1: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "2025-05-03 14:35:27,786 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《雅骚》（校对版全本）作者：贼道三痴_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:35:39,770 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/八零福星俏媳妇/summary_trimmed.txt\n",
      "2025-05-03 14:35:55,507 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《上品寒士》（校对版全本）作者：贼道三痴_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:36:07,203 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《十州风云志》（校对版全本）作者：知秋_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:36:19,399 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《史上第一混乱》（校对版全本）作者：张小花_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:36:30,241 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《随波逐流之一代军师》（校对版全本）作者：随波逐流_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:36:52,727 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/重生八零：佳妻致富忙/summary_trimmed.txt\n",
      "2025-05-03 14:37:03,484 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/重回八零过好日子/summary_trimmed.txt\n",
      "2025-05-03 14:37:15,983 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《绝对一番》（校对版全本）作者：海底漫步者_utf8/summary_trimmed.txt\n",
      "2025-05-03 14:37:25,670 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/30000_outline_1000/《我的女友是恶女》（校对版全本）作者：海底漫步者_utf8/summary_trimmed.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 All done! Outputs saved to /content/json_results/json_gemini_normalized/30000_outline_1000\n"
     ]
    }
   ],
   "source": [
    "import os, chardet, logging, time, random, string\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 初始化 Gemini（只做一次）\n",
    "def init_gemini(api_key: str):\n",
    "    os.environ[\"GEMINI_API_KEY\"] = api_key\n",
    "    genai.configure(api_key=api_key)\n",
    "    return genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# 解码器\n",
    "def _auto_decode(p: Path) -> str:\n",
    "    raw = p.read_bytes()\n",
    "    enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "    return raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "def _rand_tag(k=6):\n",
    "    return ''.join(random.choices(string.ascii_uppercase, k=k))\n",
    "\n",
    "# 调用 Gemini API\n",
    "def summarize(text: str, prompt: str, gemini, retries=3) -> str:\n",
    "    base_prompt = f\"{prompt.strip()}\\n\\n#TAG:{_rand_tag()}\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            rsp = gemini.generate_content(\n",
    "                [base_prompt, text],\n",
    "                generation_config={\"temperature\": 0.2, \"max_output_tokens\": 8192}\n",
    "            )\n",
    "            if rsp.text:\n",
    "                return rsp.text.strip()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Gemini retry {attempt+1}: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    raise RuntimeError(\"Gemini summarization failed\")\n",
    "\n",
    "# ✅ 主函数接口\n",
    "def summarize_outlines_batch(input_dir: str,\n",
    "                              input_filename: str,\n",
    "                              output_dir: str,\n",
    "                              output_filename: str,\n",
    "                              prompt_text: str,\n",
    "                              gemini_api_key: str):\n",
    "    \"\"\"\n",
    "    批量处理细纲，输入输出目录可分离。\n",
    "    每本书输入路径为 input_dir/书名/input_filename\n",
    "    每本书输出路径为 output_dir/书名/output_filename\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    \n",
    "    gemini = init_gemini(gemini_api_key)\n",
    "    input_root = Path(input_dir)\n",
    "    output_root = Path(output_dir)\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    book_dirs = [d for d in input_root.iterdir() if d.is_dir()]\n",
    "    if not book_dirs:\n",
    "        raise FileNotFoundError(f\"❌ No book folders found in {input_root}\")\n",
    "\n",
    "    for book_dir in tqdm(book_dirs, desc=\"📚 Summarizing Outlines\"):\n",
    "        book_name = book_dir.name\n",
    "        src = book_dir / input_filename\n",
    "        dst_dir = output_root / book_name\n",
    "        dst = dst_dir / output_filename\n",
    "\n",
    "        if not src.exists():\n",
    "            logging.warning(f\"⚠️ Missing file: {src}\")\n",
    "            continue\n",
    "        if dst.exists():\n",
    "            logging.info(f\"✅ Output exists, skipping: {dst}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            content = _auto_decode(src)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"❌ Decode failed for {src}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            trimmed = summarize(content, prompt_text, gemini)\n",
    "            dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "            dst.write_text(trimmed, encoding=\"utf-8\")\n",
    "            logging.info(f\"✅ Written: {dst}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"❌ Summarization failed for {book_name}: {e}\")\n",
    "\n",
    "    print(f\"\\n🎉 All done! Outputs saved to {output_root}\")\n",
    "\n",
    "prompt_10000 = \"\"\"你是一位资深中文小说编辑，请将下面的【整书细纲】精炼为总字数 ≤10 000 字的大纲。\n",
    "【输出要求】\n",
    "• 只用简体中文，以章节：大纲情节的格式输出，不要章节这两个字；\n",
    "• 保持情节完整，突出主要人物、冲突、转折、结局；\n",
    "• 不得输出任何额外解释或注释。\n",
    "• 每个段落要精简，必须涵盖从第一章到最后一章的全部细纲内容。\n",
    "\n",
    "【整书细纲】：\n",
    "\"\"\"\n",
    "\n",
    "prompt_10000 = \"\"\"你是一位资深中文小说编辑，请将下面的【整书细纲】精炼为总字数 ≤10000 字的大纲。\n",
    "【输出要求】\n",
    "• 只用简体中文，以第几章节：大纲情节的格式输出，不要章节这两个字，保留章节号；\n",
    "• 保持情节完整，突出主要人物、冲突、转折、结局；\n",
    "• 不得输出任何额外解释或注释。\n",
    "• 每个段落要精简，必须涵盖从第一章到最后一章的全部细纲内容。\n",
    "不得超过10000字，及每章在25字以内。\n",
    "【整书细纲】：\n",
    "\"\"\"\n",
    "prompt_1000 = \"\"\"你是一位资深中文小说编辑，请将下面的【整书细纲】精炼为总字数 ≤1000 字的大纲。\n",
    "【输出要求】\n",
    "• 只用简体中文，以第几章节：大纲情节的格式输出，不要章节这两个字，保留章节号；\n",
    "• 保持情节完整，突出主要人物、冲突、转折、结局；\n",
    "• 不得输出任何额外解释或注释。\n",
    "• 每个段落要精简，必须涵盖从第一章到最后一章的全部细纲内容。\n",
    "总字数必须不得超过1000字，即每章在5字以内，非常非常简短。200章总结完即停止输出。\n",
    "【整书细纲】：\n",
    "\"\"\"\n",
    "\n",
    "prompt_5000 = \"\"\"你是一位资深中文小说编辑，请将下面的【整书细纲】精炼为总字数 ≤5000 字的大纲。\n",
    "【输出要求】\n",
    "• 只用简体中文，以第几章节：大纲情节的格式输出，不要章节这两个字，保留章节号；\n",
    "• 保持情节完整，突出主要人物、冲突、转折、结局；\n",
    "• 不得输出任何额外解释或注释。\n",
    "• 每个段落要精简，必须涵盖从第一章到最后一章的全部细纲内容。\n",
    "不得超过5000字，即每章在15个字以内。\n",
    "【整书细纲】：\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# summarize_outlines_batch(\n",
    "#     input_dir=\"/content/json_results/json_gemini_normalized/30000_summary_gathered\",\n",
    "#     input_filename=\"entire_section_outline.txt\",\n",
    "#     output_dir=\"/content/json_results/json_gemini_normalized/30000_outline_10000\",\n",
    "#     output_filename=\"summary_trimmed.txt\",\n",
    "#     prompt_text=prompt_10000,\n",
    "#     gemini_api_key=\"your-default-api-key\"\n",
    "# )\n",
    "\n",
    "# summarize_outlines_batch(\n",
    "#     input_dir=\"/content/json_results/json_gemini_normalized/30000_summary_gathered\",\n",
    "#     input_filename=\"entire_section_outline.txt\",\n",
    "#     output_dir=\"/content/json_results/json_gemini_normalized/30000_outline_5000\",\n",
    "#     output_filename=\"summary_trimmed.txt\",\n",
    "#     prompt_text=prompt_5000,\n",
    "#     gemini_api_key=\"your-default-api-key\"\n",
    "# )\n",
    "\n",
    "summarize_outlines_batch(\n",
    "    input_dir=\"/content/json_results/json_gemini_normalized/30000_summary_gathered\",\n",
    "    input_filename=\"entire_section_outline.txt\",\n",
    "    output_dir=\"/content/json_results/json_gemini_normalized/30000_outline_1000\",\n",
    "    output_filename=\"summary_trimmed.txt\",\n",
    "    prompt_text=prompt_1000,\n",
    "    gemini_api_key=\"your-default-api-key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73f9d43f-402c-4592-b21b-65171f892a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/json_results/json_gemini_normalized/30000_outline_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2814e752-42a5-493d-93ac-83f2b322fc26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623fb7d8bcfc426eadca7f4fbc11cef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "📚 Summarizing Outlines:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 11:02:54,609 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《奋斗在新明朝》（校对版全本）作者：随轻风去_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:03:28,911 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《反正我是超能力者》（校对版全本）作者：吃书妖_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:03:43,714 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《天可汗》（校对版全本）作者：西风紧_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:04:17,004 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《崩坏世界的传奇大冒险》（精校版全本）作者：国王陛下_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:04:36,075 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《全球进化》（精校版全本）作者：咬狗_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:04:51,199 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《武林半侠传》（校对版全本）作者：文抄公_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:05:11,754 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/国宴大厨在八零/summary_trimmed.txt\n",
      "2025-05-03 11:05:35,318 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《搜神记》（精校版全本）作者：树下野狐_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:06:06,104 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/重生八零：毒妻不好惹/summary_trimmed.txt\n",
      "2025-05-03 11:06:23,196 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《窃明》（校对版全本）作者：大爆炸(灰熊猫)_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:06:49,208 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《蜀山》（精校版全本）作者：流浪的蛤蟆_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:07:04,425 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《陈二狗的妖孽人生》（校对版全本）作者：烽火戏诸侯_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:07:35,197 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《贩罪》（精校版全本）作者：三天两觉_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:08:04,576 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《重生之出人头地》（校对版全本）作者：闹闹不爱闹_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:08:24,032 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/八零喜事：当家肥妻大翻身/summary_trimmed.txt\n",
      "2025-05-03 11:08:39,889 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《肆虐韩娱》（校对版全本）作者：姬叉_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:09:05,604 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/八零年代好时光/summary_trimmed.txt\n",
      "2025-05-03 11:09:25,242 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《食物链顶端的男人》（校对版全本）作者：熊狼狗_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:10:02,583 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《高手寂寞2》（校对版全本）作者：兰帝魅晨_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:10:25,086 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《黑龙法典》（校对版全本）作者：欢声_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:10:44,209 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《诛仙》（校对版全本）作者：萧鼎_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:11:02,240 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《回到过去变成猫》（精校版全本）作者：陈词懒调_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:11:22,115 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《神游》（校对版全本）作者：徐公子胜治_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:11:41,631 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《老子是癞蛤蟆》（校对版全本） 作者：烽火戏诸侯_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:12:01,624 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《未来天王》（校对版全本）作者：陈词懒调_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:12:30,932 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《大画家》（校对版全本）作者：醛石_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:13:00,969 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《超级惊悚直播》作者：宇文长弓_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:13:25,945 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《人道天堂》（校对版全本）作者：荆柯守_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:13:57,968 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《道缘浮图》（校对版全本）作者：烟雨江南_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:14:18,628 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《重活了》（精校版全本）作者： 尝谕_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:14:45,073 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《雅骚》（校对版全本）作者：贼道三痴_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:15:09,569 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/八零福星俏媳妇/summary_trimmed.txt\n",
      "2025-05-03 11:15:30,190 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《上品寒士》（校对版全本）作者：贼道三痴_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:15:49,271 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《十州风云志》（校对版全本）作者：知秋_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:16:12,023 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《史上第一混乱》（校对版全本）作者：张小花_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:16:37,079 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《随波逐流之一代军师》（校对版全本）作者：随波逐流_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:16:55,972 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/重生八零：佳妻致富忙/summary_trimmed.txt\n",
      "2025-05-03 11:17:16,402 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/重回八零过好日子/summary_trimmed.txt\n",
      "2025-05-03 11:17:35,559 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《绝对一番》（校对版全本）作者：海底漫步者_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:18:00,047 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_10000/《我的女友是恶女》（校对版全本）作者：海底漫步者_utf8/summary_trimmed.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 All done! Outputs saved to /content/json_results/json_gemini_normalized/100000_outline_10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14813ec8acc5460d8d63b928778b5873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "📚 Summarizing Outlines:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 11:18:32,586 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《奋斗在新明朝》（校对版全本）作者：随轻风去_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:18:58,143 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《反正我是超能力者》（校对版全本）作者：吃书妖_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:19:37,091 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《天可汗》（校对版全本）作者：西风紧_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:19:53,281 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《崩坏世界的传奇大冒险》（精校版全本）作者：国王陛下_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:20:05,627 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《全球进化》（精校版全本）作者：咬狗_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:20:21,087 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《武林半侠传》（校对版全本）作者：文抄公_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:20:38,917 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/国宴大厨在八零/summary_trimmed.txt\n",
      "2025-05-03 11:20:58,092 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《搜神记》（精校版全本）作者：树下野狐_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:21:17,746 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/重生八零：毒妻不好惹/summary_trimmed.txt\n",
      "2025-05-03 11:21:42,315 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《窃明》（校对版全本）作者：大爆炸(灰熊猫)_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:21:54,308 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《蜀山》（精校版全本）作者：流浪的蛤蟆_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:22:05,647 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《陈二狗的妖孽人生》（校对版全本）作者：烽火戏诸侯_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:22:23,980 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《贩罪》（精校版全本）作者：三天两觉_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:22:42,816 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《重生之出人头地》（校对版全本）作者：闹闹不爱闹_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:23:06,882 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/八零喜事：当家肥妻大翻身/summary_trimmed.txt\n",
      "2025-05-03 11:23:22,996 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《肆虐韩娱》（校对版全本）作者：姬叉_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:23:48,014 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/八零年代好时光/summary_trimmed.txt\n",
      "2025-05-03 11:24:14,563 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《食物链顶端的男人》（校对版全本）作者：熊狼狗_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:24:42,968 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《高手寂寞2》（校对版全本）作者：兰帝魅晨_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:24:55,921 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《黑龙法典》（校对版全本）作者：欢声_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:25:27,981 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《诛仙》（校对版全本）作者：萧鼎_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:25:45,859 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《回到过去变成猫》（精校版全本）作者：陈词懒调_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:26:24,057 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《神游》（校对版全本）作者：徐公子胜治_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:26:41,643 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《老子是癞蛤蟆》（校对版全本） 作者：烽火戏诸侯_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:27:00,788 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《未来天王》（校对版全本）作者：陈词懒调_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:27:16,126 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《大画家》（校对版全本）作者：醛石_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:27:46,927 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《超级惊悚直播》作者：宇文长弓_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:28:12,887 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《人道天堂》（校对版全本）作者：荆柯守_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:28:29,394 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《道缘浮图》（校对版全本）作者：烟雨江南_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:28:43,943 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《重活了》（精校版全本）作者： 尝谕_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:29:14,119 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《雅骚》（校对版全本）作者：贼道三痴_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:29:31,574 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/八零福星俏媳妇/summary_trimmed.txt\n",
      "2025-05-03 11:29:50,658 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《上品寒士》（校对版全本）作者：贼道三痴_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:30:04,476 - WARNING - Gemini retry 1: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "2025-05-03 11:30:20,533 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《十州风云志》（校对版全本）作者：知秋_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:30:37,982 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《史上第一混乱》（校对版全本）作者：张小花_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:30:54,421 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《随波逐流之一代军师》（校对版全本）作者：随波逐流_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:31:09,309 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/重生八零：佳妻致富忙/summary_trimmed.txt\n",
      "2025-05-03 11:31:30,619 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/重回八零过好日子/summary_trimmed.txt\n",
      "2025-05-03 11:31:49,911 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《绝对一番》（校对版全本）作者：海底漫步者_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:32:06,969 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_5000/《我的女友是恶女》（校对版全本）作者：海底漫步者_utf8/summary_trimmed.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 All done! Outputs saved to /content/json_results/json_gemini_normalized/100000_outline_5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016a54e91bff4c59ba5c4e0bca4d0a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "📚 Summarizing Outlines:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 11:32:19,748 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《奋斗在新明朝》（校对版全本）作者：随轻风去_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:32:31,299 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《反正我是超能力者》（校对版全本）作者：吃书妖_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:33:01,650 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《天可汗》（校对版全本）作者：西风紧_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:33:14,863 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《崩坏世界的传奇大冒险》（精校版全本）作者：国王陛下_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:33:28,179 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《全球进化》（精校版全本）作者：咬狗_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:33:53,035 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《武林半侠传》（校对版全本）作者：文抄公_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:34:08,834 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/国宴大厨在八零/summary_trimmed.txt\n",
      "2025-05-03 11:34:26,689 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《搜神记》（精校版全本）作者：树下野狐_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:34:44,217 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/重生八零：毒妻不好惹/summary_trimmed.txt\n",
      "2025-05-03 11:34:57,470 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《窃明》（校对版全本）作者：大爆炸(灰熊猫)_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:35:12,429 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《蜀山》（精校版全本）作者：流浪的蛤蟆_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:35:27,480 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《陈二狗的妖孽人生》（校对版全本）作者：烽火戏诸侯_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:35:48,475 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《贩罪》（精校版全本）作者：三天两觉_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:36:05,356 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《重生之出人头地》（校对版全本）作者：闹闹不爱闹_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:36:16,768 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/八零喜事：当家肥妻大翻身/summary_trimmed.txt\n",
      "2025-05-03 11:36:27,850 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《肆虐韩娱》（校对版全本）作者：姬叉_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:36:38,329 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/八零年代好时光/summary_trimmed.txt\n",
      "2025-05-03 11:36:49,381 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《食物链顶端的男人》（校对版全本）作者：熊狼狗_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:37:19,990 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《高手寂寞2》（校对版全本）作者：兰帝魅晨_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:37:34,962 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《黑龙法典》（校对版全本）作者：欢声_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:37:46,342 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《诛仙》（校对版全本）作者：萧鼎_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:37:58,216 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《回到过去变成猫》（精校版全本）作者：陈词懒调_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:38:10,330 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《神游》（校对版全本）作者：徐公子胜治_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:38:24,718 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《老子是癞蛤蟆》（校对版全本） 作者：烽火戏诸侯_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:38:40,800 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《未来天王》（校对版全本）作者：陈词懒调_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:39:00,294 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《大画家》（校对版全本）作者：醛石_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:39:34,520 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《超级惊悚直播》作者：宇文长弓_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:40:03,421 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《人道天堂》（校对版全本）作者：荆柯守_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:40:23,441 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《道缘浮图》（校对版全本）作者：烟雨江南_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:40:38,157 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《重活了》（精校版全本）作者： 尝谕_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:40:49,381 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《雅骚》（校对版全本）作者：贼道三痴_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:41:15,703 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/八零福星俏媳妇/summary_trimmed.txt\n",
      "2025-05-03 11:41:38,607 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《上品寒士》（校对版全本）作者：贼道三痴_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:41:47,160 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《十州风云志》（校对版全本）作者：知秋_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:42:05,013 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《史上第一混乱》（校对版全本）作者：张小花_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:42:30,366 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《随波逐流之一代军师》（校对版全本）作者：随波逐流_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:42:49,709 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/重生八零：佳妻致富忙/summary_trimmed.txt\n",
      "2025-05-03 11:42:59,531 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/重回八零过好日子/summary_trimmed.txt\n",
      "2025-05-03 11:43:11,520 - WARNING - Gemini retry 1: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "2025-05-03 11:43:29,176 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《绝对一番》（校对版全本）作者：海底漫步者_utf8/summary_trimmed.txt\n",
      "2025-05-03 11:43:41,364 - INFO - ✅ Written: /content/json_results/json_gemini_normalized/100000_outline_1000/《我的女友是恶女》（校对版全本）作者：海底漫步者_utf8/summary_trimmed.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 All done! Outputs saved to /content/json_results/json_gemini_normalized/100000_outline_1000\n"
     ]
    }
   ],
   "source": [
    "summarize_outlines_batch(\n",
    "    input_dir=\"/content/json_results/json_gemini_normalized/100000_summary_gathered\",\n",
    "    input_filename=\"entire_section_outline.txt\",\n",
    "    output_dir=\"/content/json_results/json_gemini_normalized/100000_outline_10000\",\n",
    "    output_filename=\"summary_trimmed.txt\",\n",
    "    prompt_text=prompt_10000,\n",
    "    gemini_api_key=\"your-default-api-key\"\n",
    ")\n",
    "\n",
    "summarize_outlines_batch(\n",
    "    input_dir=\"/content/json_results/json_gemini_normalized/100000_summary_gathered\",\n",
    "    input_filename=\"entire_section_outline.txt\",\n",
    "    output_dir=\"/content/json_results/json_gemini_normalized/100000_outline_5000\",\n",
    "    output_filename=\"summary_trimmed.txt\",\n",
    "    prompt_text=prompt_5000,\n",
    "    gemini_api_key=\"your-default-api-key\"\n",
    ")\n",
    "\n",
    "summarize_outlines_batch(\n",
    "    input_dir=\"/content/json_results/json_gemini_normalized/100000_summary_gathered\",\n",
    "    input_filename=\"entire_section_outline.txt\",\n",
    "    output_dir=\"/content/json_results/json_gemini_normalized/100000_outline_1000\",\n",
    "    output_filename=\"summary_trimmed.txt\",\n",
    "    prompt_text=prompt_1000,\n",
    "    gemini_api_key=\"your-default-api-key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d56ba92-a3cc-4d05-ad55-aea3ca93e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/json_results/json_gemini_normalized/100000_outline_10000\n",
    "!rm -rf /content/json_results/json_gemini_normalized/100000_outline_1000\n",
    "!rm -rf /content/json_results/json_gemini_normalized/100000_outline_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578292de-cbbd-4437-ae51-9ce1d19290c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
