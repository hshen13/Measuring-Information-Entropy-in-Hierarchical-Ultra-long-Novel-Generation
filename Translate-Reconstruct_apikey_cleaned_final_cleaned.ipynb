{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d332418-7ca6-4a7b-ba70-3034134e8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 0âƒ£ï¸ å®‰è£…ä¾èµ–\n",
    "# ============================================================== \n",
    "!pip -q install --upgrade openai chardet tqdm\n",
    "\n",
    "# ==============================================================\n",
    "# 1âƒ£ï¸ DeepSeek API åŸºæœ¬é…ç½®\n",
    "# ============================================================== \n",
    "import os, re, json, unicodedata, datetime, time\n",
    "from pathlib import Path\n",
    "from typing  import List, Tuple, Union\n",
    "import openai, chardet\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "DEEPSEEK_API_KEY  = \"your-default-api-key\"\n",
    "DEEPSEEK_URL      = \"https://api.deepseek.com\"\n",
    "DEEPSEEK_MODEL    = \"deepseek-chat\"\n",
    "\n",
    "openai.api_key = DEEPSEEK_API_KEY\n",
    "deep_client    = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_URL)\n",
    "\n",
    "# ==============================================================\n",
    "# 2âƒ£ï¸ ç”¨æˆ·å‚æ•°\n",
    "# ============================================================== \n",
    "SRC_ROOT        = \"/content/novels_chapters\"        # â† ä¸€çº§ç›®å½•ï¼›å…¶ä¸‹æ˜¯â€œå°è¯´åâ€å­ç›®å½•\n",
    "OUT_EN_ROOT     = \"/content/translate_EN\"           # â† è‹±æ–‡è¯‘æ–‡æ ¹ç›®å½•\n",
    "OUT_ZH_ROOT     = \"/content/translate_back_ZH\"      # â† å›è¯‘ä¸­æ–‡æ ¹ç›®å½•\n",
    "CHAPTER_RANGE   = [1,3,5,25,50,75,100,125]                          # â† ç« èŠ‚èŒƒå›´æˆ– [3,7,9]\n",
    "WORKERS         = 2                              # å¹¶è¡Œçº¿ç¨‹\n",
    "\n",
    "PROMPT_ZH2EN = \"You are a professional translator. Translate the following Chinese literary text into vivid, fluent English. Keep paragraph breaks. Output ONLY the translation:\"\n",
    "PROMPT_EN2ZH = \"ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–‡å­¦è¯‘è€…ï¼Œè¯·å°†ä¸‹é¢çš„è‹±æ–‡æ–‡æœ¬ç²¾å‡†åœ°è¯‘å›ä¸­æ–‡ï¼Œä¿æŒæ®µè½åˆ’åˆ†ï¼Œä¸è¦åŠ å…¥é¢å¤–è¯´æ˜ï¼Œåªè¾“å‡ºè¯‘æ–‡ï¼š\"\n",
    "\n",
    "# ==============================================================\n",
    "# 3âƒ£ï¸ å·¥å…·å‡½æ•°\n",
    "# ============================================================== \n",
    "_CHAP_NO_RE = re.compile(r\"(\\d{1,4})\")\n",
    "\n",
    "def _chapter_no(path: Path) -> int:\n",
    "    m = _CHAP_NO_RE.search(path.stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"æ–‡ä»¶åæ— æ³•è¯†åˆ«ç« èŠ‚å·: {path}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def _auto_read(path: Path) -> str:\n",
    "    raw = path.read_bytes()\n",
    "    enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "    return raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "def _call_deepseek(prompt: str, text: str) -> str:\n",
    "    rsp = deep_client.chat.completions.create(\n",
    "        model=DEEPSEEK_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt},\n",
    "                  {\"role\": \"user\",   \"content\": text}],\n",
    "        temperature=0.1,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    return rsp.choices[0].message.content.strip()\n",
    "\n",
    "# ==============================================================\n",
    "# 4âƒ£ï¸ ç¿»è¯‘å•æœ¬å°è¯´ translate_one_book()\n",
    "# ============================================================== \n",
    "def translate_one_book(book_dir: Path,\n",
    "                       out_en_root: Path,\n",
    "                       out_zh_root: Path,\n",
    "                       chapters: Union[Tuple[int, int], List[int]],\n",
    "                       workers: int = 4):\n",
    "    if isinstance(chapters, tuple):\n",
    "        s, e = chapters\n",
    "        chap_set = set(range(s, e + 1))\n",
    "    else:\n",
    "        chap_set = set(chapters)\n",
    "\n",
    "    #â”€â”€ è¯»å–ç« èŠ‚æ–‡ä»¶ & æ ¡éªŒç¼ºå¤± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    file_pairs = []\n",
    "    for fp in book_dir.glob(\"*.txt\"):\n",
    "        no = _chapter_no(fp)\n",
    "        if no in chap_set:\n",
    "            file_pairs.append((no, fp))\n",
    "            chap_set.remove(no)\n",
    "\n",
    "    if chap_set:\n",
    "        raise FileNotFoundError(f\"ã€Š{book_dir.name}ã€‹ç¼ºå°‘ç« èŠ‚: {sorted(chap_set)}\")\n",
    "\n",
    "    file_pairs.sort()       # ç« èŠ‚å·æ’åº\n",
    "\n",
    "    #â”€â”€ å‡†å¤‡è¾“å‡ºå­ç›®å½• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    en_dir = out_en_root / book_dir.name\n",
    "    zh_dir = out_zh_root / book_dir.name\n",
    "    en_dir.mkdir(parents=True, exist_ok=True)\n",
    "    zh_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #â”€â”€ ç¿»è¯‘å‡½æ•°ï¼ˆå•ç« ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def _translate(no: int, fp: Path):\n",
    "        zh_text = _auto_read(fp)\n",
    "\n",
    "        # zh â†’ en\n",
    "        en_text = _call_deepseek(PROMPT_ZH2EN, zh_text)\n",
    "        en_path = en_dir / f\"{fp.stem}_en.txt\"\n",
    "        en_path.write_text(en_text, encoding=\"utf-8\")\n",
    "\n",
    "        # en â†’ zh (back-translation)\n",
    "        zh_back = _call_deepseek(PROMPT_EN2ZH, en_text)\n",
    "        zh_path = zh_dir / f\"{fp.stem}_back_cn.txt\"\n",
    "        zh_path.write_text(zh_back, encoding=\"utf-8\")\n",
    "        return fp.name\n",
    "\n",
    "    #â”€â”€ å¹¶è¡Œæ‰§è¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        list(tqdm(ex.map(lambda t: _translate(*t), file_pairs),\n",
    "                  total=len(file_pairs),\n",
    "                  desc=f\"ğŸ”„ {book_dir.name}\"))\n",
    "\n",
    "# ==============================================================\n",
    "# 5âƒ£ï¸ ç¿»è¯‘å…¨é›† translate_library()\n",
    "# ============================================================== \n",
    "def translate_library(src_root: str,\n",
    "                      out_en_root: str,\n",
    "                      out_zh_root: str,\n",
    "                      chapters: Union[Tuple[int, int], List[int]],\n",
    "                      workers: int = 4):\n",
    "    src_root = Path(src_root)\n",
    "    out_en_root = Path(out_en_root)\n",
    "    out_zh_root = Path(out_zh_root)\n",
    "\n",
    "    novels = [d for d in src_root.iterdir() if d.is_dir()]\n",
    "    if not novels:\n",
    "        raise FileNotFoundError(f\"{src_root} ä¸‹æœªæ‰¾åˆ°ä»»ä½•å°è¯´å­ç›®å½•\")\n",
    "\n",
    "    for book in novels:\n",
    "        translate_one_book(book, out_en_root, out_zh_root, chapters, workers)\n",
    "\n",
    "    print(\"\\nâœ… å…¨éƒ¨å°è¯´ç¿»è¯‘å®Œæˆ\")\n",
    "    print(f\"è‹±æ–‡è¯‘æ–‡ç›®å½• : {out_en_root}\")\n",
    "    print(f\"ä¸­æ–‡å›è¯‘ç›®å½• : {out_zh_root}\")\n",
    "\n",
    "# ==============================================================\n",
    "# 6âƒ£ï¸ è¿è¡Œ\n",
    "# ============================================================== \n",
    "translate_library(SRC_ROOT,\n",
    "                  OUT_EN_ROOT,\n",
    "                  OUT_ZH_ROOT,\n",
    "                  CHAPTER_RANGE,\n",
    "                  workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784d6b7a-56f5-4b9e-93eb-cd51b24a5e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5769dbc6772b4841bf40673fe70f7add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šå¥‹æ–—åœ¨æ–°æ˜æœã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šéšè½»é£å»_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b50d39443ab4871a6512f3c39559a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šåæ­£æˆ‘æ˜¯è¶…èƒ½åŠ›è€…ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šåƒä¹¦å¦–_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbbdd0d7c8442c7b5dcd4460e1ca772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šå¤©å¯æ±—ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè¥¿é£ç´§_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8294352159ed45d9929fc9487e857320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šå´©åä¸–ç•Œçš„ä¼ å¥‡å¤§å†’é™©ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå›½ç‹é™›ä¸‹_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014981c49bae4db8b809501e1c38665a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šå…¨çƒè¿›åŒ–ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå’¬ç‹—_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abd2702c65047c19e0324cdd2072537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šæ­¦æ—åŠä¾ ä¼ ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæ–‡æŠ„å…¬_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m, in \u001b[0;36mtranslate_one_book\u001b[0;34m(book_dir, out_en_root, out_zh_root, chapters, workers)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_pairs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_pairs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mğŸ”„ \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbook_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 144\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mä¸­æ–‡å›è¯‘ç›®å½• : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_zh_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# ==============================================================\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# 6âƒ£ï¸ è¿è¡Œç¿»è¯‘ä»»åŠ¡\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# ============================================================== \u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[43mtranslate_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSRC_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mOUT_EN_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mOUT_ZH_ROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mchapter_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHAPTER_RANGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mn_chapters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_CHAPTERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWORKERS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 135\u001b[0m, in \u001b[0;36mtranslate_library\u001b[0;34m(src_root, out_en_root, out_zh_root, chapter_range, n_chapters, workers)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m book \u001b[38;5;129;01min\u001b[39;00m novels:\n\u001b[1;32m    134\u001b[0m     chapters_sample \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(full_range, k\u001b[38;5;241m=\u001b[39mn_chapters)\n\u001b[0;32m--> 135\u001b[0m     \u001b[43mtranslate_one_book\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_en_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_zh_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchapters_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… å…¨éƒ¨å°è¯´ç¿»è¯‘å®Œæˆ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè‹±æ–‡è¯‘æ–‡ç›®å½• : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_en_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 102\u001b[0m, in \u001b[0;36mtranslate_one_book\u001b[0;34m(book_dir, out_en_root, out_zh_root, chapters, workers)\u001b[0m\n\u001b[1;32m     99\u001b[0m     zh_path\u001b[38;5;241m.\u001b[39mwrite_text(zh_back, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fp\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mlist\u001b[39m(tqdm(ex\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m t: _translate(\u001b[38;5;241m*\u001b[39mt), file_pairs),\n\u001b[1;32m    104\u001b[0m               total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(file_pairs),\n\u001b[1;32m    105\u001b[0m               desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”„ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbook_dir\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 0âƒ£ï¸ å®‰è£…ä¾èµ–\n",
    "# ============================================================== \n",
    "!pip -q install --upgrade openai chardet tqdm\n",
    "\n",
    "# ==============================================================\n",
    "# 1âƒ£ï¸ DeepSeek API åŸºæœ¬é…ç½®\n",
    "# ============================================================== \n",
    "import os, re, json, unicodedata, datetime, time, random\n",
    "from pathlib import Path\n",
    "from typing  import List, Tuple, Union\n",
    "import openai, chardet\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "DEEPSEEK_API_KEY  = \"your-default-api-key\"\n",
    "DEEPSEEK_URL      = \"https://api.deepseek.com\"\n",
    "DEEPSEEK_MODEL    = \"deepseek-chat\"\n",
    "\n",
    "openai.api_key = DEEPSEEK_API_KEY\n",
    "deep_client    = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_URL)\n",
    "\n",
    "# ==============================================================\n",
    "# 2âƒ£ï¸ ç”¨æˆ·å‚æ•°\n",
    "# ============================================================== \n",
    "SRC_ROOT        = \"/content/novels_normalized\"\n",
    "OUT_EN_ROOT     = \"/content/translate_EN_normalized\"\n",
    "OUT_ZH_ROOT     = \"/content/translate_back_ZH_normalized\"\n",
    "CHAPTER_RANGE   = (1, 200)       # âœ… å¯ä¸º tuple(start, end) æˆ– listï¼Œå¦‚ [1,3,5,10]\n",
    "N_CHAPTERS      = 8\n",
    "WORKERS         = 2\n",
    "\n",
    "PROMPT_ZH2EN = \"You are a professional translator. Translate the following Chinese literary text into vivid, fluent English. Keep paragraph breaks. Output ONLY the translation:\"\n",
    "PROMPT_EN2ZH = \"ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–‡å­¦è¯‘è€…ï¼Œè¯·å°†ä¸‹é¢çš„è‹±æ–‡æ–‡æœ¬ç²¾å‡†åœ°è¯‘å›ä¸­æ–‡ï¼Œä¿æŒæ®µè½åˆ’åˆ†ï¼Œä¸è¦åŠ å…¥é¢å¤–è¯´æ˜ï¼Œåªè¾“å‡ºè¯‘æ–‡ï¼š\"\n",
    "\n",
    "# ==============================================================\n",
    "# 3âƒ£ï¸ å·¥å…·å‡½æ•°\n",
    "# ============================================================== \n",
    "_CHAP_NO_RE = re.compile(r\"(\\d{1,4})\")\n",
    "\n",
    "def _chapter_no(path: Path) -> int:\n",
    "    m = _CHAP_NO_RE.search(path.stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"æ–‡ä»¶åæ— æ³•è¯†åˆ«ç« èŠ‚å·: {path}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def _auto_read(path: Path) -> str:\n",
    "    raw = path.read_bytes()\n",
    "    enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "    return raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "def _call_deepseek(prompt: str, text: str) -> str:\n",
    "    rsp = deep_client.chat.completions.create(\n",
    "        model=DEEPSEEK_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt},\n",
    "                  {\"role\": \"user\",   \"content\": text}],\n",
    "        temperature=0.1,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    return rsp.choices[0].message.content.strip()\n",
    "\n",
    "# ==============================================================\n",
    "# 4âƒ£ï¸ ç¿»è¯‘å•æœ¬å°è¯´ translate_one_book()\n",
    "# ============================================================== \n",
    "def translate_one_book(book_dir: Path,\n",
    "                       out_en_root: Path,\n",
    "                       out_zh_root: Path,\n",
    "                       chapters: List[int],\n",
    "                       workers: int = 4):\n",
    "\n",
    "    chap_set = set(chapters)\n",
    "\n",
    "    file_pairs = []\n",
    "    for fp in book_dir.glob(\"*.txt\"):\n",
    "        no = _chapter_no(fp)\n",
    "        if no in chap_set:\n",
    "            file_pairs.append((no, fp))\n",
    "            chap_set.remove(no)\n",
    "\n",
    "    if chap_set:\n",
    "        raise FileNotFoundError(f\"ã€Š{book_dir.name}ã€‹ç¼ºå°‘ç« èŠ‚: {sorted(chap_set)}\")\n",
    "\n",
    "    file_pairs.sort()\n",
    "\n",
    "    en_dir = out_en_root / book_dir.name\n",
    "    zh_dir = out_zh_root / book_dir.name\n",
    "    en_dir.mkdir(parents=True, exist_ok=True)\n",
    "    zh_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _translate(no: int, fp: Path):\n",
    "        zh_text = _auto_read(fp)\n",
    "\n",
    "        en_text = _call_deepseek(PROMPT_ZH2EN, zh_text)\n",
    "        en_path = en_dir / f\"{fp.stem}_en.txt\"\n",
    "        en_path.write_text(en_text, encoding=\"utf-8\")\n",
    "\n",
    "        zh_back = _call_deepseek(PROMPT_EN2ZH, en_text)\n",
    "        zh_path = zh_dir / f\"{fp.stem}_back_cn.txt\"\n",
    "        zh_path.write_text(zh_back, encoding=\"utf-8\")\n",
    "        return fp.name\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        list(tqdm(ex.map(lambda t: _translate(*t), file_pairs),\n",
    "                  total=len(file_pairs),\n",
    "                  desc=f\"ğŸ”„ {book_dir.name}\"))\n",
    "\n",
    "# ==============================================================\n",
    "# 5âƒ£ï¸ ç¿»è¯‘å…¨é›† translate_library()\n",
    "# ============================================================== \n",
    "def translate_library(src_root: str,\n",
    "                      out_en_root: str,\n",
    "                      out_zh_root: str,\n",
    "                      chapter_range: Union[Tuple[int, int], List[int]],\n",
    "                      n_chapters: int,\n",
    "                      workers: int = 4):\n",
    "    src_root = Path(src_root)\n",
    "    out_en_root = Path(out_en_root)\n",
    "    out_zh_root = Path(out_zh_root)\n",
    "\n",
    "    # âœ… è½¬æ¢ç« èŠ‚èŒƒå›´\n",
    "    if isinstance(chapter_range, tuple):\n",
    "        full_range = list(range(chapter_range[0], chapter_range[1] + 1))\n",
    "    else:\n",
    "        full_range = list(chapter_range)\n",
    "\n",
    "    if n_chapters > len(full_range):\n",
    "        raise ValueError(f\"æŒ‡å®šç« èŠ‚æ•° n_chapters={n_chapters} è¶…è¿‡ç« èŠ‚èŒƒå›´é•¿åº¦ {len(full_range)}\")\n",
    "\n",
    "    novels = [d for d in src_root.iterdir() if d.is_dir()]\n",
    "    if not novels:\n",
    "        raise FileNotFoundError(f\"{src_root} ä¸‹æœªæ‰¾åˆ°ä»»ä½•å°è¯´å­ç›®å½•\")\n",
    "\n",
    "    for book in novels:\n",
    "        chapters_sample = random.sample(full_range, k=n_chapters)\n",
    "        translate_one_book(book, out_en_root, out_zh_root, chapters_sample, workers)\n",
    "\n",
    "    print(\"\\nâœ… å…¨éƒ¨å°è¯´ç¿»è¯‘å®Œæˆ\")\n",
    "    print(f\"è‹±æ–‡è¯‘æ–‡ç›®å½• : {out_en_root}\")\n",
    "    print(f\"ä¸­æ–‡å›è¯‘ç›®å½• : {out_zh_root}\")\n",
    "\n",
    "# ==============================================================\n",
    "# 6âƒ£ï¸ è¿è¡Œç¿»è¯‘ä»»åŠ¡\n",
    "# ============================================================== \n",
    "translate_library(SRC_ROOT,\n",
    "                  OUT_EN_ROOT,\n",
    "                  OUT_ZH_ROOT,\n",
    "                  chapter_range=CHAPTER_RANGE,\n",
    "                  n_chapters=N_CHAPTERS,\n",
    "                  workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbb65b-693b-4564-a208-7d750074b9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949c7367-be36-4027-b1b7-a29f02ad087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æ‰€æœ‰ç« èŠ‚è®°å½•å·²ä¿å­˜åˆ° randomseed æ–‡ä»¶å¤¹ï¼ˆç»Ÿä¸€å‘½åä¸º *_randomseed.txtï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# è·¯å¾„é…ç½®\n",
    "TRANSLATED_ROOT = Path(\"/content/translate_EN\")\n",
    "NOVELS_DIR      = Path(\"/content/novels\")\n",
    "OUTPUT_DIR      = Path(\"/content/randomseed\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# åŒ¹é…ç« èŠ‚å·ç”¨çš„æ­£åˆ™\n",
    "chapter_num_re = re.compile(r\"(\\d{1,4})\")\n",
    "\n",
    "# æå–ç« èŠ‚å·\n",
    "def extract_chapter_no(filename: str) -> int:\n",
    "    match = chapter_num_re.search(filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"æ— æ³•è§£æç« èŠ‚å·: {filename}\")\n",
    "\n",
    "# æ ‡å‡†åŒ–ä¹¦åï¼ˆå»é™¤æ ‡ç‚¹ã€ç©ºæ ¼ã€å…¨è§’ç¬¦å·ç­‰ï¼‰\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"\\.txt$\", \"\", text)  # å»æ‰.txtåç¼€\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[_\\-ï¼š:ï¼ˆï¼‰\\(\\)\\[\\]ã€ã€‘Â·â€¢â€”â€¦,.!ï¼?ï¼Ÿ\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# åŒ¹é…ç¿»è¯‘ç›®å½•ååˆ°åŸå§‹ä¹¦åï¼ˆå°½é‡è´´åˆåŸæ–‡ï¼‰\n",
    "def match_original_title(trans_name: str, original_names: list) -> str:\n",
    "    norm_trans = normalize(trans_name)\n",
    "    original_map = {normalize(name): name for name in original_names}\n",
    "    return original_map.get(norm_trans, trans_name)\n",
    "\n",
    "# è·å–åŸå§‹ä¹¦ååˆ—è¡¨ï¼ˆæ”¯æŒæ–‡ä»¶æˆ–ç›®å½•ï¼‰\n",
    "original_titles = [d.name for d in NOVELS_DIR.iterdir() if d.is_file() or d.is_dir()]\n",
    "\n",
    "unmatched_books = []\n",
    "\n",
    "# éå†ç¿»è¯‘ç›®å½•\n",
    "for book_dir in TRANSLATED_ROOT.iterdir():\n",
    "    if not book_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    chapter_nos = []\n",
    "    for file in book_dir.glob(\"*_en.txt\"):\n",
    "        try:\n",
    "            no = extract_chapter_no(file.name)\n",
    "            chapter_nos.append(no)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    chapter_nos = sorted(set(chapter_nos))\n",
    "    if not chapter_nos:\n",
    "        continue\n",
    "\n",
    "    matched_name = match_original_title(book_dir.name, original_titles)\n",
    "    if matched_name == book_dir.name:\n",
    "        unmatched_books.append(book_dir.name)\n",
    "\n",
    "    # âœ… æ„é€ æ–‡ä»¶åå¹¶åŠ  _randomseed.txt åç¼€ï¼Œé¿å…é‡å¤\n",
    "    final_name = matched_name.rsplit(\".txt\", 1)[0] + \"_randomseed.txt\"\n",
    "    out_path = OUTPUT_DIR / final_name\n",
    "\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(str(no) for no in chapter_nos))\n",
    "\n",
    "# âœ… ä¿å­˜æœªåŒ¹é…ä¹¦åæ¸…å•\n",
    "if unmatched_books:\n",
    "    unmatched_path = OUTPUT_DIR / \"unmatched_books.txt\"\n",
    "    with unmatched_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ä»¥ä¸‹ä¹¦åæœªåŒ¹é…åˆ° novels åŸå§‹æ ‡é¢˜ï¼š\\n\\n\")\n",
    "        for name in unmatched_books:\n",
    "            f.write(f\"{name}\\n\")\n",
    "    print(f\"\\nğŸ“„ æœªåŒ¹é…ä¹¦åæ¸…å•å·²ä¿å­˜åˆ°ï¼š{unmatched_path}\")\n",
    "\n",
    "print(\"\\nâœ… æ‰€æœ‰ç« èŠ‚è®°å½•å·²ä¿å­˜åˆ° randomseed æ–‡ä»¶å¤¹ï¼ˆç»Ÿä¸€å‘½åä¸º *_randomseed.txtï¼‰ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb339c4-5c30-49a4-83c7-7010f038d4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è·³è¿‡ã€Šã€Šå¥‹æ–—åœ¨æ–°æ˜æœã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šéšè½»é£å»_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šåæ­£æˆ‘æ˜¯è¶…èƒ½åŠ›è€…ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šåƒä¹¦å¦–_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šå¤©å¯æ±—ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè¥¿é£ç´§_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šå´©åä¸–ç•Œçš„ä¼ å¥‡å¤§å†’é™©ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå›½ç‹é™›ä¸‹_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šå…¨çƒè¿›åŒ–ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå’¬ç‹—_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šæ­¦æ—åŠä¾ ä¼ ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæ–‡æŠ„å…¬_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šå›½å®´å¤§å¨åœ¨å…«é›¶ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šæœç¥è®°ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæ ‘ä¸‹é‡ç‹_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šé‡ç”Ÿå…«é›¶ï¼šæ¯’å¦»ä¸å¥½æƒ¹ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šçªƒæ˜ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå¤§çˆ†ç‚¸(ç°ç†ŠçŒ«)_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šèœ€å±±ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæµæµªçš„è›¤èŸ†_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šé™ˆäºŒç‹—çš„å¦–å­½äººç”Ÿã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šçƒ½ç«æˆè¯¸ä¾¯_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šè´©ç½ªã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šä¸‰å¤©ä¸¤è§‰_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šé‡ç”Ÿä¹‹å‡ºäººå¤´åœ°ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šé—¹é—¹ä¸çˆ±é—¹_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šå…«é›¶å–œäº‹ï¼šå½“å®¶è‚¥å¦»å¤§ç¿»èº«ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šè‚†è™éŸ©å¨±ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå§¬å‰_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šå…«é›¶å¹´ä»£å¥½æ—¶å…‰ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šé£Ÿç‰©é“¾é¡¶ç«¯çš„ç”·äººã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šç†Šç‹¼ç‹—_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šé«˜æ‰‹å¯‚å¯2ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå…°å¸é­…æ™¨_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šé»‘é¾™æ³•å…¸ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæ¬¢å£°_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šè¯›ä»™ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè§é¼_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šå›åˆ°è¿‡å»å˜æˆçŒ«ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šé™ˆè¯æ‡’è°ƒ_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šç¥æ¸¸ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå¾å…¬å­èƒœæ²»_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šè€å­æ˜¯ç™è›¤èŸ†ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ ä½œè€…ï¼šçƒ½ç«æˆè¯¸ä¾¯_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šæœªæ¥å¤©ç‹ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šé™ˆè¯æ‡’è°ƒ_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šå¤§ç”»å®¶ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šé†›çŸ³_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šè¶…çº§æƒŠæ‚šç›´æ’­ã€‹ä½œè€…ï¼šå®‡æ–‡é•¿å¼“_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šäººé“å¤©å ‚ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè†æŸ¯å®ˆ_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šé“ç¼˜æµ®å›¾ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šçƒŸé›¨æ±Ÿå—_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šé‡æ´»äº†ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼š å°è°•_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šã€Šé›…éªšã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè´¼é“ä¸‰ç—´_utf8ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n",
      "âœ… è·³è¿‡ã€Šå…«é›¶ç¦æ˜Ÿä¿åª³å¦‡ã€‹ï¼Œå·²æœ‰ 8 ä¸ªç« èŠ‚ç¿»è¯‘\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97857faef9924f98a85544f64d4c2892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šä¸Šå“å¯’å£«ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè´¼é“ä¸‰ç—´_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 0âƒ£ï¸ å®‰è£…ä¾èµ–\n",
    "# ============================================================== \n",
    "!pip -q install --upgrade openai google-generativeai chardet tqdm\n",
    "\n",
    "# ==============================================================\n",
    "# 1âƒ£ï¸ é…ç½®\n",
    "# ============================================================== \n",
    "import os, re, json, unicodedata, time, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "from tqdm.auto import tqdm\n",
    "import chardet\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# DeepSeek\n",
    "import openai\n",
    "DEEPSEEK_API_KEY  = \"your-default-api-key\"\n",
    "DEEPSEEK_URL      = \"https://api.deepseek.com\"\n",
    "openai.api_key    = DEEPSEEK_API_KEY\n",
    "deep_client       = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_URL)\n",
    "\n",
    "# Gemini\n",
    "import google.generativeai as genai\n",
    "GEMINI_API_KEY     = \"your-default-api-key\"\n",
    "GEMINI_MODEL       = \"gemini-2.0-flash\"  # æ¨è pro\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "\n",
    "# ç¿»è¯‘æ§åˆ¶å‚æ•°\n",
    "TRANSLATION_ENGINE = \"gemini\"  # å¯é€‰ï¼š \"deepseek\" / \"gemini\"\n",
    "\n",
    "# è·¯å¾„å‚æ•°\n",
    "SRC_ROOT    = \"/content/novels_normalized\"\n",
    "OUT_EN_ROOT = \"/content/translate_EN_normalized\"\n",
    "OUT_ZH_ROOT = \"/content/translate_back_ZH_normalized\"\n",
    "CHAPTER_RANGE = (1, 200)\n",
    "N_CHAPTERS    = 8\n",
    "WORKERS       = 2\n",
    "\n",
    "PROMPT_ZH2EN = \"You are a professional translator. Translate the following Chinese literary text into vivid, fluent English. Keep paragraph breaks. Output ONLY the translation:\"\n",
    "PROMPT_EN2ZH = \"ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–‡å­¦è¯‘è€…ï¼Œè¯·å°†ä¸‹é¢çš„è‹±æ–‡æ–‡æœ¬ç²¾å‡†åœ°è¯‘å›ä¸­æ–‡ï¼Œä¿æŒæ®µè½åˆ’åˆ†ï¼Œä¸è¦åŠ å…¥é¢å¤–è¯´æ˜ï¼Œåªè¾“å‡ºè¯‘æ–‡ï¼š\"\n",
    "\n",
    "# ==============================================================\n",
    "# 2âƒ£ï¸ å·¥å…·å‡½æ•°\n",
    "# ============================================================== \n",
    "_CHAP_NO_RE = re.compile(r\"(\\d{1,4})\")\n",
    "\n",
    "def _chapter_no(path: Path) -> int:\n",
    "    m = _CHAP_NO_RE.search(path.stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"æ–‡ä»¶åæ— æ³•è¯†åˆ«ç« èŠ‚å·: {path}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def _auto_read(path: Path) -> str:\n",
    "    raw = path.read_bytes()\n",
    "    enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "    return raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "def _call_deepseek(prompt: str, text: str) -> str:\n",
    "    rsp = deep_client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt},\n",
    "                  {\"role\": \"user\",   \"content\": text}],\n",
    "        temperature=0.1,\n",
    "        max_tokens=8092\n",
    "    )\n",
    "    return rsp.choices[0].message.content.strip()\n",
    "\n",
    "def _call_gemini(prompt: str, text: str) -> str:\n",
    "    full_prompt = f\"{prompt.strip()}\\n\\n{text.strip()}\"\n",
    "    rsp = gemini_model.generate_content(full_prompt, generation_config={\"temperature\": 0.3})\n",
    "    return rsp.text.strip()\n",
    "\n",
    "def _call_translate(prompt: str, text: str, engine: str) -> str:\n",
    "    if engine == \"gemini\":\n",
    "        return _call_gemini(prompt, text)\n",
    "    elif engine == \"deepseek\":\n",
    "        return _call_deepseek(prompt, text)\n",
    "    else:\n",
    "        raise ValueError(f\"æœªçŸ¥ç¿»è¯‘å¼•æ“: {engine}\")\n",
    "    \n",
    "# ==============================================================\n",
    "# 3âƒ£ï¸ ç¿»è¯‘å•æœ¬å°è¯´\n",
    "# ============================================================== \n",
    "def translate_one_book(book_dir: Path,\n",
    "                       out_en_root: Path,\n",
    "                       out_zh_root: Path,\n",
    "                       chapters: List[int],\n",
    "                       engine: str = \"deepseek\",\n",
    "                       workers: int = 4):\n",
    "    \"\"\"\n",
    "    ç¿»è¯‘å•æœ¬å°è¯´çš„æŒ‡å®šç« èŠ‚ã€‚å¦‚æœå·²å­˜åœ¨è‹±æ–‡å’Œå›è¯‘æ–‡ä»¶åˆ™è·³è¿‡ã€‚\n",
    "    \"\"\"\n",
    "    chap_set = set(chapters)\n",
    "    file_pairs = []\n",
    "    for fp in book_dir.glob(\"*.txt\"):\n",
    "        no = _chapter_no(fp)\n",
    "        if no in chap_set:\n",
    "            file_pairs.append((no, fp))\n",
    "            chap_set.remove(no)\n",
    "\n",
    "    if chap_set:\n",
    "        raise FileNotFoundError(f\"ã€Š{book_dir.name}ã€‹ç¼ºå°‘ç« èŠ‚: {sorted(chap_set)}\")\n",
    "\n",
    "    file_pairs.sort()\n",
    "    en_dir = out_en_root / book_dir.name\n",
    "    zh_dir = out_zh_root / book_dir.name\n",
    "    en_dir.mkdir(parents=True, exist_ok=True)\n",
    "    zh_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _translate(no: int, fp: Path):\n",
    "        en_path = en_dir / f\"{fp.stem}_en.txt\"\n",
    "        zh_path = zh_dir / f\"{fp.stem}_back_cn.txt\"\n",
    "\n",
    "        # è·³è¿‡å·²å­˜åœ¨çš„ç¿»è¯‘ç»“æœ\n",
    "        if en_path.exists() and zh_path.exists():\n",
    "            return f\"âœ… skipped: {fp.name}\"\n",
    "\n",
    "        zh_text = _auto_read(fp)\n",
    "\n",
    "        # ä¸­æ–‡ â†’ è‹±æ–‡\n",
    "        en_text = _call_translate(PROMPT_ZH2EN, zh_text, engine)\n",
    "        en_path.write_text(en_text, encoding=\"utf-8\")\n",
    "\n",
    "        # è‹±æ–‡ â†’ ä¸­æ–‡å›è¯‘\n",
    "        zh_back = _call_translate(PROMPT_EN2ZH, en_text, engine)\n",
    "        zh_path.write_text(zh_back, encoding=\"utf-8\")\n",
    "\n",
    "        return f\"âœ… translated: {fp.name}\"\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        results = list(tqdm(ex.map(lambda t: _translate(*t), file_pairs),\n",
    "                            total=len(file_pairs),\n",
    "                            desc=f\"ğŸ”„ {book_dir.name}\"))\n",
    "    for line in results:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# 4âƒ£ï¸ ç¿»è¯‘å…¨é›†\n",
    "# ============================================================== \n",
    "def translate_library(src_root: str,\n",
    "                      out_en_root: str,\n",
    "                      out_zh_root: str,\n",
    "                      chapter_range: Union[Tuple[int, int], List[int]],\n",
    "                      n_chapters: int,\n",
    "                      engine: str = \"deepseek\",\n",
    "                      workers: int = 4):\n",
    "    \"\"\"\n",
    "    ç¿»è¯‘æ•´ä¸ªå°è¯´åº“ï¼šæ¯æœ¬å°è¯´æŠ½æ · n_chapters ä¸ªç« èŠ‚ç¿»è¯‘ã€‚\n",
    "    å¦‚æœè‹±æ–‡è¯‘æ–‡ç›®å½•ä¸­å·²å­˜åœ¨ >= n_chapters ä¸ªç¿»è¯‘æ–‡ä»¶ï¼Œåˆ™è·³è¿‡è¯¥æœ¬å°è¯´ã€‚\n",
    "    \"\"\"\n",
    "    src_root = Path(src_root)\n",
    "    out_en_root = Path(out_en_root)\n",
    "    out_zh_root = Path(out_zh_root)\n",
    "\n",
    "    full_range = list(range(chapter_range[0], chapter_range[1] + 1)) if isinstance(chapter_range, tuple) else list(chapter_range)\n",
    "\n",
    "    if n_chapters > len(full_range):\n",
    "        raise ValueError(f\"æŒ‡å®šç« èŠ‚æ•° n_chapters={n_chapters} è¶…è¿‡ç« èŠ‚èŒƒå›´é•¿åº¦ {len(full_range)}\")\n",
    "\n",
    "    novels = [d for d in src_root.iterdir() if d.is_dir()]\n",
    "    if not novels:\n",
    "        raise FileNotFoundError(f\"{src_root} ä¸‹æœªæ‰¾åˆ°ä»»ä½•å°è¯´å­ç›®å½•\")\n",
    "\n",
    "    for book in novels:\n",
    "        en_dir = out_en_root / book.name\n",
    "        if en_dir.exists():\n",
    "            completed = len(list(en_dir.glob(\"*_en.txt\")))\n",
    "            if completed >= n_chapters:\n",
    "                print(f\"âœ… è·³è¿‡ã€Š{book.name}ã€‹ï¼Œå·²æœ‰ {completed} ä¸ªç« èŠ‚ç¿»è¯‘\")\n",
    "                continue\n",
    "\n",
    "        chapters_sample = random.sample(full_range, k=n_chapters)\n",
    "        translate_one_book(book, out_en_root, out_zh_root, chapters_sample, engine, workers)\n",
    "\n",
    "    print(\"\\nâœ… å…¨éƒ¨å°è¯´ç¿»è¯‘å®Œæˆ\")\n",
    "    print(f\"è‹±æ–‡è¯‘æ–‡ç›®å½• : {out_en_root}\")\n",
    "    print(f\"ä¸­æ–‡å›è¯‘ç›®å½• : {out_zh_root}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# 5âƒ£ï¸ è¿è¡Œä»»åŠ¡\n",
    "# ============================================================== \n",
    "translate_library(SRC_ROOT,\n",
    "                  OUT_EN_ROOT,\n",
    "                  OUT_ZH_ROOT,\n",
    "                  chapter_range=CHAPTER_RANGE,\n",
    "                  n_chapters=N_CHAPTERS,\n",
    "                  engine=TRANSLATION_ENGINE,\n",
    "                  workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85692c5a-917a-49e5-998c-5a171939ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/translate_back_ZH_normalized\n",
    "!rm -rf /content/translate_EN_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5c5d520-8931-4060-9480-2f3e3da7cff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è·³è¿‡ã€Šã€Šå¥‹æ–—åœ¨æ–°æ˜æœã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šéšè½»é£å»_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šåæ­£æˆ‘æ˜¯è¶…èƒ½åŠ›è€…ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šåƒä¹¦å¦–_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šå¤©å¯æ±—ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè¥¿é£ç´§_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šå´©åä¸–ç•Œçš„ä¼ å¥‡å¤§å†’é™©ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå›½ç‹é™›ä¸‹_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šå…¨çƒè¿›åŒ–ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå’¬ç‹—_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šæ­¦æ—åŠä¾ ä¼ ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæ–‡æŠ„å…¬_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šå›½å®´å¤§å¨åœ¨å…«é›¶ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šæœç¥è®°ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæ ‘ä¸‹é‡ç‹_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šé‡ç”Ÿå…«é›¶ï¼šæ¯’å¦»ä¸å¥½æƒ¹ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šçªƒæ˜ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå¤§çˆ†ç‚¸(ç°ç†ŠçŒ«)_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šèœ€å±±ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæµæµªçš„è›¤èŸ†_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šé™ˆäºŒç‹—çš„å¦–å­½äººç”Ÿã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šçƒ½ç«æˆè¯¸ä¾¯_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šè´©ç½ªã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šä¸‰å¤©ä¸¤è§‰_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šé‡ç”Ÿä¹‹å‡ºäººå¤´åœ°ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šé—¹é—¹ä¸çˆ±é—¹_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šå…«é›¶å–œäº‹ï¼šå½“å®¶è‚¥å¦»å¤§ç¿»èº«ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šè‚†è™éŸ©å¨±ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå§¬å‰_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šå…«é›¶å¹´ä»£å¥½æ—¶å…‰ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šé£Ÿç‰©é“¾é¡¶ç«¯çš„ç”·äººã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šç†Šç‹¼ç‹—_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šé«˜æ‰‹å¯‚å¯2ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå…°å¸é­…æ™¨_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šé»‘é¾™æ³•å…¸ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæ¬¢å£°_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šè¯›ä»™ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè§é¼_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šå›åˆ°è¿‡å»å˜æˆçŒ«ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šé™ˆè¯æ‡’è°ƒ_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šç¥æ¸¸ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå¾å…¬å­èƒœæ²»_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šè€å­æ˜¯ç™è›¤èŸ†ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ ä½œè€…ï¼šçƒ½ç«æˆè¯¸ä¾¯_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šæœªæ¥å¤©ç‹ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šé™ˆè¯æ‡’è°ƒ_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šå¤§ç”»å®¶ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šé†›çŸ³_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šè¶…çº§æƒŠæ‚šç›´æ’­ã€‹ä½œè€…ï¼šå®‡æ–‡é•¿å¼“_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šäººé“å¤©å ‚ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè†æŸ¯å®ˆ_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šé“ç¼˜æµ®å›¾ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šçƒŸé›¨æ±Ÿå—_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šé‡æ´»äº†ã€‹ï¼ˆç²¾æ ¡ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼š å°è°•_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šé›…éªšã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè´¼é“ä¸‰ç—´_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šå…«é›¶ç¦æ˜Ÿä¿åª³å¦‡ã€‹ï¼Œå·²æœ‰ 8 ç« \n",
      "âœ… è·³è¿‡ã€Šã€Šä¸Šå“å¯’å£«ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šè´¼é“ä¸‰ç—´_utf8ã€‹ï¼Œå·²æœ‰ 8 ç« \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea1ab46502e44edbd6ba6635eb9eae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šåå·é£äº‘å¿—ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šçŸ¥ç§‹_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… translated: 007.txt\n",
      "âœ… translated: 011.txt\n",
      "âœ… translated: 048.txt\n",
      "âœ… translated: 078.txt\n",
      "âœ… translated: 122.txt\n",
      "âœ… translated: 146.txt\n",
      "âœ… translated: 152.txt\n",
      "âœ… translated: 195.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfd782434384c35a10b5803e8c6f1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šå²ä¸Šç¬¬ä¸€æ··ä¹±ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šå¼ å°èŠ±_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… translated: 015.txt\n",
      "âœ… translated: 050.txt\n",
      "âœ… translated: 088.txt\n",
      "âœ… translated: 112.txt\n",
      "âœ… translated: 125.txt\n",
      "âœ… translated: 142.txt\n",
      "âœ… translated: 175.txt\n",
      "âœ… translated: 176.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a1a3d07344408f9381eeae2c75cbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šéšæ³¢é€æµä¹‹ä¸€ä»£å†›å¸ˆã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šéšæ³¢é€æµ_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… translated: 058.txt\n",
      "âœ… translated: 060.txt\n",
      "âœ… translated: 144.txt\n",
      "âœ… translated: 164.txt\n",
      "âœ… translated: 185.txt\n",
      "âœ… translated: 193.txt\n",
      "âœ… translated: 194.txt\n",
      "âœ… translated: 196.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42648ce30d444659b8fd9bd2b60b0107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ é‡ç”Ÿå…«é›¶ï¼šä½³å¦»è‡´å¯Œå¿™:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… translated: 024.txt\n",
      "âœ… translated: 025.txt\n",
      "âœ… translated: 049.txt\n",
      "âœ… translated: 068.txt\n",
      "âœ… translated: 085.txt\n",
      "âœ… translated: 145.txt\n",
      "âœ… translated: 166.txt\n",
      "âœ… translated: 183.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de26fd89e5ae4fe0a646b6ed1c8d54e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ é‡å›å…«é›¶è¿‡å¥½æ—¥å­:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… translated: 063.txt\n",
      "âœ… translated: 066.txt\n",
      "âœ… translated: 129.txt\n",
      "âœ… translated: 150.txt\n",
      "âœ… translated: 162.txt\n",
      "âœ… translated: 176.txt\n",
      "âœ… translated: 180.txt\n",
      "âœ… translated: 186.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb802d533a246b0841f2adead1fad11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šç»å¯¹ä¸€ç•ªã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæµ·åº•æ¼«æ­¥è€…_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… translated: 009.txt\n",
      "âœ… translated: 029.txt\n",
      "âœ… translated: 037.txt\n",
      "âœ… translated: 046.txt\n",
      "âœ… translated: 089.txt\n",
      "âœ… translated: 099.txt\n",
      "âœ… translated: 145.txt\n",
      "âœ… translated: 161.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f13b4f88404a8f870f37d7eb202f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ ã€Šæˆ‘çš„å¥³å‹æ˜¯æ¶å¥³ã€‹ï¼ˆæ ¡å¯¹ç‰ˆå…¨æœ¬ï¼‰ä½œè€…ï¼šæµ·åº•æ¼«æ­¥è€…_utf8:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… translated: 034.txt\n",
      "âœ… translated: 053.txt\n",
      "âœ… translated: 063.txt\n",
      "âœ… translated: 116.txt\n",
      "âœ… translated: 142.txt\n",
      "âœ… translated: 154.txt\n",
      "âœ… translated: 179.txt\n",
      "âœ… translated: 198.txt\n",
      "\n",
      "ğŸ‰ å…¨éƒ¨å°è¯´å¤„ç†å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 0âƒ£ï¸ å®‰è£…ä¾èµ–\n",
    "# ==============================================================\n",
    "!pip -q install --upgrade openai google-generativeai chardet tqdm\n",
    "\n",
    "# ==============================================================\n",
    "# 1âƒ£ï¸ é…ç½®\n",
    "# ==============================================================\n",
    "import os, re, json, unicodedata, time, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "from tqdm.auto import tqdm\n",
    "import chardet\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# DeepSeek\n",
    "import openai\n",
    "DEEPSEEK_API_KEY  = \"your-default-api-key\"\n",
    "DEEPSEEK_URL      = \"https://api.deepseek.com\"\n",
    "openai.api_key    = DEEPSEEK_API_KEY\n",
    "deep_client       = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_URL)\n",
    "\n",
    "# Gemini\n",
    "import google.generativeai as genai\n",
    "GEMINI_API_KEY     = \"your-default-api-key\"\n",
    "GEMINI_MODEL       = \"gemini-2.0-flash\"\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "\n",
    "# è·¯å¾„å‚æ•°\n",
    "SRC_ROOT    = Path(\"/content/novels_normalized\")\n",
    "OUT_EN_ROOT = Path(\"/content/translate_EN_normalized\")\n",
    "OUT_ZH_ROOT = Path(\"/content/translate_back_ZH_normalized\")\n",
    "SEED_ROOT   = Path(\"/content/randomseed\")\n",
    "\n",
    "CHAPTER_RANGE = (1, 200)\n",
    "N_CHAPTERS    = 8\n",
    "WORKERS       = 2\n",
    "TRANSLATION_ENGINE = \"gemini\"  # å¯é€‰ï¼š \"deepseek\" / \"gemini\"\n",
    "\n",
    "PROMPT_ZH2EN = \"You are a professional translator. Translate the following Chinese literary text into vivid, fluent English. Keep paragraph breaks. Output ONLY the translation:\"\n",
    "PROMPT_EN2ZH = \"ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–‡å­¦è¯‘è€…ï¼Œè¯·å°†ä¸‹é¢çš„è‹±æ–‡æ–‡æœ¬ç²¾å‡†åœ°è¯‘å›ä¸­æ–‡ï¼Œä¿æŒæ®µè½åˆ’åˆ†ï¼Œä¸è¦åŠ å…¥é¢å¤–è¯´æ˜ï¼Œåªè¾“å‡ºè¯‘æ–‡ï¼š\"\n",
    "\n",
    "# ==============================================================\n",
    "# 2âƒ£ï¸ å·¥å…·å‡½æ•°\n",
    "# ==============================================================\n",
    "_CHAP_NO_RE = re.compile(r\"(\\d{1,4})\")\n",
    "\n",
    "def _chapter_no(path: Path) -> int:\n",
    "    m = _CHAP_NO_RE.search(path.stem)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def _auto_read(path: Path) -> str:\n",
    "    raw = path.read_bytes()\n",
    "    enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "    return raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "def _call_deepseek(prompt, text):\n",
    "    rsp = deep_client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": text}],\n",
    "        temperature=0.1, max_tokens=8092\n",
    "    )\n",
    "    return rsp.choices[0].message.content.strip()\n",
    "\n",
    "def _call_gemini(prompt, text):\n",
    "    full_prompt = f\"{prompt.strip()}\\n\\n{text.strip()}\"\n",
    "    rsp = gemini_model.generate_content(full_prompt, generation_config={\"temperature\": 0.3})\n",
    "    return rsp.text.strip()\n",
    "\n",
    "def _call_translate(prompt, text, engine):\n",
    "    try:\n",
    "        return _call_gemini(prompt, text) if engine == \"gemini\" else _call_deepseek(prompt, text)\n",
    "    except Exception as e:\n",
    "        return f\"ã€ç¿»è¯‘å‡ºé”™ã€‘{e}\"\n",
    "\n",
    "# âœ… ç¿»è¯‘ä¸€æœ¬å°è¯´çš„æŒ‡å®šç« èŠ‚ï¼ˆå¸¦è·³è¿‡ï¼‰\n",
    "def translate_one_book(book_dir: Path, out_en_root: Path, out_zh_root: Path,\n",
    "                       chapters: List[int], engine: str = \"gemini\", workers: int = 2):\n",
    "    chap_set = set(chapters)\n",
    "    file_pairs = []\n",
    "    for fp in book_dir.glob(\"*.txt\"):\n",
    "        no = _chapter_no(fp)\n",
    "        if no in chap_set:\n",
    "            file_pairs.append((no, fp))\n",
    "            chap_set.remove(no)\n",
    "\n",
    "    if chap_set:\n",
    "        print(f\"âš ï¸ ç¼ºç« èŠ‚: {chap_set}ï¼Œè·³è¿‡éƒ¨åˆ†\")\n",
    "\n",
    "    file_pairs.sort()\n",
    "    en_dir = out_en_root / book_dir.name\n",
    "    zh_dir = out_zh_root / book_dir.name\n",
    "    en_dir.mkdir(parents=True, exist_ok=True)\n",
    "    zh_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _translate(no, fp):\n",
    "        try:\n",
    "            en_path = en_dir / f\"{fp.stem}_en.txt\"\n",
    "            zh_path = zh_dir / f\"{fp.stem}_back_cn.txt\"\n",
    "\n",
    "            if en_path.exists() and zh_path.exists():\n",
    "                return f\"âœ… skipped: {fp.name}\"\n",
    "\n",
    "            zh_text = _auto_read(fp)\n",
    "            en_text = _call_translate(PROMPT_ZH2EN, zh_text, engine)\n",
    "            en_path.write_text(en_text, encoding=\"utf-8\")\n",
    "            zh_back = _call_translate(PROMPT_EN2ZH, en_text, engine)\n",
    "            zh_path.write_text(zh_back, encoding=\"utf-8\")\n",
    "            return f\"âœ… translated: {fp.name}\"\n",
    "        except Exception as e:\n",
    "            return f\"âŒ error in {fp.name}: {e}\"\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        results = list(tqdm(ex.map(lambda t: _translate(*t), file_pairs), total=len(file_pairs),\n",
    "                            desc=f\"ğŸ”„ {book_dir.name}\"))\n",
    "    for line in results:\n",
    "        print(line)\n",
    "\n",
    "# âœ… éå†æ•´ä¸ªå°è¯´ç›®å½•ç¿»è¯‘ï¼ˆè·³è¿‡å·²å®Œæˆæˆ–å‡ºé”™çš„ï¼‰\n",
    "def translate_library(src_root: str, out_en_root: str, out_zh_root: str,\n",
    "                      chapter_range: Union[Tuple[int, int], List[int]],\n",
    "                      n_chapters: int, engine: str = \"gemini\", workers: int = 2):\n",
    "    src_root = Path(src_root)\n",
    "    out_en_root = Path(out_en_root)\n",
    "    out_zh_root = Path(out_zh_root)\n",
    "    full_range = list(range(chapter_range[0], chapter_range[1] + 1)) if isinstance(chapter_range, tuple) else list(chapter_range)\n",
    "\n",
    "    novels = [d for d in src_root.iterdir() if d.is_dir()]\n",
    "    for book in novels:\n",
    "        try:\n",
    "            en_dir = out_en_root / book.name\n",
    "            if en_dir.exists():\n",
    "                done = len(list(en_dir.glob(\"*_en.txt\")))\n",
    "                if done >= n_chapters:\n",
    "                    print(f\"âœ… è·³è¿‡ã€Š{book.name}ã€‹ï¼Œå·²æœ‰ {done} ç« \")\n",
    "                    continue\n",
    "            chapters = random.sample(full_range, k=n_chapters)\n",
    "            translate_one_book(book, out_en_root, out_zh_root, chapters, engine, workers)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è·³è¿‡ã€Š{book.name}ã€‹ï¼Œé”™è¯¯ï¼š{e}\")\n",
    "\n",
    "    print(\"\\nğŸ‰ å…¨éƒ¨å°è¯´å¤„ç†å®Œæˆã€‚\")\n",
    "\n",
    "# âœ… æ‰§è¡Œä»»åŠ¡\n",
    "translate_library(SRC_ROOT, OUT_EN_ROOT, OUT_ZH_ROOT,\n",
    "                  chapter_range=CHAPTER_RANGE,\n",
    "                  n_chapters=N_CHAPTERS,\n",
    "                  engine=TRANSLATION_ENGINE,\n",
    "                  workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30b0eb-dd89-4925-a212-5b5081ce1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 0âƒ£ï¸ å®‰è£…ä¾èµ–\n",
    "# ==============================================================Â \n",
    "!pip -q install --upgrade openai google-generativeai chardet tqdm\n",
    "\n",
    "# ==============================================================\n",
    "# 1âƒ£ï¸ é…ç½®\n",
    "# ==============================================================Â \n",
    "import os, re, json, unicodedata, time, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "from tqdm.auto import tqdm\n",
    "import chardet\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# DeepSeek\n",
    "import openai\n",
    "DEEPSEEK_API_KEYÂ  = \"your-default-api-key\"\n",
    "DEEPSEEK_URLÂ  Â  Â  = \"https://api.deepseek.com\"\n",
    "openai.api_keyÂ  Â  = DEEPSEEK_API_KEY\n",
    "deep_clientÂ  Â  Â  Â = openai.OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_URL)\n",
    "\n",
    "# Gemini\n",
    "import google.generativeai as genai\n",
    "GEMINI_API_KEYÂ  Â  Â = \"your-default-api-key\"\n",
    "GEMINI_MODELÂ  Â  Â  Â = \"gemini-2.0-flash\"Â  # æ¨è pro\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "\n",
    "# ç¿»è¯‘æ§åˆ¶å‚æ•°\n",
    "TRANSLATION_ENGINE = \"gemini\"Â  # å¯é€‰ï¼š \"deepseek\" / \"gemini\"\n",
    "\n",
    "# è·¯å¾„å‚æ•°\n",
    "SRC_ROOTÂ  Â  = \"/content/novels_normalized\"\n",
    "OUT_EN_ROOT = \"/content/translate_EN_normalized\"\n",
    "OUT_ZH_ROOT = \"/content/translate_back_ZH_normalized\"\n",
    "CHAPTER_RANGE = (1, 200)\n",
    "N_CHAPTERSÂ  Â  = 8\n",
    "WORKERSÂ  Â  Â  Â = 2\n",
    "\n",
    "PROMPT_ZH2EN = \"You are a professional translator. Translate the following Chinese literary text into vivid, fluent English. Keep paragraph breaks. Output ONLY the translation:\"\n",
    "PROMPT_EN2ZH = \"ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–‡å­¦è¯‘è€…ï¼Œè¯·å°†ä¸‹é¢çš„è‹±æ–‡æ–‡æœ¬ç²¾å‡†åœ°è¯‘å›ä¸­æ–‡ï¼Œä¿æŒæ®µè½åˆ’åˆ†ï¼Œä¸è¦åŠ å…¥é¢å¤–è¯´æ˜ï¼Œåªè¾“å‡ºè¯‘æ–‡ï¼š\"\n",
    "\n",
    "# ==============================================================\n",
    "# 2âƒ£ï¸ å·¥å…·å‡½æ•°\n",
    "# ==============================================================Â \n",
    "_CHAP_NO_RE = re.compile(r\"(\\d{1,4})\")\n",
    "\n",
    "def _chapter_no(path: Path) -> int:\n",
    "Â  Â  m = _CHAP_NO_RE.search(path.stem)\n",
    "Â  Â  if not m:\n",
    "Â  Â  Â  Â  raise ValueError(f\"æ–‡ä»¶åæ— æ³•è¯†åˆ«ç« èŠ‚å·: {path}\")\n",
    "Â  Â  return int(m.group(1))\n",
    "\n",
    "def _auto_read(path: Path) -> str:\n",
    "Â  Â  raw = path.read_bytes()\n",
    "Â  Â  enc = chardet.detect(raw)[\"encoding\"] or \"utf-8\"\n",
    "Â  Â  return raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "def _call_deepseek(prompt: str, text: str) -> str:\n",
    "Â  Â  rsp = deep_client.chat.completions.create(\n",
    "Â  Â  Â  Â  model=\"deepseek-chat\",\n",
    "Â  Â  Â  Â  messages=[{\"role\": \"system\", \"content\": prompt},\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  {\"role\": \"user\",Â  Â \"content\": text}],\n",
    "Â  Â  Â  Â  temperature=0.1,\n",
    "Â  Â  Â  Â  max_tokens=8092\n",
    "Â  Â  )\n",
    "Â  Â  return rsp.choices[0].message.content.strip()\n",
    "\n",
    "def _call_gemini(prompt: str, text: str) -> str:\n",
    "Â  Â  full_prompt = f\"{prompt.strip()}\\n\\n{text.strip()}\"\n",
    "Â  Â  rsp = gemini_model.generate_content(full_prompt, generation_config={\"temperature\": 0.3})\n",
    "Â  Â  return rsp.text.strip()\n",
    "\n",
    "def _call_translate(prompt: str, text: str, engine: str) -> str:\n",
    "Â  Â  if engine == \"gemini\":\n",
    "Â  Â  Â  Â  return _call_gemini(prompt, text)\n",
    "Â  Â  elif engine == \"deepseek\":\n",
    "Â  Â  Â  Â  return _call_deepseek(prompt, text)\n",
    "Â  Â  else:\n",
    "Â  Â  Â  Â  raise ValueError(f\"æœªçŸ¥ç¿»è¯‘å¼•æ“: {engine}\")\n",
    "Â  Â Â \n",
    "# ==============================================================\n",
    "# 3âƒ£ï¸ ç¿»è¯‘å•æœ¬å°è¯´\n",
    "# ==============================================================Â \n",
    "def translate_one_book(book_dir: Path,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â out_en_root: Path,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â out_zh_root: Path,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â chapters: List[int],\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â engine: str = \"deepseek\",\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â workers: int = 4):\n",
    "Â  Â  \"\"\"\n",
    "Â  Â  ç¿»è¯‘å•æœ¬å°è¯´çš„æŒ‡å®šç« èŠ‚ã€‚å¦‚æœå·²å­˜åœ¨è‹±æ–‡å’Œå›è¯‘æ–‡ä»¶åˆ™è·³è¿‡ã€‚\n",
    "Â  Â  \"\"\"\n",
    "Â  Â  chap_set = set(chapters)\n",
    "Â  Â  file_pairs = []\n",
    "Â  Â  for fp in book_dir.glob(\"*.txt\"):\n",
    "Â  Â  Â  Â  no = _chapter_no(fp)\n",
    "Â  Â  Â  Â  if no in chap_set:\n",
    "Â  Â  Â  Â  Â  Â  file_pairs.append((no, fp))\n",
    "Â  Â  Â  Â  Â  Â  chap_set.remove(no)\n",
    "\n",
    "Â  Â  if chap_set:\n",
    "Â  Â  Â  Â  raise FileNotFoundError(f\"ã€Š{book_dir.name}ã€‹ç¼ºå°‘ç« èŠ‚: {sorted(chap_set)}\")\n",
    "\n",
    "Â  Â  file_pairs.sort()\n",
    "Â  Â  en_dir = out_en_root / book_dir.name\n",
    "Â  Â  zh_dir = out_zh_root / book_dir.name\n",
    "Â  Â  en_dir.mkdir(parents=True, exist_ok=True)\n",
    "Â  Â  zh_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Â  Â  def _translate(no: int, fp: Path):\n",
    "Â  Â  Â  Â  en_path = en_dir / f\"{fp.stem}_en.txt\"\n",
    "Â  Â  Â  Â  zh_path = zh_dir / f\"{fp.stem}_back_cn.txt\"\n",
    "\n",
    "Â  Â  Â  Â  # è·³è¿‡å·²å­˜åœ¨çš„ç¿»è¯‘ç»“æœ\n",
    "Â  Â  Â  Â  if en_path.exists() and zh_path.exists():\n",
    "Â  Â  Â  Â  Â  Â  return f\"âœ… skipped: {fp.name}\"\n",
    "\n",
    "Â  Â  Â  Â  zh_text = _auto_read(fp)\n",
    "\n",
    "Â  Â  Â  Â  # ä¸­æ–‡ â†’ è‹±æ–‡\n",
    "Â  Â  Â  Â  en_text = _call_translate(PROMPT_ZH2EN, zh_text, engine)\n",
    "Â  Â  Â  Â  en_path.write_text(en_text, encoding=\"utf-8\")\n",
    "\n",
    "Â  Â  Â  Â  # è‹±æ–‡ â†’ ä¸­æ–‡å›è¯‘\n",
    "Â  Â  Â  Â  zh_back = _call_translate(PROMPT_EN2ZH, en_text, engine)\n",
    "Â  Â  Â  Â  zh_path.write_text(zh_back, encoding=\"utf-8\")\n",
    "\n",
    "Â  Â  Â  Â  return f\"âœ… translated: {fp.name}\"\n",
    "\n",
    "Â  Â  with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "Â  Â  Â  Â  results = list(tqdm(ex.map(lambda t: _translate(*t), file_pairs),\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total=len(file_pairs),\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  desc=f\"ğŸ”„ {book_dir.name}\"))\n",
    "Â  Â  for line in results:\n",
    "Â  Â  Â  Â  print(line)\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# 4âƒ£ï¸ ç¿»è¯‘å…¨é›†\n",
    "# ==============================================================Â \n",
    "def translate_library(src_root: str,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  out_en_root: str,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  out_zh_root: str,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chapter_range: Union[Tuple[int, int], List[int]],\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  n_chapters: int,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  engine: str = \"deepseek\",\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  workers: int = 4):\n",
    "Â  Â  \"\"\"\n",
    "Â  Â  ç¿»è¯‘æ•´ä¸ªå°è¯´åº“ï¼šæ¯æœ¬å°è¯´æŠ½æ · n_chapters ä¸ªç« èŠ‚ç¿»è¯‘ã€‚\n",
    "Â  Â  å¦‚æœè‹±æ–‡è¯‘æ–‡ç›®å½•ä¸­å·²å­˜åœ¨ >= n_chapters ä¸ªç¿»è¯‘æ–‡ä»¶ï¼Œåˆ™è·³è¿‡è¯¥æœ¬å°è¯´ã€‚\n",
    "Â  Â  \"\"\"\n",
    "Â  Â  src_root = Path(src_root)\n",
    "Â  Â  out_en_root = Path(out_en_root)\n",
    "Â  Â  out_zh_root = Path(out_zh_root)\n",
    "\n",
    "Â  Â  full_range = list(range(chapter_range[0], chapter_range[1] + 1)) if isinstance(chapter_range, tuple) else list(chapter_range)\n",
    "\n",
    "Â  Â  if n_chapters > len(full_range):\n",
    "Â  Â  Â  Â  raise ValueError(f\"æŒ‡å®šç« èŠ‚æ•° n_chapters={n_chapters} è¶…è¿‡ç« èŠ‚èŒƒå›´é•¿åº¦ {len(full_range)}\")\n",
    "\n",
    "Â  Â  novels = [d for d in src_root.iterdir() if d.is_dir()]\n",
    "Â  Â  if not novels:\n",
    "Â  Â  Â  Â  raise FileNotFoundError(f\"{src_root} ä¸‹æœªæ‰¾åˆ°ä»»ä½•å°è¯´å­ç›®å½•\")\n",
    "\n",
    "Â  Â  for book in novels:\n",
    "Â  Â  Â  Â  en_dir = out_en_root / book.name\n",
    "Â  Â  Â  Â  if en_dir.exists():\n",
    "Â  Â  Â  Â  Â  Â  completed = len(list(en_dir.glob(\"*_en.txt\")))\n",
    "Â  Â  Â  Â  Â  Â  if completed >= n_chapters:\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  print(f\"âœ… è·³è¿‡ã€Š{book.name}ã€‹ï¼Œå·²æœ‰ {completed} ä¸ªç« èŠ‚ç¿»è¯‘\")\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  continue\n",
    "\n",
    "Â  Â  Â  Â  chapters_sample = random.sample(full_range, k=n_chapters)\n",
    "Â  Â  Â  Â  translate_one_book(book, out_en_root, out_zh_root, chapters_sample, engine, workers)\n",
    "\n",
    "Â  Â  print(\"\\nâœ… å…¨éƒ¨å°è¯´ç¿»è¯‘å®Œæˆ\")\n",
    "Â  Â  print(f\"è‹±æ–‡è¯‘æ–‡ç›®å½• : {out_en_root}\")\n",
    "Â  Â  print(f\"ä¸­æ–‡å›è¯‘ç›®å½• : {out_zh_root}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# 5âƒ£ï¸ è¿è¡Œä»»åŠ¡\n",
    "# ==============================================================Â \n",
    "translate_library(SRC_ROOT,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  OUT_EN_ROOT,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  OUT_ZH_ROOT,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  chapter_range=CHAPTER_RANGE,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  n_chapters=N_CHAPTERS,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  engine=TRANSLATION_ENGINE,\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eaee2e-b9d1-4076-ab7e-2ac2eec6d047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
