{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955c6d05-73b5-4a67-8802-463aacf7dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c03dc0-5603-4209-a0d8-397fa84fc6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re, csv, tqdm, pandas as pd\n",
    "\n",
    "# ① 修改成你的 Google Drive 路径\n",
    "INPUT_DIR  = Path(\"/content/novels\")     # 原始 .txt\n",
    "OUTPUT_DIR = Path(\"/content/novels_normalized\")   # 输出根目录\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ② 拆分参数\n",
    "CAP_WORDS  = 1_000_000      # 仅取前 100 万“词”\n",
    "N_NORM     = 200            # 拆成 200 章\n",
    "TOLERANCE  = 200            # 回溯段落时最多回退 200 词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ad9fb7-f942-496e-8610-9fccad5ac61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 词切分：用正则把所有连续字母/数字/中文视为一个“词” ===\n",
    "WORD_RE = re.compile(r\"\\w+\", re.UNICODE)   # 可自行换更复杂分词器\n",
    "\n",
    "def words_list(text: str):\n",
    "    \"\"\"返回 word 列表（带起始字符索引）\"\"\"\n",
    "    return [(m.group(), m.start()) for m in WORD_RE.finditer(text)]\n",
    "\n",
    "def nearest_par_break(text: str, abs_pos: int, tol: int = TOLERANCE):\n",
    "    \"\"\"向前找最近双换行；找不到则硬切\"\"\"\n",
    "    idx = abs_pos\n",
    "    while idx > 0 and abs_pos - idx < tol * 8:      # *8 ≈ 词→字符粗略放大\n",
    "        if text[idx-1:idx+1] == \"\\n\\n\":\n",
    "            return idx\n",
    "        idx -= 1\n",
    "    return abs_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7856b797-79b0-46f3-bd27-645bedc8a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_words(book_path: Path):\n",
    "    raw_text = book_path.read_text(encoding=\"utf-8\")\n",
    "    wlist = words_list(raw_text)[:CAP_WORDS]        # [(word, char_pos), ...]\n",
    "    total_words = len(wlist)\n",
    "    target = total_words // N_NORM or 1             # 防 0 除\n",
    "\n",
    "    meta_rows, chapter_texts = [], []\n",
    "    cursor_idx = 0\n",
    "    prev_char_pos = 0\n",
    "\n",
    "    for k in range(1, N_NORM + 1):\n",
    "        start_idx = cursor_idx\n",
    "        cursor_idx += target\n",
    "        if cursor_idx >= total_words:\n",
    "            cursor_idx = total_words\n",
    "\n",
    "        # 回溯到段落边界\n",
    "        char_pos = wlist[cursor_idx-1][1] + len(wlist[cursor_idx-1][0])\n",
    "        char_pos = nearest_par_break(raw_text, char_pos)\n",
    "\n",
    "        # 更新 cursor_idx 到新的 char_pos 对应 word 下标\n",
    "        while cursor_idx < total_words and wlist[cursor_idx][1] < char_pos:\n",
    "            cursor_idx += 1\n",
    "\n",
    "        chapter_text = raw_text[prev_char_pos:char_pos]\n",
    "\n",
    "        meta_rows.append({\n",
    "            \"book_id\"        : book_path.stem,\n",
    "            \"norm_chap_id\"   : f\"{k:03}\",\n",
    "            \"start_word_idx\" : start_idx,\n",
    "            \"end_word_idx\"   : cursor_idx,\n",
    "            \"chapter_word_cnt\": cursor_idx - start_idx,\n",
    "            \"book_word_total\": total_words\n",
    "        })\n",
    "        chapter_texts.append(chapter_text)\n",
    "        prev_char_pos = char_pos\n",
    "\n",
    "        if cursor_idx >= total_words:\n",
    "            break\n",
    "\n",
    "    return meta_rows, chapter_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef38288-ba58-49b3-a787-e250c67b1f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting by words: 100%|██████████| 40/40 [00:04<00:00,  8.73it/s]\n"
     ]
    }
   ],
   "source": [
    "all_meta = []\n",
    "for p in tqdm.tqdm(sorted(INPUT_DIR.glob(\"*.txt\")), desc=\"Splitting by words\"):\n",
    "    meta, txts = split_by_words(p)\n",
    "    all_meta.extend(meta)\n",
    "\n",
    "    out_dir = OUTPUT_DIR / \"split_txt\" / p.stem\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for m, t in zip(meta, txts):\n",
    "        (out_dir / f\"{p.stem}_{m['norm_chap_id']}.txt\").write_text(t, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76198f46-70e5-4fc9-919c-5fa6b09793de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>norm_chap_id</th>\n",
       "      <th>start_word_idx</th>\n",
       "      <th>end_word_idx</th>\n",
       "      <th>chapter_word_cnt</th>\n",
       "      <th>book_word_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>《上品寒士》（校对版全本）作者：贼道三痴_utf8</td>\n",
       "      <td>001</td>\n",
       "      <td>0</td>\n",
       "      <td>823</td>\n",
       "      <td>823</td>\n",
       "      <td>164712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>《上品寒士》（校对版全本）作者：贼道三痴_utf8</td>\n",
       "      <td>002</td>\n",
       "      <td>823</td>\n",
       "      <td>1646</td>\n",
       "      <td>823</td>\n",
       "      <td>164712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>《上品寒士》（校对版全本）作者：贼道三痴_utf8</td>\n",
       "      <td>003</td>\n",
       "      <td>1646</td>\n",
       "      <td>2469</td>\n",
       "      <td>823</td>\n",
       "      <td>164712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>《上品寒士》（校对版全本）作者：贼道三痴_utf8</td>\n",
       "      <td>004</td>\n",
       "      <td>2469</td>\n",
       "      <td>3292</td>\n",
       "      <td>823</td>\n",
       "      <td>164712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>《上品寒士》（校对版全本）作者：贼道三痴_utf8</td>\n",
       "      <td>005</td>\n",
       "      <td>3292</td>\n",
       "      <td>4115</td>\n",
       "      <td>823</td>\n",
       "      <td>164712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     book_id norm_chap_id  start_word_idx  end_word_idx  \\\n",
       "0  《上品寒士》（校对版全本）作者：贼道三痴_utf8          001               0           823   \n",
       "1  《上品寒士》（校对版全本）作者：贼道三痴_utf8          002             823          1646   \n",
       "2  《上品寒士》（校对版全本）作者：贼道三痴_utf8          003            1646          2469   \n",
       "3  《上品寒士》（校对版全本）作者：贼道三痴_utf8          004            2469          3292   \n",
       "4  《上品寒士》（校对版全本）作者：贼道三痴_utf8          005            3292          4115   \n",
       "\n",
       "   chapter_word_cnt  book_word_total  \n",
       "0               823           164712  \n",
       "1               823           164712  \n",
       "2               823           164712  \n",
       "3               823           164712  \n",
       "4               823           164712  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_meta)\n",
    "df.to_csv(OUTPUT_DIR / \"normalized_200chapters_meta_words.csv\",\n",
    "          index=False, encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617c68af-ce64-41c9-bd2c-806d082a4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/novels_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b282a244-f044-4e5d-b739-0c47dbdff17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Books: 100%|██████████| 40/40 [00:04<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 全部完成：\n",
      "  - 拆分小说数：40\n",
      "  - 输出目录  : /content/novels_normalized\n",
      "  - 每本书生成 200 个文件 001.txt–200.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ① 依赖\n",
    "# ===============================================================\n",
    "!pip install -q tqdm pandas\n",
    "\n",
    "from pathlib import Path\n",
    "import re, tqdm, pandas as pd\n",
    "\n",
    "INPUT_DIR  = Path(\"/content/novels\")             # 改成你的根目录\n",
    "OUTPUT_DIR = Path(\"/content/novels_normalized\")  # 输出根目录\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CAP_WORDS  = 1_000_000        # 取前 100 万词，如需全长改 None\n",
    "N_NORM     = 200\n",
    "TOLERANCE  = 200\n",
    "\n",
    "# ===============================================================\n",
    "# ② 基础工具\n",
    "# ===============================================================\n",
    "WORD_RE = re.compile(r\"\\w+\", re.UNICODE)\n",
    "\n",
    "def words_list(text):\n",
    "    return [(m.group(), m.start()) for m in WORD_RE.finditer(text)]\n",
    "\n",
    "def nearest_par_break(text, abs_pos, tol=TOLERANCE):\n",
    "    idx = abs_pos\n",
    "    while idx > 0 and abs_pos - idx < tol * 8:\n",
    "        if text[idx-1:idx+1] == \"\\n\\n\":\n",
    "            return idx\n",
    "        idx -= 1\n",
    "    return abs_pos\n",
    "\n",
    "def split_by_words(raw_text: str, book_id: str):\n",
    "    w = words_list(raw_text)\n",
    "    if CAP_WORDS is not None:\n",
    "        w = w[:CAP_WORDS]\n",
    "    tot = len(w); target = tot // N_NORM or 1\n",
    "    metas, chunks = [], []; cur = 0; prev_char = 0\n",
    "    for k in range(1, N_NORM + 1):\n",
    "        start = cur; cur += target\n",
    "        if cur >= tot: cur = tot\n",
    "        char_pos = w[cur-1][1] + len(w[cur-1][0])\n",
    "        char_pos = nearest_par_break(raw_text, char_pos)\n",
    "        while cur < tot and w[cur][1] < char_pos:\n",
    "            cur += 1\n",
    "        chunks.append(raw_text[prev_char:char_pos])\n",
    "        metas.append(dict(book_id=book_id, norm_chap_id=f\"{k:03}\",\n",
    "                          start_word_idx=start, end_word_idx=cur,\n",
    "                          chapter_word_cnt=cur-start, book_word_total=tot))\n",
    "        prev_char = char_pos\n",
    "        if cur >= tot: break\n",
    "    while len(chunks) < N_NORM:                   # 补空章\n",
    "        k = len(chunks) + 1\n",
    "        chunks.append(\"\")\n",
    "        metas.append(dict(book_id=book_id, norm_chap_id=f\"{k:03}\",\n",
    "                          start_word_idx=cur, end_word_idx=cur,\n",
    "                          chapter_word_cnt=0, book_word_total=tot))\n",
    "    return metas, chunks\n",
    "\n",
    "def natural_key(p: Path):\n",
    "    parts = re.split(r'(\\d+)', p.name)\n",
    "    return [int(s) if s.isdigit() else s.lower() for s in parts]\n",
    "\n",
    "# ===============================================================\n",
    "# ③ 递归寻找“书”\n",
    "#    - 若文件夹下面直接有 *.txt → 视为“一本书”\n",
    "#    - 若根目录存在孤立 *.txt → 也当成一本书\n",
    "# ===============================================================\n",
    "def yield_books(root: Path):\n",
    "    # 情形 B：根目录孤立 txt\n",
    "    singles = [p for p in root.glob(\"*.txt\")]\n",
    "    for p in singles:\n",
    "        yield p.stem, [p]          # book_id, list[Path]\n",
    "\n",
    "    # 情形 A/C：子文件夹\n",
    "    for sub in root.rglob(\"*\"):\n",
    "        if not sub.is_dir(): continue\n",
    "        txts = list(sub.glob(\"*.txt\"))\n",
    "        if txts:\n",
    "            yield sub.relative_to(root).parts[0], txts  # book_id=顶层文件夹名\n",
    "\n",
    "# ===============================================================\n",
    "# ④ 主流程\n",
    "# ===============================================================\n",
    "all_meta = []\n",
    "book_cnt = 0\n",
    "\n",
    "for book_id, txt_files in tqdm.tqdm(list(yield_books(INPUT_DIR)),\n",
    "                                    desc=\"Books\"):\n",
    "    book_cnt += 1\n",
    "    txt_files = sorted(txt_files, key=natural_key)\n",
    "    full_text = \"\\n\".join(f.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                          for f in txt_files)\n",
    "\n",
    "    metas, chunks = split_by_words(full_text, book_id)\n",
    "\n",
    "    out_dir = OUTPUT_DIR / book_id\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for m, chunk in zip(metas, chunks):\n",
    "        (out_dir / f\"{m['norm_chap_id']}.txt\").write_text(\n",
    "            chunk, encoding=\"utf-8\")\n",
    "    all_meta.extend(metas)\n",
    "\n",
    "# ===============================================================\n",
    "# ⑤ 汇总 CSV\n",
    "# ===============================================================\n",
    "pd.DataFrame(all_meta).to_csv(\n",
    "    OUTPUT_DIR / \"normalized_200chapters_meta_words.csv\",\n",
    "    index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ 全部完成：\")\n",
    "print(f\"  - 拆分小说数：{book_cnt}\")\n",
    "print(f\"  - 输出目录  : {OUTPUT_DIR}\")\n",
    "print(\"  - 每本书生成 200 个文件 001.txt–200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c747c1-23fb-4b2a-89ba-b2ebdb2f872e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc89b890-f65c-4584-8861-c18bfa3c763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/translate_back_ZH\n",
    "!rm -rf /content/translate_EN\n",
    "!rm -rf /content/novels_chapters\n",
    "!rm -rf /content/最终精修保存路径\n",
    "!rm -rf /content/randomseed\n",
    "!rm -rf /content/outputs\n",
    "!rm -rf /content/outlines\n",
    "!rm -rf /content/1000_word_chapters_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c5653-3e51-4e1b-904a-4da323640245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
